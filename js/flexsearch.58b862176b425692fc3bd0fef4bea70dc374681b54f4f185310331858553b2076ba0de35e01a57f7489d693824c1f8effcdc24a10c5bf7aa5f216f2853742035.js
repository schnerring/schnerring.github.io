(()=>{var ne=Object.create;var ee=Object.defineProperty;var ae=Object.getOwnPropertyDescriptor;var ie=Object.getOwnPropertyNames;var se=Object.getPrototypeOf,ue=Object.prototype.hasOwnProperty;var le=e=>ee(e,"__esModule",{value:!0});var ce=(e,r)=>()=>(r||e((r={exports:{}}).exports,r),r.exports);var de=(e,r,o,n)=>{if(r&&typeof r=="object"||typeof r=="function")for(let i of ie(r))!ue.call(e,i)&&(o||i!=="default")&&ee(e,i,{get:()=>r[i],enumerable:!(n=ae(r,i))||n.enumerable});return e},he=(e,r)=>de(le(ee(e!=null?ne(se(e)):{},"default",!r&&e&&e.__esModule?{get:()=>e.default,enumerable:!0}:{value:e,enumerable:!0})),e);var re=ce((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let r=Array(e);for(let o=0;o<e;o++)r[o]=v();return r}function v(){return Object.create(null)}function ba(e,r){return r.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,r){var o=da;if(e&&(r&&(e=E(e,r)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),o||o==="")){if(e=e.split(o),this.filter){r=this.filter,o=e.length;let n=[];for(let i=0,s=0;i<o;i++){let l=e[i];l&&!r[l]&&(n[s++]=l)}e=n}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,r){let o=Object.keys(e),n=o.length,i=[],s="",l=0;for(let h=0,p,g;h<n;h++)p=o[h],(g=e[p])?(i[l++]=F(r?"(?!\\b)"+p+"(\\b|_)":p),i[l++]=g):s+=(s?"|":"")+p;return s&&(i[l++]=F(r?"(?!\\b)("+s+")(\\b|_)":"("+s+")"),i[l]=""),i}function E(e,r){for(let o=0,n=r.length;o<n&&(e=e.replace(r[o],r[o+1]),e);o+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let r="",o="";for(let n=0,i=e.length,s;n<i;n++)(s=e[n])!==o&&(r+=o=s);return r}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,r){e[r+"Async"]=function(){let o=this,n=arguments;var i=n[n.length-1];let s;return D(i)&&(s=i,delete n[n.length-1]),i=new Promise(function(l){setTimeout(function(){o.async=!0;let h=o[r].apply(o,n);o.async=!1,l(h)})}),s?(i.then(s),this):i}}function ma(e,r,o,n){let i=e.length,s=[],l,h,p=0;n&&(n=[]);for(let g=i-1;0<=g;g--){let f=e[g],_=f.length,w=v(),k=!l;for(let m=0;m<_;m++){let y=f[m],q=y.length;if(q)for(let H=0,R,A;H<q;H++)if(A=y[H],l){if(l[A]){if(!g){if(o)o--;else if(s[p++]=A,p===r)return s}(g||n)&&(w[A]=1),k=!0}if(n&&(h[A]=(R=h[A])?++R:R=1,R<i)){let B=n[R-2]||(n[R-2]=[]);B[B.length]=A}}else w[A]=1}if(n)l||(h=w);else if(!k)return[];l=w}if(n)for(let g=n.length-1,f,_;0<=g;g--){f=n[g],_=f.length;for(let w=0,k;w<_;w++)if(k=f[w],!l[k]){if(o)o--;else if(s[p++]=k,p===r)return s;l[k]=1}}return s}function na(e,r){let o=v(),n=v(),i=[];for(let s=0;s<e.length;s++)o[e[s]]=1;for(let s=0,l;s<r.length;s++){l=r[s];for(let h=0,p;h<l.length;h++)p=l[h],o[p]&&!n[p]&&(n[p]=1,i[i.length]=p)}return i}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,r,o){C(e)&&(e=e.query);let n=this.cache.get(e);return n||(n=this.search(e,r,o),this.cache.set(e,n)),n}J.prototype.set=function(e,r){if(!this.cache[e]){var o=this.h.length;for(o===this.l?delete this.cache[this.h[o-1]]:o++,--o;0<o;o--)this.h[o]=this.h[o-1];this.h[0]=e}this.cache[e]=r},J.prototype.get=function(e){let r=this.cache[e];if(this.l&&r&&(e=this.h.indexOf(e))){let o=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=o}return r};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,r,o,n,i,s){setTimeout(function(){let l=e(o,JSON.stringify(s));l&&l.then?l.then(function(){r.export(e,r,o,n,i+1)}):r.export(e,r,o,n,i+1)})}function K(e,r){if(!(this instanceof K))return new K(e);var o;if(e){x(e)?e=qa[e]:(o=e.preset)&&(e=Object.assign({},o[o],e)),o=e.charset;var n=e.lang;x(o)&&(o.indexOf(":")===-1&&(o+=":default"),o=G[o]),x(n)&&(n=ka[n])}else e={};let i,s,l=e.context||{};if(this.encode=e.encode||o&&o.encode||ia,this.register=r||v(),this.D=i=e.resolution||9,this.G=r=o&&o.G||e.tokenize||"strict",this.depth=r==="strict"&&l.depth,this.l=u(l.bidirectional),this.s=s=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=s?aa(i):v(),this.A=i=l.resolution||1,this.h=s?aa(i):v(),this.F=o&&o.F||e.rtl,this.H=(r=e.matcher||n&&n.H)&&fa(r,!1),this.J=(r=e.stemmer||n&&n.J)&&fa(r,!0),o=r=e.filter||n&&n.filter){o=r,n=v();for(let h=0,p=o.length;h<p;h++)n[o[h]]=1;o=n}this.filter=o,this.cache=(r=e.cache)&&new J(r)}t=K.prototype,t.append=function(e,r){return this.add(e,r,!0)},t.add=function(e,r,o,n){if(r&&(e||e===0)){if(!n&&!o&&this.register[e])return this.update(e,r);if(r=this.encode(r),n=r.length){let g=v(),f=v(),_=this.depth,w=this.D;for(let k=0;k<n;k++){let m=r[this.F?n-1-k:k];var i=m.length;if(m&&i>=this.B&&(_||!f[m])){var s=L(w,n,k),l="";switch(this.G){case"full":if(3<i){for(s=0;s<i;s++)for(var h=i;h>s;h--)if(h-s>=this.B){var p=L(w,n,k,i,s);l=m.substring(s,h),M(this,f,l,p,e,o)}break}case"reverse":if(2<i){for(h=i-1;0<h;h--)l=m[h]+l,l.length>=this.B&&M(this,f,l,L(w,n,k,i,h),e,o);l=""}case"forward":if(1<i){for(h=0;h<i;h++)l+=m[h],l.length>=this.B&&M(this,f,l,s,e,o);break}default:if(this.C&&(s=Math.min(s/this.C(r,m,k)|0,w-1)),M(this,f,m,s,e,o),_&&1<n&&k<n-1){for(i=v(),l=this.A,s=m,h=Math.min(_+1,n-k),i[s]=1,p=1;p<h;p++)if((m=r[this.F?n-1-k-p:k+p])&&m.length>=this.B&&!i[m]){i[m]=1;let y=this.l&&m>s;M(this,g,y?s:m,L(l+(n/2>l?0:1),n,k,h-1,p-1),e,o,y?m:s)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,r,o,n,i){return o&&1<e?r+(n||0)<=e?o+(i||0):(e-1)/(r+(n||0))*(o+(i||0))+1|0:0}function M(e,r,o,n,i,s,l){let h=l?e.h:e.map;(!r[o]||l&&!r[o][l])&&(e.s&&(h=h[n]),l?(r=r[o]||(r[o]=v()),r[l]=1,h=h[l]||(h[l]=v())):r[o]=1,h=h[o]||(h[o]=[]),e.s||(h=h[n]||(h[n]=[])),s&&h.indexOf(i)!==-1||(h[h.length]=i,e.m&&(e=e.register[i]||(e.register[i]=[]),e[e.length]=h)))}t.search=function(e,r,o){o||(!r&&C(e)?(o=e,e=o.query):C(r)&&(o=r));let n=[],i,s,l=0;if(o){r=o.limit,l=o.offset||0;var h=o.context;s=o.suggest}if(e&&(e=this.encode(e),i=e.length,1<i)){o=v();var p=[];for(let f=0,_=0,w;f<i;f++)if((w=e[f])&&w.length>=this.B&&!o[w])if(this.s||s||this.map[w])p[_++]=w,o[w]=1;else return n;e=p,i=e.length}if(!i)return n;r||(r=100),h=this.depth&&1<i&&h!==!1,o=0;let g;h?(g=e[0],o=1):1<i&&e.sort(ba);for(let f,_;o<i;o++){if(_=e[o],h?(f=sa(this,n,s,r,l,i===2,_,g),s&&f===!1&&n.length||(g=_)):f=sa(this,n,s,r,l,i===1,_),f)return f;if(s&&o===i-1){if(p=n.length,!p){if(h){h=0,o=-1;continue}return n}if(p===1)return ta(n[0],r,l)}}return ma(n,r,l,s)};function sa(e,r,o,n,i,s,l,h){let p=[],g=h?e.h:e.map;if(e.s||(g=ua(g,l,h,e.l)),g){let f=0,_=Math.min(g.length,h?e.A:e.D);for(let w=0,k=0,m,y;w<_&&!((m=g[w])&&(e.s&&(m=ua(m,l,h,e.l)),i&&m&&s&&(y=m.length,y<=i?(i-=y,m=null):(m=m.slice(i),i=0)),m&&(p[f++]=m,s&&(k+=m.length,k>=n))));w++);if(f){if(s)return ta(p,n,0);r[r.length]=p;return}}return!o&&p}function ta(e,r,o){return e=e.length===1?e[0]:[].concat.apply([],e),o||e.length>r?e.slice(o,o+r):e}function ua(e,r,o,n){return o?(n=n&&r>o,e=(e=e[n?r:o])&&e[n?o:r]):e=e[r],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,r){return this.remove(e).add(e,r)},t.remove=function(e,r){let o=this.register[e];if(o){if(this.m)for(let n=0,i;n<o.length;n++)i=o[n],i.splice(i.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(r||delete this.register[e],this.cache){r=this.cache;for(let n=0,i,s;n<r.h.length;n++)s=r.h[n],i=r.cache[s],i.indexOf(e)!==-1&&(r.h.splice(n--,1),delete r.cache[s])}}return this};function N(e,r,o,n,i){let s=0;if(e.constructor===Array)if(i)r=e.indexOf(r),r!==-1?1<e.length&&(e.splice(r,1),s++):s++;else{i=Math.min(e.length,o);for(let l=0,h;l<i;l++)(h=e[l])&&(s=N(h,r,o,n,i),n||s||delete e[l])}else for(let l in e)(s=N(e[l],r,o,n,i))||delete e[l];return s}t.searchCache=oa,t.export=function(e,r,o,n,i){let s,l;switch(i||(i=0)){case 0:if(s="reg",this.m){l=v();for(let h in this.register)l[h]=1}else l=this.register;break;case 1:s="cfg",l={doc:0,opt:this.s?1:0};break;case 2:s="map",l=this.map;break;case 3:s="ctx",l=this.h;break;default:return}return ra(e,r||this,o?o+"."+s:s,n,i,l),!0},t.import=function(e,r){if(r)switch(x(r)&&(r=JSON.parse(r)),e){case"cfg":this.s=!!r.opt;break;case"reg":this.m=!1,this.register=r;break;case"map":this.map=r;break;case"ctx":this.h=r}},la(K.prototype);function va(e){e=e.data;var r=self._index;let o=e.args;var n=e.task;switch(n){case"init":n=e.options||{},e=e.factory,r=n.encode,n.cache=!1,r&&r.indexOf("function")===0&&(n.encode=Function("return "+r)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(n),delete self.FlexSearch):self._index=new K(n);break;default:e=e.id,r=r[n].apply(r,o),postMessage(n==="search"?{id:e,msg:r}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var r;e?D(r=e.encode)&&(e.encode=r.toString()):e={},(r=(self||window)._factory)&&(r=r.toString());let o=self.exports,n=this;this.o=xa(r,o,e.worker),this.h=v(),this.o&&(o?this.o.on("message",function(i){n.h[i.id](i.msg),delete n.h[i.id]}):this.o.onmessage=function(i){i=i.data,n.h[i.id](i.msg),delete n.h[i.id]},this.o.postMessage({task:"init",factory:r,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let r=this,o=[].slice.call(arguments);var n=o[o.length-1];let i;return D(n)&&(i=n,o.splice(o.length-1,1)),n=new Promise(function(s){setTimeout(function(){r.h[++wa]=s,r.o.postMessage({task:e,id:wa,args:o})})}),i?(n.then(i),this):n}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var r=e.document||e.doc||e,o;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(o=r.key||r.id)&&S(o,this.A)||"id",this.m=u(e.fastupdate),this.C=(o=r.store)&&o!==!0&&[],this.store=o&&v(),this.I=(o=r.tag)&&S(o,this.A),this.l=o&&v(),this.cache=(o=e.cache)&&new J(o),e.cache=!1,this.o=e.worker,this.async=!1,o=v();let n=r.index||r.field||r;x(n)&&(n=[n]);for(let i=0,s,l;i<n.length;i++)s=n[i],x(s)||(l=s,s=s.field),l=C(l)?Object.assign({},e,l):e,this.o&&(o[s]=new O(l),o[s].o||(this.o=!1)),this.o||(o[s]=new K(l,this.register)),this.K[i]=S(s,this.A),this.h[i]=s;if(this.C)for(e=r.store,x(e)&&(e=[e]),r=0;r<e.length;r++)this.C[r]=S(e[r],this.A);this.index=o}function S(e,r){let o=e.split(":"),n=0;for(let i=0;i<o.length;i++)e=o[i],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(r[n]=!0),e&&(o[n++]=e);return n<o.length&&(o.length=n),1<n?o:o[0]}function T(e,r){if(x(r))e=e[r];else for(let o=0;e&&o<r.length;o++)e=e[r[o]];return e}function U(e,r,o,n,i){if(e=e[i],n===o.length-1)r[i]=e;else if(e)if(e.constructor===Array)for(r=r[i]=Array(e.length),i=0;i<e.length;i++)U(e,r,o,n,i);else r=r[i]||(r[i]=v()),i=o[++n],U(e,r,o,n,i)}function V(e,r,o,n,i,s,l,h){if(e=e[l])if(n===r.length-1){if(e.constructor===Array){if(o[n]){for(r=0;r<e.length;r++)i.add(s,e[r],!0,!0);return}e=e.join(" ")}i.add(s,e,h,!0)}else if(e.constructor===Array)for(l=0;l<e.length;l++)V(e,r,o,n,i,s,l,h);else l=r[++n],V(e,r,o,n,i,s,l,h)}t=Q.prototype,t.add=function(e,r,o){if(C(e)&&(r=e,e=T(r,this.key)),r&&(e||e===0)){if(!o&&this.register[e])return this.update(e,r);for(let n=0,i,s;n<this.h.length;n++)s=this.h[n],i=this.K[n],x(i)&&(i=[i]),V(r,i,this.A,0,this.index[s],e,i[0],o);if(this.I){let n=T(r,this.I),i=v();x(n)&&(n=[n]);for(let s=0,l,h;s<n.length;s++)if(l=n[s],!i[l]&&(i[l]=1,h=this.l[l]||(this.l[l]=[]),!o||h.indexOf(e)===-1)&&(h[h.length]=e,this.m)){let p=this.register[e]||(this.register[e]=[]);p[p.length]=h}}if(this.store&&(!o||!this.store[e])){let n;if(this.C){n=v();for(let i=0,s;i<this.C.length;i++)s=this.C[i],x(s)?n[s]=r[s]:U(r,n,s,0,s[0])}this.store[e]=n||r}}return this},t.append=function(e,r){return this.add(e,r,!0)},t.update=function(e,r){return this.remove(e).add(e,r)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var r=0;r<this.h.length&&(this.index[this.h[r]].remove(e,!this.o),!this.m);r++);if(this.I&&!this.m)for(let o in this.l){r=this.l[o];let n=r.indexOf(e);n!==-1&&(1<r.length?r.splice(n,1):delete this.l[o])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,r,o,n){o||(!r&&C(e)?(o=e,e=o.query):C(r)&&(o=r,r=0));let i=[],s=[],l,h,p,g,f,_,w=0;if(o)if(o.constructor===Array)p=o,o=null;else{if(p=(l=o.pluck)||o.index||o.field,g=o.tag,h=this.store&&o.enrich,f=o.bool==="and",r=o.limit||100,_=o.offset||0,g&&(x(g)&&(g=[g]),!e)){for(let m=0,y;m<g.length;m++)(y=ya.call(this,g[m],r,_,h))&&(i[i.length]=y,w++);return w?i:[]}x(p)&&(p=[p])}p||(p=this.h),f=f&&(1<p.length||g&&1<g.length);let k=!n&&(this.o||this.async)&&[];for(let m=0,y,q,H;m<p.length;m++){let R;if(q=p[m],x(q)||(R=q,q=q.field),k)k[m]=this.index[q].searchAsync(e,r,R||o);else{if(n?y=n[m]:y=this.index[q].search(e,r,R||o),H=y&&y.length,g&&H){let A=[],B=0;f&&(A[0]=[y]);for(let X=0,te,j;X<g.length;X++)te=g[X],(H=(j=this.l[te])&&j.length)&&(B++,A[A.length]=f?[j]:j);B&&(y=f?ma(A,r||100,_||0):na(y,A),H=y.length)}if(H)s[w]=q,i[w++]=y;else if(f)return[]}}if(k){let m=this;return new Promise(function(y){Promise.all(k).then(function(q){y(m.search(e,r,o,q))})})}if(!w)return[];if(l&&(!h||!this.store))return i[0];for(let m=0,y;m<s.length;m++){if(y=i[m],y.length&&h&&(y=za.call(this,y)),l)return y;i[m]={field:s[m],result:y}}return i};function ya(e,r,o,n){let i=this.l[e],s=i&&i.length-o;if(s&&0<s)return(s>r||o)&&(i=i.slice(o,o+r)),n&&(i=za.call(this,i)),{tag:e,result:i}}function za(e){let r=Array(e.length);for(let o=0,n;o<e.length;o++)n=e[o],r[o]={id:n,doc:this.store[n]};return r}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,r){return this.store[e]=r,this},t.searchCache=oa,t.export=function(e,r,o,n,i){if(i||(i=0),n||(n=0),n<this.h.length){let s=this.h[n],l=this.index[s];r=this,setTimeout(function(){l.export(e,r,i?s.replace(":","-"):"",n,i++)||(n++,i=1,r.export(e,r,s,n,i))})}else{let s;switch(i){case 1:o="tag",s=this.l;break;case 2:o="store",s=this.store;break;default:return}ra(e,this,o,n,i,s)}},t.import=function(e,r){if(r)switch(x(r)&&(r=JSON.parse(r)),e){case"tag":this.l=r;break;case"reg":this.m=!1,this.register=r;for(let n=0,i;n<this.h.length;n++)i=this.index[this.h[n]],i.register=r,i.m=!1;break;case"store":this.store=r;break;default:e=e.split(".");let o=e[0];e=e[1],o&&e&&this.index[o].import(e,r)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var r=e;return r.normalize&&(r=r.normalize("NFD").replace(ea,"")),ca.call(this,r.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let r=[];if(e){let o=e.split(Fa),n=o.length;for(let i=0,s,l=0;i<n;i++)if((e=o[i])&&(!this.filter||!this.filter[e])){s=e[0];let h=Ga[s]||s,p=h;for(let g=1;g<e.length;g++){s=e[g];let f=Ga[s]||s;f&&f!==p&&(h+=f,p=f)}r[l++]=h}}return r}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,r){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),r||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,r){G[e]=r},registerLanguage:function(e,r){ka[e]=r}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var oe=he(re());var $=document.getElementById("search__text"),z=document.getElementById("search__suggestions");$!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),$.focus()):e.key==="Escape"&&($.blur(),z.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{z.contains(e.target)||z.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(z.classList.contains("search__suggestions--hidden"))return;let o=[...z.querySelectorAll("a")];if(o.length===0)return;let n=o.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let i=n+1<o.length?n+1:n;o[i].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=n>0?n-1:0,o[nextIndex].focus())});(function(){let e=new oe.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/blog/deploy-azure-virtual-desktop-avd-using-terraform-and-azure-active-directory-domain-services-aadds/",title:"Deploy Azure Virtual Desktop (AVD) Using Terraform and Azure Active Directory Domain Services (AADDS)",description:`With Azure Virtual Desktop (AVD), you can deliver secure Windows 11 desktops and environments anywhere. It&rsquo;s pretty easy to deploy and scale. You can provide a coherent user experience from any end-user device and reduce costs by leveraging Windows 11 multi-session licensing. This tutorial will guide you through setting up AVD with AADDS using Terraform.
`,content:`With Azure Virtual Desktop (AVD), you can deliver secure Windows 11 desktops and environments anywhere. It&rsquo;s pretty easy to deploy and scale. You can provide a coherent user experience from any end-user device and reduce costs by leveraging Windows 11 multi-session licensing. This tutorial will guide you through setting up AVD with AADDS using Terraform.
As always, all the code is available on my GitHub.
Prerequisites #  Besides an active Azure subscription and Terraform configured on your workstation, Azure Active Directory Domain Services (AADDS) are required. Check out my previous post on setting up AADDS with Terraform if you haven&rsquo;t already! Some Terraform resources in this guide, e.g., the network peerings and AADDS domain-join (AADDS-join) VM extension, depend on the AADDS resources from that post.
Do You Know What&rsquo;s Exciting? #  It&rsquo;s possible to just Azure AD-join (AAD-join) AVD session hosts, eliminating the requirement to use AADDS or on-premise AD DS and reduce the costs and complexity of AVD deployments even more. Unfortunately, it&rsquo;s not yet fully production-ready because FSLogix profile support for AAD-joined AVD VMs is only in public preview. Currently, using AAD authentication with Azure Files still requires hybrid identities. But it&rsquo;s nice that AVD is one step closer to being a cloud-only solution. I can&rsquo;t wait to terraformify all of it! Stay tuned because I&rsquo;ll post about it as soon as things are generally available.
Overview #  We&rsquo;ll deploy AADDS and AVD resources to separate virtual networks and resource groups. It is called a hub-spoke network topology, a typical approach to organize large-scale networks. The hub (aadds-vnet), the central connectivity point, typically contains other services besides AADDS. E.g., a VPN gateway connecting your on-premises network to the Azure cloud. Azure Bastion or Azure Firewall are also services that might reside in the hub network. Spoke networks (avd-vnet) contain isolated workloads using network peerings to connect to the hub.
Network Resources #  Create the avd-rg resource group and add the avd-vnet spoke network to it. The network uses the AADDS domain controllers (DCs) as dns_servers:
resource &#34;azurerm_resource_group&#34; &#34;avd&#34; { name = &#34;avd-rg&#34; location = &#34;Switzerland North&#34; }# Network Resources  resource &#34;azurerm_virtual_network&#34; &#34;avd&#34; { name = &#34;avd-vnet&#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name address_space = [&#34;10.10.0.0/16&#34;] dns_servers = azurerm_active_directory_domain_service.aadds.initial_replica_set.0.domain_controller_ip_addresses } resource &#34;azurerm_subnet&#34; &#34;avd&#34; { name = &#34;avd-snet&#34; resource_group_name = azurerm_resource_group.avd.name virtual_network_name = azurerm_virtual_network.avd.name address_prefixes = [&#34;10.10.0.0/24&#34;] } To give AVD VMs line of sight of AADDS, we need to add the following network peerings:
resource &#34;azurerm_virtual_network_peering&#34; &#34;aadds_to_avd&#34; { name = &#34;hub-to-avd-peer&#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name remote_virtual_network_id = azurerm_virtual_network.avd.id } resource &#34;azurerm_virtual_network_peering&#34; &#34;avd_to_aadds&#34; { name = &#34;avd-to-aadds-peer&#34; resource_group_name = azurerm_resource_group.avd.name virtual_network_name = azurerm_virtual_network.avd.name remote_virtual_network_id = azurerm_virtual_network.aadds.id } Host Pool #   A host pool is a collection of Azure virtual machines that register to Azure Virtual Desktop as session hosts when you run the Azure Virtual Desktop agent. All session host virtual machines in a host pool should be sourced from the same image for a consistent user experience.
 We add the AVD host pool and the registration info. We&rsquo;ll later register the session hosts via VM extension to the host pool using the token from the registration info:
locals {# Switzerland North is not supported  avd_location = &#34;West Europe&#34; } resource &#34;azurerm_virtual_desktop_host_pool&#34; &#34;avd&#34; { name = &#34;avd-vdpool&#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name type = &#34;Pooled&#34; load_balancer_type = &#34;BreadthFirst&#34; friendly_name = &#34;AVD Host Pool using AADDS&#34; } resource &#34;time_rotating&#34; &#34;avd_registration_expiration&#34; {# Must be between 1 hour and 30 days  rotation_days = 29 } resource &#34;azurerm_virtual_desktop_host_pool_registration_info&#34; &#34;avd&#34; { hostpool_id = azurerm_virtual_desktop_host_pool.avd.id expiration_date = time_rotating.avd_registration_expiration.rotation_rfc3339 } I deploy my AADDS and AVD resources to the Switzerland North region. However, I have to deploy AVD service resources to West Europe because the AVD service isn&rsquo;t available in all regions.
To get the latest supported regions, re-register the AVD resource provider:
 Select your subscription under Subscriptions in the Azure Portal. Select the Resource Provider menu. Re-register Microsoft.DesktopVirtualization.  Workspace and App Group #  Next, we create a workspace and add an app group. Two types of app groups exist:
 Desktop: full desktop RemoteApp: individual apps  Adding the following gives AVD users the full desktop experience:
resource &#34;azurerm_virtual_desktop_workspace&#34; &#34;avd&#34; { name = &#34;avd-vdws&#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name } resource &#34;azurerm_virtual_desktop_application_group&#34; &#34;avd&#34; { name = &#34;desktop-vdag&#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name type = &#34;Desktop&#34; host_pool_id = azurerm_virtual_desktop_host_pool.avd.id } resource &#34;azurerm_virtual_desktop_workspace_application_group_association&#34; &#34;avd&#34; { workspace_id = azurerm_virtual_desktop_workspace.avd.id application_group_id = azurerm_virtual_desktop_application_group.avd.id } Session Hosts #  Let&rsquo;s add two session hosts to the AVD host pool. To be able to adjust the amount of VMs inside the host pool later, we define a variable:
variable &#34;avd_host_pool_size&#34; { type = number description = &#34;Number of session hosts to add to the AVD host pool.&#34; } Next, we add the VM NICs for the session hosts:
resource &#34;azurerm_network_interface&#34; &#34;avd&#34; { count = var.avd_host_pool_size name = &#34;avd-nic-\${count.index}&#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name ip_configuration { name = &#34;avd-ipconf&#34; subnet_id = azurerm_subnet.avd.id private_ip_address_allocation = &#34;Dynamic&#34; } } After, we add the session hosts:
resource &#34;random_password&#34; &#34;avd_local_admin&#34; { length = 64 } resource &#34;random_id&#34; &#34;avd&#34; { count = length(azurerm_network_interface.avd) byte_length = 4 } resource &#34;azurerm_windows_virtual_machine&#34; &#34;avd&#34; { count = length(random_id.avd) name = &#34;avd-vm-\${count.index}-\${random_id.avd[count.index].hex}&#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name size = &#34;Standard_D4s_v4&#34; license_type = &#34;Windows_Client&#34; admin_username = &#34;avd-local-admin&#34; admin_password = random_password.avd_local_admin.result network_interface_ids = [azurerm_network_interface.avd[count.index].id] os_disk { caching = &#34;ReadWrite&#34; storage_account_type = &#34;Premium_LRS&#34; } source_image_reference { publisher = &#34;MicrosoftWindowsDesktop&#34; offer = &#34;windows-11&#34; sku = &#34;win11-21h2-avd&#34; version = &#34;latest&#34; } } To ensure the session hosts utilize the licensing benefits available with AVD, we select Windows_Client as license_type value.
The reason we append a random number to the VM name is to prevent name conflicts with dangling host pool registrations.
Understanding VM Extensions #  To figure out the required VM extensions, I used the AVD wizard of the Azure Portal. During the review + create step, I downloaded the ARM template and reverse-engineered it.
Sometimes, creating complex deployments via Azure Portal feels like magic. Backtracking the generated ARM templates is something I like to do to get a deeper understanding of what&rsquo;s happening under the hood. It&rsquo;s usually my initial step when trying to terraformify something for the first time that I can&rsquo;t find good examples of elsewhere.
You can find the AVD ARM templates on official Azure GitHub github.com/Azure/RDS-Templates/ARM-wvd-templates. The VM templates reside in the nestedtemplates directory containing the VM extension resources that we want to replicate with Terraform:
{ &#34;apiVersion&#34;: &#34;2018-10-01&#34;, &#34;type&#34;: &#34;Microsoft.Compute/virtualMachines/extensions&#34;, &#34;name&#34;: &#34;[concat(parameters(&#39;rdshPrefix&#39;), add(copyindex(), parameters(&#39;vmInitialNumber&#39;)), &#39;/&#39;, &#39;Microsoft.PowerShell.DSC&#39;)]&#34;, &#34;location&#34;: &#34;[parameters(&#39;location&#39;)]&#34;, &#34;dependsOn&#34;: [ &#34;rdsh-vm-loop&#34; ], &#34;copy&#34;: { &#34;name&#34;: &#34;rdsh-dsc-loop&#34;, &#34;count&#34;: &#34;[parameters(&#39;rdshNumberOfInstances&#39;)]&#34; }, &#34;properties&#34;: { &#34;publisher&#34;: &#34;Microsoft.Powershell&#34;, &#34;type&#34;: &#34;DSC&#34;, &#34;typeHandlerVersion&#34;: &#34;2.73&#34;, &#34;autoUpgradeMinorVersion&#34;: true, &#34;settings&#34;: { &#34;modulesUrl&#34;: &#34;[parameters(&#39;artifactsLocation&#39;)]&#34;, &#34;configurationFunction&#34;: &#34;Configuration.ps1\\\\AddSessionHost&#34;, &#34;properties&#34;: { &#34;hostPoolName&#34;: &#34;[parameters(&#39;hostpoolName&#39;)]&#34;, &#34;registrationInfoToken&#34;: &#34;[parameters(&#39;hostpoolToken&#39;)]&#34;, &#34;aadJoin&#34;: &#34;[parameters(&#39;aadJoin&#39;)]&#34;, &#34;sessionHostConfigurationLastUpdateTime&#34;: &#34;[parameters(&#39;SessionHostConfigurationVersion&#39;)]&#34; } } } }, { &#34;condition&#34;: &#34;[not(parameters(&#39;aadJoin&#39;))]&#34;, &#34;apiVersion&#34;: &#34;2018-10-01&#34;, &#34;type&#34;: &#34;Microsoft.Compute/virtualMachines/extensions&#34;, &#34;name&#34;: &#34;[concat(parameters(&#39;rdshPrefix&#39;), add(copyindex(), parameters(&#39;vmInitialNumber&#39;)), &#39;/&#39;, &#39;joindomain&#39;)]&#34;, &#34;location&#34;: &#34;[parameters(&#39;location&#39;)]&#34;, &#34;dependsOn&#34;: [ &#34;rdsh-dsc-loop&#34; ], &#34;copy&#34;: { &#34;name&#34;: &#34;rdsh-domain-join-loop&#34;, &#34;count&#34;: &#34;[parameters(&#39;rdshNumberOfInstances&#39;)]&#34; }, &#34;properties&#34;: { &#34;publisher&#34;: &#34;Microsoft.Compute&#34;, &#34;type&#34;: &#34;JsonADDomainExtension&#34;, &#34;typeHandlerVersion&#34;: &#34;1.3&#34;, &#34;autoUpgradeMinorVersion&#34;: true, &#34;settings&#34;: { &#34;name&#34;: &#34;[variables(&#39;domain&#39;)]&#34;, &#34;ouPath&#34;: &#34;[parameters(&#39;ouPath&#39;)]&#34;, &#34;user&#34;: &#34;[parameters(&#39;administratorAccountUsername&#39;)]&#34;, &#34;restart&#34;: &#34;true&#34;, &#34;options&#34;: &#34;3&#34; }, &#34;protectedSettings&#34;: { &#34;password&#34;: &#34;[parameters(&#39;administratorAccountPassword&#39;)]&#34; } } }, However, the default parameters of the ARM template downloaded from the Azure Portal differ from the values found on GitHub, e.g., modulesParameter:
 GitHub: https://raw.githubusercontent.com/Azure/RDS-Templates/master/ARM-wvd-templates/DSC/Configuration.zip Azure: https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-20-2022.zip  It seems that Microsoft periodically releases the Configuration.zip to the galleryartifacts container of the wvdportalstorageblob storage account. To peek inside the container, we can use the List Blobs operation of the Blob Service REST API.
The URLs that the Azure Portal uses sometimes change. At the time of writing, it uses the Configuration_01-20-2022.zip file despite Configuration_02-23-2022.zip being available.
AADDS-join the VMs #  When AADDS-joining a computer, it will be added to the built-in AADDS Computers Organizational Unit (OU) of the domain by default. To add the VM to a different OU, we can optionally specify the OU path during domain-join. Create an optional variable:
variable &#34;avd_ou_path&#34; { type = string description = &#34;OU path used to AADDS domain-join AVD session hosts.&#34; default = &#34;&#34; } We then AADDS-join the session hosts with the JsonADDomainExtension VM extension:
resource &#34;azurerm_virtual_machine_extension&#34; &#34;avd_aadds_join&#34; { count = length(azurerm_windows_virtual_machine.avd) name = &#34;aadds-join-vmext&#34; virtual_machine_id = azurerm_windows_virtual_machine.avd[count.index].id publisher = &#34;Microsoft.Compute&#34; type = &#34;JsonADDomainExtension&#34; type_handler_version = &#34;1.3&#34; auto_upgrade_minor_version = true settings = &lt;&lt;-SETTINGS { &#34;Name&#34;: &#34;\${azurerm_active_directory_domain_service.aadds.domain_name}&#34;, &#34;OUPath&#34;: &#34;\${var.avd_ou_path}&#34;, &#34;User&#34;: &#34;\${azuread_user.dc_admin.user_principal_name}&#34;, &#34;Restart&#34;: &#34;true&#34;, &#34;Options&#34;: &#34;3&#34; } SETTINGS protected_settings = &lt;&lt;-PROTECTED_SETTINGS { &#34;Password&#34;: &#34;\${random_password.dc_admin.result}&#34; } PROTECTED_SETTINGS lifecycle { ignore_changes = [settings, protected_settings] } depends_on = [ azurerm_virtual_network_peering.aadds_to_avd, azurerm_virtual_network_peering.avd_to_aadds ] } We have to ensure that the session hosts have line of sight to the AADDS DCs. To do that, we add the network peering resources to the depends_on list of the VM extension.
After a VM has been AADDS-joined, it doesn&rsquo;t make sense to join it again when the settings or protected_settings of the VM extension change, so we ignore_changes of these properties.
Register VMs to the Host Pool #  First, let&rsquo;s add a variable containing the URL to the zip file containing the DSC configuration, making it easier to update it in the future:
variable &#34;avd_register_session_host_modules_url&#34; { type = string description = &#34;URL to .zip file containing DSC configuration to register AVD session hosts to AVD host pool.&#34; default = &#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_02-23-2022.zip&#34; } Then, we register the session hosts to the host pool with the DSC VM extension:
resource &#34;azurerm_virtual_machine_extension&#34; &#34;avd_register_session_host&#34; { count = length(azurerm_windows_virtual_machine.avd) name = &#34;register-session-host-vmext&#34; virtual_machine_id = azurerm_windows_virtual_machine.avd[count.index].id publisher = &#34;Microsoft.Powershell&#34; type = &#34;DSC&#34; type_handler_version = &#34;2.73&#34; settings = &lt;&lt;-SETTINGS { &#34;modulesUrl&#34;: &#34;\${var.avd_register_session_host_modules_url}&#34;, &#34;configurationFunction&#34;: &#34;Configuration.ps1\\\\AddSessionHost&#34;, &#34;properties&#34;: { &#34;hostPoolName&#34;: &#34;\${azurerm_virtual_desktop_host_pool.avd.name}&#34;, &#34;aadJoin&#34;: false } } SETTINGS protected_settings = &lt;&lt;-PROTECTED_SETTINGS { &#34;properties&#34;: { &#34;registrationInfoToken&#34;: &#34;\${azurerm_virtual_desktop_host_pool_registration_info.avd.token}&#34; } } PROTECTED_SETTINGS lifecycle { ignore_changes = [settings, protected_settings] } depends_on = [azurerm_virtual_machine_extension.avd_aadds_join] } Again, we ignore_changes to the settings and protected_settings properties.
Role-based Access Control (RBAC) #  Let&rsquo;s create a group in AAD that authorizes its members to access the AVD application group we created earlier. To do so, we create a group and assign the AAD built-in Desktop Virtualization User role to it:
data &#34;azurerm_role_definition&#34; &#34;desktop_virtualization_user&#34; { name = &#34;Desktop Virtualization User&#34; } resource &#34;azuread_group&#34; &#34;avd_users&#34; { display_name = &#34;AVD Users&#34; security_enabled = true } resource &#34;azurerm_role_assignment&#34; &#34;avd_users_desktop_virtualization_user&#34; { scope = azurerm_virtual_desktop_application_group.avd.id role_definition_id = data.azurerm_role_definition.desktop_virtualization_user.id principal_id = azuread_group.avd_users.id } Assuming that we want to authorize users that already exist within our AAD, we create a variable containing the UPNs of these users:
variable &#34;avd_user_upns&#34; { type = list(string) description = &#34;List of user UPNs authorized to access AVD.&#34; default = [] } We are able then query those users with Terraform and add them to the group like this:
data &#34;azuread_user&#34; &#34;avd_users&#34; { for_each = toset(var.avd_user_upns) user_principal_name = each.key } resource &#34;azuread_group_member&#34; &#34;avd_users&#34; { for_each = data.azuread_user.avd_users group_object_id = azuread_group.avd_users.id member_object_id = each.value.id } What&rsquo;s Next? #  Great! We successfully created an AVD environment with Terraform. Test it by logging into one of the available AVD clients.
I&rsquo;ll write about creating custom AVD images with Packer next and follow it up by showing you how to configure FSLogix user profiles on your AADDS-joined AVD session hosts. Stay tuned!
`}).add({id:1,href:"/blog/set-up-azure-active-directory-domain-services-aadds-with-terraform-updated/",title:"Set Up Azure Active Directory Domain Services (AADDS) With Terraform",description:`I wanted to revisit this topic for a while because the previous guide I wrote about setting up Azure Active Directory Domain Services (AADDS) with Terraform is outdated. However, the article still attracts around 100 visitors per month. People also keep downloading the deprecated Terraform module I created. Time to set things right!
`,content:`I wanted to revisit this topic for a while because the previous guide I wrote about setting up Azure Active Directory Domain Services (AADDS) with Terraform is outdated. However, the article still attracts around 100 visitors per month. People also keep downloading the deprecated Terraform module I created. Time to set things right!
With v2.69.0 of the official Terraform azurerm provider released, the active_directory_domain_service resource is now available. In this post, I&rsquo;ll briefly walk you through the required steps of setting up AADDS. See also the official Microsoft documentation for more details.
I also published the code to a sample GitHub repo.
What Are Azure Active Directory Domain Services? #  Bringing traditional Active Directory Domain Services (AD DS) to the cloud, typically required to set up, secure, and maintain domain controllers (DCs). Azure Active Directory Domain Services (AADDS or Azure AD DS) is a Microsoft-managed solution, providing a subset of traditional AD DS features without the need to self-manage DCs. One such service that requires AD DS features is Azure Virtual Desktop (AVD).
Prerequisites #  Before getting started, you need the following things:
 Active Azure subscription Azure Active Directory (Azure AD / AAD) tenant  Service Principal #  First, create the service principal for the Domain Controller Services published application. In public Azure, the ID is 2565bd9d-da50-47d4-8b85-4c97f669dc36. For other clouds the value is 6ba9a5d4-8456-4118-b521-9c5ca10cdf84.
resource &#34;azuread_service_principal&#34; &#34;aadds&#34; { application_id = &#34;2565bd9d-da50-47d4-8b85-4c97f669dc36&#34; } If the service principal already exists, the following error occurs:
Error: A resource with the ID &#34;11111111-1111-1111-1111-111111111111&#34; already exists with azuread_service_principal.aadds, on aadds.tf line 2, in resource &#34;azuread_service_principal&#34; &#34;aadds&#34;: 2: resource &#34;azuread_service_principal&#34; &#34;aadds&#34; { To be managed via Terraform, this resource needs to be imported into the State. Please see the resource documentation for &#34;azuread_service_principal&#34; for more information. Import the service principal with the following command:
terraform import azuread_service_principal.aadds 11111111-1111-1111-1111-111111111111 Note that 11111111-1111-1111-1111-111111111111 is the Object ID and not the Application ID.
Microsoft.AAD Resource Provider Registration #  To use AADDS, register the Microsoft.AAD resource provider:
resource &#34;azurerm_resource_provider_registration&#34; &#34;aadds&#34; { name = &#34;Microsoft.AAD&#34; } If the provider is already registered, you can import it into Terraform with the following command:
terraform import azurerm_resource_provider_registration.aadds /subscriptions/00000000-0000-0000-0000-000000000000/providers/Microsoft.AAD DC Admin Group and User #  Next, create an Azure AD group for users administering the AADDS domain and add an admin.
resource &#34;azuread_group&#34; &#34;dc_admins&#34; { display_name = &#34;AAD DC Administrators&#34; description = &#34;AADDS Administrators&#34; members = [azuread_user.dc_admin.object_id] security_enabled = true } resource &#34;random_password&#34; &#34;dc_admin&#34; { length = 64 } resource &#34;azuread_user&#34; &#34;dc_admin&#34; { user_principal_name = &#34;dc-admin@example.com&#34; display_name = &#34;AADDS DC Administrator&#34; password = random_password.dc_admin.result } Resource Group #  Add the resource group for AADDS resources:
resource &#34;azurerm_resource_group&#34; &#34;aadds&#34; { name = &#34;aadds-rg&#34; location = &#34;Switzerland North&#34; } Network Resources #  Add the virtual network and subnet next.
resource &#34;azurerm_virtual_network&#34; &#34;aadds&#34; { name = &#34;aadds-vnet&#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name address_space = [&#34;10.0.0.0/16&#34;] } resource &#34;azurerm_subnet&#34; &#34;aadds&#34; { name = &#34;aadds-snet&#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [&#34;10.0.0.0/24&#34;] } To lock down access to the managed domain, add the following network security group. The AllowRD and AllowPSRemoting rules allow the Azure platform to monitor, manage, and update the managed domain:
resource &#34;azurerm_network_security_group&#34; &#34;aadds&#34; { name = &#34;aadds-nsg&#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name security_rule { name = &#34;AllowRD&#34; priority = 201 direction = &#34;Inbound&#34; access = &#34;Allow&#34; protocol = &#34;Tcp&#34; source_port_range = &#34;*&#34; destination_port_range = &#34;3389&#34; source_address_prefix = &#34;CorpNetSaw&#34; destination_address_prefix = &#34;*&#34; } security_rule { name = &#34;AllowPSRemoting&#34; priority = 301 direction = &#34;Inbound&#34; access = &#34;Allow&#34; protocol = &#34;Tcp&#34; source_port_range = &#34;*&#34; destination_port_range = &#34;5986&#34; source_address_prefix = &#34;AzureActiveDirectoryDomainServices&#34; destination_address_prefix = &#34;*&#34; } } resource azurerm_subnet_network_security_group_association &#34;aadds&#34; { subnet_id = azurerm_subnet.aadds.id network_security_group_id = azurerm_network_security_group.aadds.id } When you enable secure LDAP, it&rsquo;s recommended to create an additional security rule to restrict inbound LDAPS access to specific IP addresses to protect the managed domain from brute force attacks:
security_rule { name = &#34;AllowLDAPS&#34; priority = 401 direction = &#34;Inbound&#34; access = &#34;Allow&#34; protocol = &#34;Tcp&#34; source_port_range = &#34;*&#34; destination_port_range = &#34;636&#34; source_address_prefix = &#34;&lt;Authorized LDAPS IPs&gt;&#34; destination_address_prefix = &#34;*&#34; } AADDS Managed Domain #  Finally, deploy the AADDS managed domain:
resource &#34;azurerm_active_directory_domain_service&#34; &#34;aadds&#34; { name = &#34;aadds&#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name domain_name = &#34;aadds.example.com&#34; sku = &#34;Standard&#34; initial_replica_set { subnet_id = azurerm_subnet.aadds.id } notifications { additional_recipients = [&#34;alice@example.com&#34;, &#34;bob@example.com&#34;] notify_dc_admins = true notify_global_admins = true } security { sync_kerberos_passwords = true sync_ntlm_passwords = true sync_on_prem_passwords = true } depends_on = [ azuread_service_principal.aadds, azurerm_resource_provider_registration.aadds, azurerm_subnet_network_security_group_association.aadds, ] } Run terraform apply to deploy everything. It takes an hour to complete. Please share your thoughts with me in the comments or on Twitter!
`}).add({id:2,href:"/blog/percent-encoding-with-the-hugo-urlquery-function/",title:"Percent-encoding with the Hugo `urlquery` Function",description:`I recently tried to percent-encode (or URL-encode) strings in Hugo. Percent-encoding is used to encode data in query strings of URLs because they must only contain ASCII characters. There is a Hugo built-in querify function to transform key-value pairs to query strings. But my requirement was to URL-encode a single string value and not build a query string from key-value pairs.
`,content:`I recently tried to percent-encode (or URL-encode) strings in Hugo. Percent-encoding is used to encode data in query strings of URLs because they must only contain ASCII characters. There is a Hugo built-in querify function to transform key-value pairs to query strings. But my requirement was to URL-encode a single string value and not build a query string from key-value pairs.
My goal is to allow users of the Hugo theme to add social media sharing buttons through the site configuration like this:
[params] [[params.socialShare]] formatString = &#34;https://www.facebook.com/sharer.php?u={url}&#34; [[params.socialShare]] formatString = &#34;https://reddit.com/submit?url={url}&amp;title={title}&#34; The formatString contains {url} and {title} placeholders. The following is a simplified sharing button implementation using the format strings where placeholders are substituted with page-level variable values during Hugo build-time:
{{ $href := .formatString }} {{ $href := replace $href &#34;{url}&#34; .Permalink }} {{ $href := replace $href &#34;{title}&#34; .Title }} &lt;a href=&#34;{{ $href | safeURL }}&#34;&gt;Share Me!&lt;/a&gt; The issue here is that .Permalink and .Title must be percent-encoded. As I&rsquo;ve searched the web on how to do this with Hugo, I only found unsolved forum posts:
 About htmlEscape: transform https into https%3A%2F%2F URL encoding (percent encoding) with Hugo? Is there a url encoder function in Hugo?  I couldn&rsquo;t believe this was an unsolved mystery and assumed I was just too dumb to read the docs. Hugo was written in Go, so I looked into how to do percent-encoding in Go: with the url.QueryEscape() function from the net/url package.
Searching the Hugo code base for url.QueryEscape leads to exactly one result. It turns out I discovered an undocumented, built-in Hugo function called urlquery. The following:
{{ urlquery &#34;https://schnerring.net&#34; }} &hellip;returns the result: https%3A%2F%2Fschnerring.net. Here is the fixed version of the sharing button:
{{ $href := .formatString }} {{ $href := replace $href &#34;{url}&#34; (urlquery .Permalink) }} {{ $href := replace $href &#34;{title}&#34; (urlquery .Title) }} &lt;a href=&#34;{{ $href | safeURL }}&#34;&gt;Share Me!&lt;/a&gt; The full social sharing button implementation is available on my GitHub.
`}).add({id:3,href:"/blog/reduce-storage-costs-when-deploying-azure-kubernetes-service-clusters-with-terraform/",title:"Reduce Storage Costs when Deploying Azure Kubernetes Service Clusters with Terraform",description:`I use the Azure Kubernetes Service (AKS) to host a few services, like Synapse, Remark42, and Plausible Analytics. As of yet, none of these services require much computing power, so I chose a small VM size for the AKS node: Standard_B2ms. I recently analyzed my Azure costs and found out that I could have saved more than a hundred bucks in the past months if I had been more diligent.
`,content:`I use the Azure Kubernetes Service (AKS) to host a few services, like Synapse, Remark42, and Plausible Analytics. As of yet, none of these services require much computing power, so I chose a small VM size for the AKS node: Standard_B2ms. I recently analyzed my Azure costs and found out that I could have saved more than a hundred bucks in the past months if I had been more diligent.
What Storage Does AKS Use? #  Besides the underlying storage of Persistent Volumes Kubernetes deployments use, there&rsquo;s also the node itself requiring an OS disk. For OS disks, two types exist: managed and ephemeral. The Microsoft docs note the following on ephemeral OS disks:
 By default, Azure automatically replicates the operating system disk for an virtual machine to Azure storage to avoid data loss should the VM need to be relocated to another host. However, since containers aren&rsquo;t designed to have local state persisted, this behavior offers limited value while providing some drawbacks, including slower node provisioning and higher read/write latency.
By contrast, ephemeral OS disks are stored only on the host machine, just like a temporary disk. This provides lower read/write latency, along with faster node scaling and cluster upgrades.
Like the temporary disk, an ephemeral OS disk is included in the price of the virtual machine, so you incur no additional storage costs.
 I created my cluster in 2021, and ephemeral OS disks have been generally available since November 2020 and B-series VMs support ephemeral OS disks. However, the cost analysis in the Azure Portal shows that I&rsquo;m incurring storage costs for my cluster&rsquo;s managed OS disks. The following screenshot shows that I paid CHF 24.43 in October 2021.
   I pay CHF 33.50 per month for the Standard_B2ms VM instance, which the screenshot doesn&rsquo;t correctly display. To save around 70% compared to pay-as-you-go prices, I use Azure Reserved Virtual Machine Instances, which Microsoft bills separately. So why am I paying for a managed OS disk that almost doubles the operating cost of my AKS cluster? Well, because I made a mistake.
Using Terraform to Deploy AKS #  I use Terraform to manage my AKS cluster. The azurerm_kubernetes_cluster Terraform resource features the os_disk_type option:
 (Optional) The type of disk which should be used for the Operating System. Possible values are Ephemeral and Managed. Defaults to Managed. Changing this forces a new resource to be created.
 Here it is! I didn&rsquo;t explicitly set it to Ephemeral when I initially configured my cluster over six months ago. An expensive mistake!
Update on Ephemeral OS Disk Support for B-series VMs #  As someone correctly pointed out in the comments, B-series VMs don&rsquo;t support ephemeral OS disks. My tests with the Azure CLI and Terraform confirm his findings. The information from the official Microsoft documentation on this is inconsistent. Someone on GitHub mentioned a query from the Microsoft docs to determine VM types supporting ephemeral OS disks. Interestingly, the query result suggests B-series VMs support ephemeral OS storage.
$vmSizes = Get-AzComputeResourceSku | where { $_.ResourceType -eq &#39;virtualMachines&#39; -and $_.Locations.Contains(&#39;CentralUSEUAP&#39;) } foreach ($vmSize in $vmSizes) { foreach ($capability in $vmSize.capabilities) { if ($capability.Name -eq &#39;EphemeralOSDiskSupported&#39; -and $capability.Value -eq &#39;true&#39;) { $vmSize } } } Due to these limitations with B-series VMs, right now, it&rsquo;s more efficient to run general-purpose D-series VMs instead.
`}).add({id:4,href:"/blog/use-mullvad-port-forwarding-to-connect-to-your-opnsense-home-network-with-wireguard/",title:"Use Mullvad Port Forwarding to Connect to Your OPNsense Home Network with WireGuard",description:`In this quick guide, I&rsquo;ll show you how to use Mullvad port forwarding and OPNsense to create a WireGuard VPN &ldquo;tunnel-inside-a-tunnel&rdquo; configuration, to be able to connect to your home network from the outside. It&rsquo;s pretty nifty because you won&rsquo;t have to expose your public IP address. This time, I&rsquo;ll give you more of a high-level overview and reference the relevant documentation instead of a detailed step-by-step guide.
`,content:`In this quick guide, I&rsquo;ll show you how to use Mullvad port forwarding and OPNsense to create a WireGuard VPN &ldquo;tunnel-inside-a-tunnel&rdquo; configuration, to be able to connect to your home network from the outside. It&rsquo;s pretty nifty because you won&rsquo;t have to expose your public IP address. This time, I&rsquo;ll give you more of a high-level overview and reference the relevant documentation instead of a detailed step-by-step guide.
&ldquo;Outer&rdquo; WireGuard Tunnel #  I won&rsquo;t cover the configuration steps of the &ldquo;outer&rdquo; tunnel leading from your OPNsense router to a Mullvad VPN server in this post. I already cover this topic in-depth in my OPNsense baseline guide.
&ldquo;Inner&rdquo; WireGuard Tunnel #  Configuring the &ldquo;inner&rdquo; tunnel is also covered by the WireGuard Road Warrior Setup guide from the official OPNsense documentation. Make sure to set it up and verify it&rsquo;s working before you continue.
Why Mullvad Port Forwarding? #  While you set up the inner tunnel, you might have noticed that the external road warrior clients need to know the public IP of your OPNsense host. If you have a residential internet subscription it&rsquo;s likely your ISP provides you with a dynamic IP address. So every time your public IP address changes, you need to update the VPN client configurations. A typical solution for this issue is to use Dynamic DNS (DDNS). You can configure a DDNS client to monitor your dynamic IP address. And as soon it changes, it associates the IP address with a public DNS record, e.g., vpn.example.com. You can then use the DNS hostname in your WireGuard client configurations instead of an explicit IP. But what if you don&rsquo;t want to expose your &ldquo;real&rdquo; IP address to the public? Cloudflare proxying comes to mind, but it operates on layer 7, so it doesn&rsquo;t work with WireGuard.
Mullvad port forwarding to the rescue! It allows us to forward any traffic through the &ldquo;outer&rdquo; tunnel. Read the official port forwarding with Mullvad VPN guide to find out how to configure your ports.
Configure OPNsense #  Only a few steps are needed to configure OPNsense. In the following, I&rsquo;ll assume the following.
 The interface of the outer WireGuard tunnel is named WAN_VPN1 The randomly generated Mullvad port number is 61234 The WireGuard local peer for external clients listens to port 51888  First, we allow inbound traffic for the Mullvad port on the WireGuard interface of the outer tunnel. Navigate to Firewall \u2192  Rules \u2192  WAN_VPN1 and add the following rule.
            Action Pass   Interface WAN_VPN1   Protocol UDP   Source any   Destination WAN_VPN1 address   Destination port range from 61234 to 61234   Description Allow Mullvad port forward    Secondly, we redirect the traffic to the WireGuard local peer for external clients. Navigate to Firewall \u2192  NAT \u2192  Port Forward and add the following rule.
            Interface WAN_VPN1   Protocol UDP   Source any   Destination WAN_VPN1 address   Destination port range from 61234 to 61234   Redirect target IP 127.0.0.1   Redirect target port 51888   Description Redirect Mullvad port forward to WireGuard    Optionally, you can configure a public DNS record pointing to the exit IP of your Mullvad VPN tunnel.
And that&rsquo;s it for this little guide. Let me know what you think on Twitter or in the comments below!
`}).add({id:5,href:"/blog/configure-nut-for-opnsense-and-truenas-with-the-cyberpower-pr750ert2u-ups/",title:"Configure NUT for OPNsense and TrueNAS with the CyberPower PR750ERT2U UPS",description:`For storage in my homelab, I use TrueNAS. Additionally, I run a couple of apps on top of it as jails. For over a year, I&rsquo;ve been using an uninterruptible power supply (UPS) to protect my TrueNAS from possible data loss in case of a power failure. What I&rsquo;ve been missing throughout that time are the monitoring and management tools to shut down everything gracefully when the battery of the UPS runs low. In the event of a power outage lasting longer than 30 minutes, the battery would run out of juice. Everything attached to the UPS would be powered off immediately, and data loss might occur. Luckily I live in an area where power outages rarely happen. I also have backups I could restore if my TrueNAS data gets corrupted. Still, doing this right and configuring Network UPS Tools (NUT) to orchestrate shutdowns has been on my to-do list for way too long. It&rsquo;s time to tackle the issue!
`,content:`For storage in my homelab, I use TrueNAS. Additionally, I run a couple of apps on top of it as jails. For over a year, I&rsquo;ve been using an uninterruptible power supply (UPS) to protect my TrueNAS from possible data loss in case of a power failure. What I&rsquo;ve been missing throughout that time are the monitoring and management tools to shut down everything gracefully when the battery of the UPS runs low. In the event of a power outage lasting longer than 30 minutes, the battery would run out of juice. Everything attached to the UPS would be powered off immediately, and data loss might occur. Luckily I live in an area where power outages rarely happen. I also have backups I could restore if my TrueNAS data gets corrupted. Still, doing this right and configuring Network UPS Tools (NUT) to orchestrate shutdowns has been on my to-do list for way too long. It&rsquo;s time to tackle the issue!
As always, I&rsquo;ll only mention settings deviating from the defaults.
Requirements #  My server rack contains the following hardware.
 OPNsense firewall and router TrueNAS storage and jail apps Mikrotik CRS328-24P-4S+RM core switch running SwOS CyberPower PR750ERT2U UPS ISP modem  Of the above, only OPNsense and TrueNAS are susceptible to data corruption in case of power loss. If a shutdown due to low battery power is required, I want to shut down TrueNAS first. After, OPNsense shuts down. To achieve this with NUT, I configure OPNsense as NUT master and TrueNAS as NUT slave. Only one master must exist. However, multiple slaves may exist.
CyberPower PR750ERT2U #  The CyberPower PR750ERT2U has about everything I&rsquo;d ever want from a UPS.
 HID-compliant USB port to connect OPNsense Fanless operation in utility mode. It isn&rsquo;t apparent from the specifications, but the CyberPower support kindly provided me with this information 750VA capacity allowing me to add another server Pure sine wave output Line-interactive topology Energy-saving capabilities  Initially, I looked at similar used APC units on eBay. At the time, I couldn&rsquo;t find any good deals on eBay meeting all of the requirements above. I then found the CyberPower unit selling new for under $400 and grabbed one. I&rsquo;ve been a happy CyberPower customer ever since. To be clear, I don&rsquo;t affiliate with CyberPower.
NUT on OPNsense #  NUT Service Configuration #  To configure the NUT service on OPNsense, we do the following.
 Install the os-nut plugin under System \u2192  Firmware \u2192  Plugins Refresh the browser and enable the USB HID driver under Services \u2192  Nut \u2192  Configuration \u2192  UPS Type \u2192  USBHID-Driver Reboot OPNsense Connect the UPS to OPNsense via USB Set the Monitor Password and Admin Password under Services \u2192  Nut \u2192  Configuration \u2192  Nut Account Settings Set a Name (e.g., cyberpower) and Enable Nut under Services \u2192  Nut \u2192  Configuration \u2192  General Settings \u2192  General Nut Settings  We should see the data of the UPS under Services \u2192  Nut \u2192  Diagnostics.
battery.charge: 100 battery.charge.low: 0 battery.charge.warning: 35 battery.mfr.date: CPS battery.runtime: 5020 battery.runtime.low: 300 battery.type: PbAcid battery.voltage: 0.6 battery.voltage.nominal: 22 device.mfr: CPS device.model: PR750ERT2U device.serial: XXXXXXXXXXXX device.type: ups driver.name: usbhid-ups driver.parameter.pollfreq: 30 driver.parameter.pollinterval: 2 driver.parameter.port: auto driver.parameter.synchronous: no driver.version: 2.7.4 driver.version.data: CyberPower HID 0.4 driver.version.internal: 0.41 input.voltage: 235.0 input.voltage.nominal: 230 output.voltage: 253.0 ups.beeper.status: enabled ups.delay.shutdown: 20 ups.delay.start: 30 ups.load: 17 ups.mfr: CPS ups.model: PR750ERT2U ups.productid: 0601 ups.realpower.nominal: 750 ups.serial: XXXXXXXXXXXX ups.status: OL ups.test.result: No test initiated ups.timer.shutdown: 0 ups.timer.start: 0 ups.vendorid: 0764 Port Forward #  To allow TrueNAS to communicate with the NUT server on OPNsense, we add the following port forward under Firewall \u2192  NAT \u2192  Port Forward. I assume that TrueNAS is part of the LAN network.
         Interface LAN   Source TrueNAS IP   Destination LAN address   Destination port range from 3493 to 3493   Redirect target IP 127.0.0.1   Redirect target port 3493   Description Redirect NUT traffic to OPNsense    To see if it works, we SSH into TrueNAS and run the following to retrieve the UPS status from OPNsense.
upsc cyberpower@192.168.1.1 TrueNAS #     Set the following options under Services \u2192  UPS \u2192  Configure.
   General Options      Identifier The Name we earlier set in OPNsense, e.g., cyberpower   UPS Mode Slave   Remote Host &lt;IP or hostname of OPNsense&gt;   Remote Port 3493   Identifier auto       Shutdown      Shutdown Mode UPS reaches low battery   Shutdown Command /sbin/poweroff       Monitor      Monitor User monuser   Monitor Password The Monitor Password we earlier set in OPNsense    Clicking Save returns us to the Services menu. We need to enable the UPS service and check Start Automatically to start the service at boot time.
   Test #  We could disconnect the UPS from power and let the battery drain until the UPS reaches low battery status. At some point, it&rsquo;s probably a good idea to do. But repeatedly doing that unnecessarily wears down the battery. Another way is to run the Force Shut Down command. SSH into the NUT master (OPNsense) and run the following.
upsmon -c fsd If we configured everything correctly, TrueNAS and OPNsense shut down.
`}).add({id:6,href:"/blog/opnsense-baseline-guide-with-vpn-guest-and-vlan-support/",title:"OPNsense Baseline Guide with Mullvad VPN Multi-WAN, Guest, and VLAN Support",description:"This beginner-friendly, step-by-step guide walks you through the initial configuration of your OPNsense firewall. The title of this guide is an homage to the pfSense baseline guide with VPN, Guest, and VLAN support that some of you guys might know, and this is an OPNsense migration of it. I found that guide two years ago and immediately fell in love with the network setup. After researching for weeks, I decided to use OPNsense instead of pfSense.",content:`This beginner-friendly, step-by-step guide walks you through the initial configuration of your OPNsense firewall. The title of this guide is an homage to the pfSense baseline guide with VPN, Guest, and VLAN support that some of you guys might know, and this is an OPNsense migration of it. I found that guide two years ago and immediately fell in love with the network setup. After researching for weeks, I decided to use OPNsense instead of pfSense. I bit the bullet and bought the Deciso DEC630 appliance. Albeit expensive and possibly overkill for my needs, I&rsquo;m happy to support the open-source mission of Deciso, the maintainers of OPNsense. The only thing I regret about the purchase is that I now can&rsquo;t afford the sexier-looking successor model, the DEC690.
To configure OPNsense, I followed the instructions of the pfSense guide, taking notes on the differences. Some options moved to different menus or changed. As my notes grew, I decided to publish them as a guide on my website.
My goal was to create a comprehensive guide that&rsquo;s easy to follow. But I tried to strike a different balance regarding the brevity of the instructions compared to the pfSense guide. It&rsquo;s a matter of personal taste, but I find the instructions in that guide too verbose. I intentionally omit most of the repetitive &ldquo;click save and apply&rdquo; instructions and only list configuration changes deviating from defaults, making exceptions for important settings. I consider the OPNsense defaults stable enough for this approach in the hope of keeping the effort required to maintain this guide to a minimum.
I&rsquo;m a homelab hobbyist, so be warned that this guide likely contains errors. Please, verify the steps yourself and do your research. I hope this guide is as helpful and inspiring to you as the pfSense guide was to me. Your feedback is always welcome and very much appreciated.
Overview #  WAN #   DHCP WAN from a single Internet Service Provider (ISP) Mullvad VPN multi-WAN with gateway groups  LAN #  We segregate the local network into several areas with different requirements.
Management Network (VLAN 10) #  The Management network connects native management interfaces like WiFi access points and IPMI interfaces.
VPN Network (VLAN 20) #  The primary LAN network uses the WireGuard VPN tunnels for outbound connections, maximizing privacy and security. If the VPN tunnels fail, outbound connections won&rsquo;t be possible. Exceptions to selectively route traffic through the ISP WAN gateway are possible.
&ldquo;Clear&rdquo; Network (VLAN 30) #  General-purpose web access network that doesn&rsquo;t use VPN tunnels. All outgoing connections leave through the ISP WAN gateway. It serves as a backup network in case the VPN tunnels fail.
Guest Network (VLAN 40) #  The network that visitors use. It allows unrestricted internet access. Local networks aren&rsquo;t accessible.
LAN Network #  &ldquo;Native&rdquo; VLAN, used to debug and test new configurations.
DNS Services #  We&rsquo;ll configure a DNS resolver (Unbound), as well as a DNS forwarder (Dnsmasq) in OPNsense. Management and VPN networks will use the resolver, the Clear network will use the forwarder, and the Guest network will use Cloudflare as an external DNS resolver. We&rsquo;ll dig into the details later.
Hardware Selection and Installation #  The original pfSense guide features a large section of hardware recommendations and installation instructions.
As mentioned earlier, I bought the Deciso DEC630 appliance, which is why I&rsquo;m not advising on hardware choices. Have a look at the official hardware sizing &amp; setup guidelines for more information. See also Initial Installation &amp; Configuration.
I verified this guide with a clean install of OPNsense version 21.7.5.
Wizard #  Navigate to 192.168.1.1 in your browser and login with default credentials:
 Username: root Password: opnsense  Click Next to leave the welcome screen and get started with the initial wizard configuration.
General Information #     I prefer using the DNS servers of Quad9 over the ones of my ISP. Only the Clear network will use these anyway, as secured networks use Unbound instead. The Guest network will use Cloudflare DNS servers.
For the domain, I prefer to use a subdomain of a domain name I own, like corp.example.com. I only use this subdomain internally. I consider the local.lan pattern a relic of the past. To prevent our local network structure from leaking to the outside world, we&rsquo;ll later configure Unbound and Dnsmasq to treat the domain as private.
         Domain corp.example.com   Primary DNS Server 9.9.9.9   Secondary DNS Server 149.112.112.112   Override DNS unchecked   Enable DNSSEC Support checked   Harden DNSSEC data checked    If you prefer using your ISP&rsquo;s DNS servers, leave the Override DNS option checked.
Time Server Information #  Choose the NTP servers geographically closest to your location. I live in Switzerland, which makes the servers from the ch.pool.ntp.org pool the natural choice.
         Time server hostname 0.ch.pool.ntp.org 1.ch.pool.ntp.org 2.ch.pool.ntp.org 3.ch.pool.ntp.org   Timezone Europe/Zurich    Configure Interfaces #  By default, the WAN interface obtains an IP address from your ISP via DHCP. DHCP is also configured for the LAN interface by default and has the IP 192.168.1.1. It works for most people, so we just keep the defaults.
Set Root Password #  Choose a strong root password and complete the wizard.
General Settings #  Access #  Navigate to System \u2192  Settings \u2192  Administration.
   HTTP Redirect      Disable web GUI redirect rule checked    Permitting root user login and password login is a quick and dirty way of enabling SSH access, but I strongly discourage you from doing it. They are disabled for security reasons. I highly recommend using certificate- or key-based authentication. If your device has a serial console port, like the Deciso DEC630, enabling SSH is not required.
   Secure Shell       Secure Shell Server checked        Authentication       Sudo Ask password Permit sudo usage for administrators with shell access.    Navigate to System \u2192  Access \u2192  Users and add a new user.
         Username &lt;choose a username&gt;   Password &lt;choose a secure password&gt;   Login shell /bin/csh   Group Memberships admins   Authorized keys &lt;valid SSH public key&gt;    Configuring the SSH client and generating keys is out of scope for this guide, so I&rsquo;ll just recommend this DigitalOcean tutorial covering SSH essentials.
Miscellaneous #  Navigate to System \u2192  Settings \u2192  Miscellaneous.
   Power Savings      Use PowerD checked   Power Mode Hiadaptive    Choose Cryptography settings and Thermal Sensors settings compatible with your hardware.
Firewall Settings #  Navigate to Firewall \u2192  Settings \u2192  Advanced.
Although IPv6 is something I want to use, it&rsquo;s out of scope for this guide, so we uncheck the following.
         Allow IPv6 unchecked    When a rule uses a specific gateway and goes down, a rule gets created, sending traffic to the default gateway. Checking this option skips the creation of this rule.
   Gateway Monitoring      Skip rules checked    Depending on your hardware, you might want to tweak the following settings to improve performance.
   Miscellaneous       Firewall Optimization conservative Tries to avoid dropping any legitimate idle connections at the expense of increased memory usage and CPU utilization.   Firewall Maximum Table Entries 2000000 default is 1'000'000    We disable the auto-generated anti-lockout rule because we&rsquo;ll define it manually later.
         Disable anti-lockout checked    Checksum Offloading #  For some hardware, checksum offloading doesn&rsquo;t work, particularly some Realtek cards. Rarely, drivers may have problems with checksum offloading and some specific NICs. If your hardware is incompatible with checksum offloading, disable it.
Navigate to Interfaces \u2192  Settings.
          Hardware CRC unchecked Disable hardware checksum offload    VLANs #  Switch Choice #  A 802.1Q-capable switch with properly configured VLANs is required. Check my router on a stick VLAN configuration guide to see an example setup with a Mikrotik switch.
VLAN Definitions #  Typically, the LAN port also carries the VLAN traffic and functions as trunk port. For me, the default is the igb0 port. I chose it as the parent interface for all VLANs in the following steps.
   Navigate to Interfaces \u2192  Other Types \u2192  VLAN and add the VLANs.
Management VLAN #           Parent Interface igb0   VLAN tag 10   Description VLAN10_MANAGE    VPN VLAN #           Parent Interface igb0   VLAN tag 20   Description VLAN20_VPN    Clear VLAN #           Parent Interface igb0   VLAN tag 30   Description VLAN30_CLEAR    Guest VLAN #           Parent Interface igb0   VLAN tag 40   Description VLAN40_GUEST    VLAN Interfaces #  We add an interface for each VLAN. Navigate to Interfaces \u2192  Assignments.
    Select vlan 10, enter the description VLAN10_MANAGE, and click + Select vlan 20, enter the description VLAN20_VPN, and click + Select vlan 30, enter the description VLAN30_CLEAR, and click + Select vlan 40, enter the description VLAN40_GUEST, and click +  Click Save.
VLAN Interface IPs #  To easier remember which IP range belongs to which VLAN, I like the convention of matching the third octet of the IP with the VLAN ID. I.e., assigning the VLAN with the ID 10 the address 192.168.10.0/24.
   Interface: VLAN10_MANAGE #  Select the VLAN10_MANAGE interface.
         Enable Interface checked   IPv4 Configuration Type Static IPv4   IPv4 Address 192.168.10.1/24    Click Save.
Interface: VLAN20_VPN #           Enable Interface checked   IPv4 Configuration Type Static IPv4   IPv4 Address 192.168.20.1/24    Interface: VLAN30_CLEAR #           Enable Interface checked   IPv4 Configuration Type Static IPv4   IPv4 Address 192.168.30.1/24    Interface: VLAN40_GUEST #           Enable Interface checked   IPv4 Configuration Type Static IPv4   IPv4 Address 192.168.40.1/24    VLAN Interface DHCP #  We need to configure DHCP for each VLAN we created. I use x.x.x.100-199 for dynamic and x.x.x.10.10-99 for static IP address assignments. You might want to amend these ranges to your requirements.
   Navigate to Services \u2192  DHCPv4.
DHCP: VLAN10_MANAGE #  Select VLAN10_MANAGE.
         Enable checked   Range from 192.168.10.100 to 192.168.10.199    Click Save.
DHCP: VLAN20_VPN #           Enable checked   Range from 192.168.20.100 to 192.168.20.199    DHCP: VLAN30_CLEAR #           Enable checked   Range from 192.168.30.100 to 192.168.30.199    DHCP: VLAN40_GUEST #           Enable checked   Range from 192.168.40.100 to 192.168.40.199   DNS servers 1.1.1.1 1.0.0.1    DHCP: LAN #           Range from 192.168.1.100 to 192.168.1.199    WireGuard VPN with Mullvad #  In recent years, Mullvad has been my VPN provider of choice. When That One Privacy Site was still a thing, Mullvad was one of the top recommendations there. After reading the review, I decided to try it out and haven&rsquo;t looked back since. No personally identifiable information is required to register, and paying cash via mail works perfectly.
I decided to go with WireGuard because I&rsquo;m fine riding the bleeding edge. \u{1F60E} For more detailed steps, check the official OPNsense documentation on setting up WireGuard with Mullvad and WireGuard selective routing.
Please note that the FreeBSD kernel does not (yet) natively support WireGuard, so you must install it as a plugin. Possibly, this doesn&rsquo;t meet your stability, security, or performance requirements.
By default, the OPNsense plugin uses the Go implementation of WireGuard. But I couldn&rsquo;t get multi-WAN working with it. However, with the experimental WireGuard kernel module wireguard-kmod, it works. I only managed to get failover working, though. Load balancing doesn&rsquo;t seem to be supported yet.
Navigate to System \u2192  Firmware \u2192  Plugins and install os-wireguard. Refresh the browser and navigate to VPN \u2192  WireGuard. Then SSH into OPNsense, run pkg install wireguard-kmod, and reboot.
Remote Peers #  Select your preferred WireGuard servers from the Mullvad&rsquo;s server list and take note of their names and public keys. It&rsquo;s worth spending some time to benchmark server performance before making a choice.
   Select the Endpoints tab and click Add. Here is the configuration for the remote ch5-wireguard Mullvad endpoint.
         Name mullvad-ch5-wireguard   Public key /iivwlyqWqxQ0BVWmJRhcXIFdJeo0WbHQ/hZwuXaN3g=   Allowed IPs 0.0.0.0/0   Endpoint Address 193.32.127.66   Endpoint Port 51820   Keepalive 25    To mitigate risks against DNS poisoning, resolve the server&rsquo;s hostname and enter its IP as Endpoint Address. You can do this by running nslookup ch5-wireguard.mullvad.net in a shell. Make sure to not confuse this address with the SOCKS5 Proxy Address from Mullvad&rsquo;s server list!
Repeat the steps above to add another server, e.g., ch6-wireguard. Note that all endpoint configurations use the Endpoint Port 51820.
Local Peers #  Select the Local tab, click Add, and enable the advanced mode.
         Name mullvad0   Listen Port 51820   Tunnel Address &lt;LEAVE EMPTY&gt;   Peers ch5-wireguard   Disable Routes checked   Gateway &lt;LEAVE EMPTY&gt;    Click Save to generate the WireGuard key pair. Click Edit and copy the generated Public Key. Next, run the following shell command.
curl -sSL https://api.mullvad.net/app/v1/wireguard-keys \\  -H &#34;Content-Type: application/json&#34; \\  -H &#34;Authorization: Token &lt;Mullvad account number&gt;&#34; \\  -d &#39;{&#34;pubkey&#34;:&#34;&lt;generated public key&gt;&#34;}&#39; This command returns a JSON response containing IPs without DNS hijacking enabled. I cover the snippet above and Mullvad&rsquo;s DNS hijacking in another post: Use Custom DNS Servers With Mullvad And Any WireGuard Client.
{ &#34;id&#34;: &#34;ufO5jCni55uvioHM%2FeLBgyrrUMocEXsADPc2OvYhF3k%3D&#34;, &#34;pubkey&#34;: &#34;ufO5jCni55uvioHM/eLBgyrrUMocEXsADPc2OvYhF3k=&#34;, &#34;ipv4_address&#34;: &#34;10.105.248.51/32&#34;, &#34;ipv6_address&#34;: &#34;fc00:bbbb:bbbb:bb01::2a:f832/128&#34; } Copy the IPv4 IP address to the Tunnel Address field of the local peer. Subtract one from the Tunnel Address and enter the result as Gateway IP. E.g., 10.105.248.50 for the example above. It&rsquo;s just a convention I like, but you can use any arbitrary, unused private RFC1918 IP.
   Repeat the steps above to create a second local peer named mullvad1. Remember to use a different Listen Port (e.g., 51821).
   When you finish, select the General tab. Check Enable WireGuard. You should see a handshake for the wg0 and wg1 tunnels on the Handshakes tab.
WireGuard Interfaces #  Navigate to Interfaces \u2192  Assignments.
 Select wg0, add the description WAN_VPN0, and click + Select wg1, add the description WAN_VPN1, and click +  Enable the newly created interfaces and restart the WireGuard service after. It ensures the interfaces get an IP address from WireGuard.
VPN Gateways #     Navigate to System \u2192  Gateways \u2192  Single and add the VPN gateways.
WAN_VPN0 #           Name WAN_VPN0   Interface WAN_VPN0   Address Family IPv4   IP Address 10.105.248.50   Far Gateway checked   Disable Gateway Monitoring unchecked   Monitor IP 100.64.0.1    WAN_VPN1 #           Name WAN_VPN1   Interface WAN_VPN1   Address Family IPv4   IP Address 10.109.231.89   Far Gateway checked   Disable Gateway Monitoring unchecked   Monitor IP 100.64.0.2    Monitoring IPs #  Each VPN gateway requires a unique monitoring IP because setting a monitoring IP installs a static route. Optimally, the monitoring IP should be the least possible amount of hops away from the gateway. For Mullvad specifically, we can &ldquo;abuse&rdquo; the local infrastructure that&rsquo;s available through a Mullvad connection. Any of the following IPs are only one hop away from the tunnel exit.
 100.64.0.1 to 100.64.0.3 are Mullvad&rsquo;s ad-blocking and tracker-blocking DNS service servers 10.64.0.1 is the local Mullvad gateway  You can easily verify the above by running traceroute 100.64.0.1 from a host connected to Mullvad.
Add Static IPv4 Configuration to the WireGuard Interfaces #  OPNsense versions newer than 21.7.3 require adding static IPv4 configuration to the WireGuard interface. Otherwise, Unbound will use the default route despite setting the Outgoing Network Interfaces option. Other solutions exist, but I&rsquo;m not sure which the &ldquo;best&rdquo; or most logical one is. As WireGuard integration matures, this section hopefully becomes obsolete. You can find more information regarding this issue on GitHub.
Navigate to Interfaces and edit the WireGuard interfaces.
IP Configuration: WAN_VPN0 #           IPv4 Configuration Type Static IPv4   IPv4 address 10.105.248.51/32   IPv4 Upstream Gateway WAN_VPN0 - 10.105.248.50    IP Configuration: WAN_VPN1 #           IPv4 Configuration Type Static IPv4   IPv4 address 10.109.231.90/32   IPv4 Upstream Gateway WAN_VPN1 - 10.109.231.89    Gateway Group #  Navigate to System \u2192  Gateways \u2192  Group and click Add.
         Group Name WAN_VPN_GROUP   WAN_VPN0 Tier 1   WAN_VPN1 Tier 2 (failover)   Trigger Level Packet Loss or High Latency    It&rsquo;s also possible to configure failover or both.
Static Routes (Optional) #  Defining static routes for the tunnel gateways is optional. It would be necessary, for example, if we want to consider the VPN gateways as default gateway candidates. It requires static routes to the ISP WAN gateway to keep the tunnel connections alive.
   Navigate to System \u2192  Routes \u2192  Configuration and click Add.
         Network Address 193.32.127.66/32   Gateway WAN_DHCP   Description Keep tunnels to mullvad-ch5-wireguard alive             Network Address 193.32.127.67/32   Gateway WAN_DHCP   Description Keep tunnels to mullvad-ch6-wireguard alive    DNS #     OPNsense includes a DNS resolver (Unbound) and a DNS forwarder (Dnsmasq / Unbound in forwarding mode). Simple setups usually use one of either, but we&rsquo;ll use both. Because we&rsquo;ll also use Unbound and Dnsmasq for internal DNS resolution, we don&rsquo;t want to use them for the Guest network, as this would expose our internal network structure. That&rsquo;s the reason why we earlier configured it to use Cloudflare DNS servers instead.
Like the name suggests, a DNS forwarder forwards DNS requests to an external DNS resolver of an ISP, Quad9, Cloudflare, or similar service provider. We&rsquo;ll configure the forwarder for the Clear network. In case the primary, secured networks lose connectivity, the Clear network can serve as a backup.
One of the advantages of self-hosting a DNS resolver is improved privacy. A resolver iteratively queries a chain of one or more DNS servers to resolve a request, so there isn&rsquo;t a single instance knowing all your DNS requests. It comes at the cost of speed when resolving a hostname for the first time. As Unbound&rsquo;s cache grows, the cost diminishes. We&rsquo;ll configure our primary networks to use Unbound.
We&rsquo;ll also keep DNS traffic from Unbound within the VPN tunnels. In the rare case of a VPN outage, we&rsquo;ll want local DNS services to fail and not leak through the ISP WAN. The reason for this isn&rsquo;t improved privacy as you might think. In some cases, this might even hurt your privacy. Why? Either your ISP or your VPN provider will see the iterative DNS requests Unbound sends. So it becomes a question of who you rather entrust with this data. But if there are no privacy benefits, why do it? Honestly, I don&rsquo;t require such a setup. I configured it for educational purposes and fun. Other reasons that don&rsquo;t affect me but other users are:
 ISP selling user data ISP enforcing censorship ISP hijacking DNS traffic to redirect it to their DNS resolver; this makes self-hosting a DNS resolver impossible  Let&rsquo;s summarize our goals:
 Use a DNS resolver for the management and VPN networks Resolve private domain hostnames for management and VPN networks Prevent DNS leaks from Unbound through the ISP WAN gateway Use DNS forwarding for the Clear network Use external DNS resolvers for the Guest network  Resolver (Unbound) #  Navigate to Services \u2192  Unbound DNS \u2192  General.
         Network Interfaces LAN VLAN10_MANAGE VLAN20_VPN   DNSSEC checked   DHCP registration checked   DHCP static mappings checked   Local Zone Type static   Outgoing Network Interfaces WAN_VPN0 WAN_VPN1    Navigate to Services \u2192  Unbound DNS \u2192  Advanced.
         Hide Identity checked   Hide Version checked   Prefetch Support checked   Prefetch DNS Key Support checked   Harden DNSSEC data checked    The final step is to add a custom SOA record to the local zone making Unbound the authoritative name server for corp.example.com. This way, we prevent Unbound from querying external name servers for the internal domain and exposing our network structure to the outside world. For advanced Unbound configuration like this, we use Templates.
Connect to OPNsense via serial console or SSH and add a +TARGETS file by running sudo vi /usr/local/opnsense/service/templates/OPNsense/Unbound/+TARGETS containing:
private_domains.conf:/usr/local/etc/unbound.opnsense.d/private_domains.conf Add the template file by running sudo vi /usr/local/opnsense/service/templates/OPNsense/Unbound/private_domains.conf containing:
server: local-data: &#34;corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600&#34; Here is a translation of what the SOA record means.
         Name corp.example.com   Record Type SOA   Primary Name Server opnsense.corp.example.com   Administrator Email root@example.com   Serial 2021110201 (YYMMDDnn)   Refresh 86400 (24 hours)   Retry 7200 (2 hours)   Expire 3600000 (1000 hours)   TTL 3600 (1 hour)    Run the following to verify the configuration.
# generate template configctl template reload OPNsense/Unbound # show generated file cat /usr/local/etc/unbound.opnsense.d/private_domains.conf # check if configuration is valid configctl unbound check Forwarder (Dnsmasq) #  Dnsmasq will forward DNS requests to the configured system DNS servers and 127.0.0.1 (Unbound). Earlier, you either explicitly configured them or decided to receive the DNS servers via DHCP from your ISP. Because Unbound already uses port 53, we&rsquo;ll use port 5335 for Dnsmasq. We&rsquo;ll later create rules to port forward DNS traffic to this port.
Navigate to Services \u2192  Dnsmasq DNS \u2192  Settings.
         Enable checked   Listen Port 5335   Do not forward private reverse lookups checked    Forward reverse DNS lookups in the 192.168.0.0/16 range to Unbound by adding the following Domain Overrides. We additionally make Unbound the authoritative DNS server for corp.example.com.
   Domain IP Description     168.192.in-addr.arpa 192.168.20.1 Forward reverse lookups of private IP addresses to Unbound   corp.example.com 192.168.20.1 Make Unbound the authoritative DNS server for private domain    Firewall #  Here is an overview of what we want to implement with firewall rules.
 Allow internet access for specific ports through WAN and VPN Allow intranet communications Redirect outbound DNS traffic to either Unbound or Dnsmasq Redirect NTP traffic to OPNsense Block intranet access for the Guest network      VLAN10 VLAN20 VLAN30 VLAN40 LAN     Internet WAN VPN + selective WAN WAN WAN WAN   Intranet pass pass pass block pass   ICMP pass pass pass pass pass   Anti-lockout yes no no no yes   DNS Unbound Unbound Dnsmasq external Unbound   NTP local local local external external    Interface Groups #  We use interface groups to apply policies to multiple interfaces at once and reduce the number of required firewall rules significantly. Do not use them for WAN interfaces because they don&rsquo;t use the reply-to directive!
I&rsquo;m honestly not sure if I went overboard with interface groups and over-abstracted things. Currently, I&rsquo;m happy with the configuration, and I guess only time will tell how maintainable this approach is. I&rsquo;d like to know what you think and would very much appreciate your feedback.
   Navigate to Firewall \u2192  Groups and add the following interface groups.
IG_LOCAL #           Name IG_LOCAL   Description All local interfaces   Members LAN VLAN10_MANAGE VLAN20_VPN VLAN30_CLEAR VLAN40_GUEST    IG_OUT_WAN #           Name IG_OUT_WAN   Description Interfaces allowing outbound WAN traffic   Members LAN VLAN10_MANAGE VLAN30_CLEAR VLAN40_GUEST    IG_OUT_VPN #           Name IG_OUT_VPN   Description Interfaces allowing outbound VPN traffic and selective outbound WAN traffic   Members VLAN20_VPN    IG_DNS_RESOLVE #           Name IG_DNS_RESOLVE   Description Interfaces forced to use Unbound   Members VLAN10_MANAGE VLAN20_VPN    IG_DNS_FORWARD #           Name IG_DNS_FORWARD   Description Interfaces forced to use Dnsmasq   Members VLAN30_CLEAR    IG_NTP #           Name IG_NTP   Description Interfaces forced to use OPNsense as NTP server   Members VLAN10_MANAGE VLAN20_VPN VLAN30_CLEAR    Aliases #  We define a few reusable aliases that help us condense our firewall rules. Some of them might become hard to maintain as they grow, in which case you might want to consider nesting aliases.
   Navigate to Firewall \u2192  Aliases and create the following aliases.
Selective Routing Addresses #  Services like banks might object to traffic originating from known VPN endpoints. We selectively route traffic from the VPN VLAN through the default WAN gateway.
         Name SELECTIVE_ROUTING   Type Host(s)   Description External hosts reachable from IG_OUT_VPN networks through WAN    If you&rsquo;re having issues with a service not working due to VPN, add the hostname to this alias, e.g., netflix.com.
Admin / Anti-lockout Ports #           Name PORTS_ANTI_LOCKOUT   Type Port(s)   Content 443 (Web GUI) 22 (SSH)   Description OPNsense admin ports    Ports Allowed To Communicate Between VLANs #  Allowed ports for intranet traffic. Amend the list depending on your needs.
         Name PORTS_OUT_LAN   Type Port(s)   Description Ports allowed for intranet    Content:
 53 DNS 5353:5354 mDNS 123 NTP 21 FTP 22 SSH 161 SNMP 80 HTTP 8080: HTTP alt / UniFi device and application communication 443 HTTPS 8443 HTTPS alt / UniFi application GUI/API as seen in a web browser 8880 UniFi HTTP portal redirection 10001 UniFi device discovery 5001 iPerf 623 IPMI 5900 VNC 3389 RDP 49152:65535 ephemeral ports  Ports Allowed to Communicate with the Internet #  Allow ports for egress internet traffic. Amend the list depending on your needs.
         Name PORTS_OUT_WAN   Type Port(s)   Description Ports allowed for internet    Content:
 21 FTP 22 SSH 80 HTTP 8080 HTTP alt 443 HTTPS 8443 HTTPS alt 465 SMTPS 587: SMTPS 993: IMAPS 49152:65535 ephemeral ports  NAT #  Network Address Translation (NAT) is required to translate private to public IP addresses. We have the following requirements.
 Translate IG_OUT_WAN and IG_OUT_VPN network addresses to the WAN address range. Translating IG_OUT_VPN to WAN allows selective routing. Translate IG_OUT_VPN network addresses to the WAN_VPN0 address range.     Navigate to Firewall \u2192  NAT \u2192  Outbound.
Select Manual outbound NAT rule generation and add the following rules.
IG_OUT_WAN to WAN #           Interface WAN   Source address IG_OUT_WAN net   Description IG_OUT_WAN to WAN    IG_OUT_VPN to WAN #           Interface WAN   Source address IG_OUT_VPN net   Description IG_OUT_VPN to WAN    IG_OUT_VPN to WAN_VPN0 #           Interface WAN_VPN0   Source address IG_OUT_VPN net   Description IG_OUT_VPN to WAN_VPN0    IG_OUT_VPN to WAN_VPN1 #           Interface WAN_VPN1   Source address IG_OUT_VPN net   Description IG_OUT_VPN to WAN_VPN1    Rules #  Navigate to Firewall \u2192  Rules.
Anti-Lockout #  Before adding any other rules, we add the anti-lockout ones on the VLAN10_MANAGE and LAN networks, so we can&rsquo;t lock ourselves out. \u{1F605}
   Select Floating and add the following rule.
         Action Pass   Interface LAN VLAN10_MANAGE   Protocol TCP/UDP   Source any   Destination This Firewall   Destination port range PORTS_ANTI_LOCKOUT   Description Anti-lockout    This Firewall is a pre-defined alias representing all interface addresses of OPNsense.
Allow Intranet Pings #  We allow ICMP pings for the entire local network. Pings are maliciously abusable, so you may want to put stricter rules into place if required.
   Select IG_LOCAL and add the following rule.
         Action Pass   Interface IG_LOCAL   TCP/IP Version IPv4   Protocol ICMP   ICMP type Echo Request   Source IG_LOCAL net   Description Allow intranet pings    Reject Intranet Traffic By Default #  By default, we reject traffic on local interfaces instead of blocking it. Block drops packets silently. Reject returns a &ldquo;friendly&rdquo; response to the sender. To be able to override this rule, unchecking Quick is crucial! To use Firewall Logs to review blocked ports and amend our port list alias if necessary, we enable logging on this rule.
Select IG_LOCAL and add the following rule.
         Action Reject   Quick unchecked   Interface IG_LOCAL   TCP/IP Version IPv4+IPv6   Protocol any   Source IG_LOCAL net   Log checked   Description Reject intranet traffic by default    Allow Intranet Traffic #  We only allow intranet traffic on the ports defined in the PORTS_OUT_LAN alias. We&rsquo;ll override this rule for the VLAN40_GUEST network later, so we must uncheck the Quick option again. For the Management network, you might want to consider stricter rules, as well.
Select IG_LOCAL and add the following rule.
         Action Pass   Quick unchecked   Interface IG_LOCAL   Protocol TCP/UDP   Source IG_LOCAL net   Destination IG_LOCAL net   Destination port range PORTS_OUT_LAN   Description Allow intranet traffic    Allow Internet Traffic #  We allow internet traffic on PORTS_OUT_WAN for IG_OUT_WAN networks.
   Select IG_OUT_WAN and add the following rule.
         Action Pass   Quick unchecked   Interface IG_OUT_WAN   Protocol TCP/UDP   Source IG_OUT_WAN net   Destination / Invert checked   Destination IG_LOCAL net   Destination port range PORTS_OUT_WAN   Description Allow internet traffic through WAN    We later want to enable unrestricted internet access on the Guest network, so make sure to uncheck the Quick option!
Next, we allow internet traffic on PORTS_OUT_WAN for the IG_OUT_VPN networks.
   Select IG_OUT_VPN and add the following rules to configure selective routing.
         Action Pass   Interface IG_OUT_VPN   Protocol TCP/UDP   Source IG_OUT_VPN net   Destination SELECTIVE_ROUTING   Destination port range PORTS_OUT_WAN   Description Allow selected internet traffic through WAN             Action Pass   Protocol TCP/UDP   Source IG_OUT_VPN net   Destination / Invert checked   Destination IG_LOCAL net   Destination port range PORTS_OUT_WAN   Description Allow internet traffic through WAN_VPN0   Gateway WAN_VPN_GROUP    Restrict Guest Network #  Select VLAN40_GUEST and add the following rules.
   To block Web GUI and SSH access from the Guest network, we block traffic to any OPNsense interface on the PORTS_ANTI_LOCKOUT ports. We enable logging for this rule to be able to see if any guests try to access OPNsense.
         Action Block   Interface VLAN40_GUEST   Protocol TCP/UDP   Source VLAN40_GUEST net   Destination This Firewall   Destination port range PORTS_ANTI_LOCKOUT   Log checked   Description Block admin ports    We block access to other local networks and also enable logging for the rule.
         Action Block   Interface VLAN40_GUEST   Protocol TCP/UDP   Source VLAN40_GUEST net   Destination IG_LOCAL net   Log checked   Description Block traffic to local networks    Finally, we enable unrestricted internet access on Guest networks.
         Action Pass   Interface VLAN40_GUEST   Protocol TCP/UDP   Source VLAN40_GUEST net   Destination / Invert checked   Destination IG_LOCAL net   Description Unrestricted internet access    LAN Network For Testing And Debugging #  I just keep the pre-defined &ldquo;LAN to any&rdquo; rules. I periodically reconfigure this network for testing and debugging and don&rsquo;t use it for anything else.
   Redirect Outbound DNS Traffic #  To prevent clients from explicitly querying outbound DNS and leaking information to the outside, we redirect any outbound DNS traffic to Unbound or Dnsmasq.
   Navigate to Firewall \u2192  NAT \u2192  Port Forward and add the following rules.
         Interface IG_DNS_FORWARD   Protocol TCP/UDP   Source IG_DNS_FORWARD net   Destination any   Destination port range DNS   Redirect target IP 127.0.0.1   Redirect target port 5335   Description Redirect any DNS traffic to Dnsmasq             Interface IG_DNS_RESOLVE   Protocol TCP/UDP   Source IG_DNS_RESOLVE net   Destination / Invert checked   Destination IG_DNS_RESOLVE net   Destination port range DNS   Redirect target IP 127.0.0.1   Redirect target port DNS   Description Redirect outbound DNS traffic to Unbound    Redirect Outbound NTP Traffic #  To sync the time of all our devices on the network to OPNsense, we redirect all NTP traffic.
   Navigate to Firewall \u2192  NAT \u2192  Port Forward and add the following rule.
         Interface IG_NTP   Protocol UDP   Source IG_NTP net   Destination / Invert checked   Destination IG_NTP net   Destination port range NTP   Redirect target IP 127.0.0.1   Redirect target port NTP   Description Redirect outbound NTP traffic to OPNsense    Test #  Now would be a could time to reboot OPNsense to make sure all settings are applied.
Test DHCP #  Connect to a host in each VLAN and verify it receives an IP inside the specified DHCP range. Here is the output of the ip -4 addr show eth0 command from a Ubuntu host connected to the VPN VLAN.
8: eth0: &lt;BROADCAST,MULTICAST,UP&gt; mtu 1500 group default qlen 1 inet 192.168.20.106/24 brd 192.168.20.255 scope global dynamic valid_lft 7196sec preferred_lft 7196sec Test DNS #  We have to verify the following functionality of our DNS architecture:
 VLAN20_VPN  Unbound resolves remote and local hostname lookups Redirect outbound DNS traffic to Unbound Reverse lookups of private IPs Don&rsquo;t leak lookups for the private corp.example.com domain   VL30_CLEAR  Dnsmasq forwards remote hostname lookups to the system DNS servers like Quad9 and Unbound Forward local hostname lookups to Unbound Redirect outbound DNS traffic to Dnsmasq Forward local reverse lookups of private IPs to Unbound Don&rsquo;t leak lookups for the private corp.example.com domain and forward them to Unbound   VL40_GUEST  Use external DNS resolvers Allow for clients to override DNS OPNsense lookups are blocked    We&rsquo;ll use the dig tool and the firewall logs under Firewall \u2192  Log Files \u2192  Live View for testing.
I&rsquo;ll also skip the Management network because it requires the same testing as the VPN network.
VLAN20_VPN: Test DNS #  Connect to VLAN20_VPN.
VLAN20_VPN: Remote Hostname Lookups #  Run dig california.gov:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; california.gov ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 41004 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;california.gov. IN A ;; ANSWER SECTION: california.gov. 300 IN A 63.196.102.29 ;; Query time: 36 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 22:37:34 CET 2021 ;; MSG SIZE rcvd: 59 Here are the firewall logs showing the iterative DNS requests Unbound sends.
   VLAN20_VPN: Local Hostname Lookups #  Run dig opnsense.corp.example.com:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; opnsense.corp.example.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 22291 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.corp.example.com. IN A ;; ANSWER SECTION: opnsense.corp.example.com. 3600 IN A 192.168.1.1 opnsense.corp.example.com. 3600 IN A 192.168.10.1 opnsense.corp.example.com. 3600 IN A 192.168.20.1 ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:48:19 CET 2021 ;; MSG SIZE rcvd: 105 VLAN20_VPN: Redirect Outbound DNS Traffic #  Run dig opnsense.org @8.8.8.8:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; opnsense.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 17970 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 184 IN A 178.162.131.118 ;; Query time: 0 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Tue Nov 16 21:51:15 CET 2021 ;; MSG SIZE rcvd: 57 dig can&rsquo;t tell that OPNsense hijacked the request and thus displays an incorrect SERVER value. If you check the firewall logs, you shouldn&rsquo;t see any requests to 8.8.8.8. Instead, you should see iterative root server requests.
VLAN20_VPN: Reverse Lookups of Private IPs #  Run dig -x 192.168.20.1:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; -x 192.168.20.1 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 9264 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;1.20.168.192.in-addr.arpa. IN PTR ;; ANSWER SECTION: 1.20.168.192.in-addr.arpa. 3600 IN PTR OPNsense.corp.example.com. ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:56:14 CET 2021 ;; MSG SIZE rcvd: 96 If you want, additionally reverse-lookup an IP that doesn&rsquo;t exist. The firewall logs mustn&rsquo;t contain requests to external DNS servers.
VLAN20_VPN: Verify corp.example.com Is Private #  To test whether OPNsense is the authoritative server for corp.example.com, we lookup a non-existent hostname in that domain. dig should return an authoritative NXDOMAIN response with the SOA record we earlier defined earlier in the AUTHORITY SECTION.
Run dig nowhere.corp.example.com:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; nowhere.corp.example.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 44590 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;nowhere.corp.example.com. IN A ;; AUTHORITY SECTION: corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600 ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:59:58 CET 2021 ;; MSG SIZE rcvd: 110 VLAN20_VPN: DNS Leak Test #  In your browser, navigate to dnsleaktest.com or mullvad.net/check. We expect the &ldquo;leaked&rdquo; DNS server to match our Mullvad public Mullvad IP. The second leak is from the Outgoing Interface we configured for Unbound:
   VLAN30_CLEAR: Test DNS #  Connect to VLAN30_CLEAR.
VLAN30_CLEAR: Remote Hostname Lookups #  Run dig opnsense.org:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; opnsense.org ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 65053 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 596 IN A 178.162.131.118 ;; Query time: 5 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Tue Nov 16 16:45:08 CET 2021 ;; MSG SIZE rcvd: 57 Check the firewall logs. Enable logging for the port forward rule if you want it to show up.
   You can see that Dnsmasq forwards to the DNS servers defined under System \u2192  Settings \u2192  General and Unbound.
VLAN30_CLEAR: Local Hostname Lookups #  Run dig opnsense.corp.example.com:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; opnsense.corp.example.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 61385 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;opnsense.corp.example.com. IN A ;; ANSWER SECTION: opnsense.corp.example.com. 1 IN A 192.168.1.1 ;; Query time: 0 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Wed Nov 17 00:45:49 CET 2021 ;; MSG SIZE rcvd: 73 VLAN30_CLEAR: Redirect Outbound DNS Traffic #  Run dig opnsense.org @8.8.8.8:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; opnsense.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 34638 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 430 IN A 178.162.131.118 ;; Query time: 3 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Tue Nov 16 00:12:15 CET 2021 ;; MSG SIZE rcvd: 57 We confirm it works by looking at the firewall logs again:
   VLAN30_CLEAR: Forward Reverse Lookups of Private IPs to Unbound #  Run dig -x 192.168.20.1:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; -x 192.168.20.1 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 20607 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;1.20.168.192.in-addr.arpa. IN PTR ;; ANSWER SECTION: 1.20.168.192.in-addr.arpa. 3600 IN PTR OPNsense.home.schnerring.net. ;; Query time: 1 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Wed Nov 17 00:09:05 CET 2021 ;; MSG SIZE rcvd: 96 The firewall logs confirm it works.
   VLAN30_CLEAR: Verify corp.example.com Is Private #  Run dig nowhere.corp.example.com:
; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; nowhere.corp.example.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 7481 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;nowhere.corp.example.com. IN A ;; AUTHORITY SECTION: corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600 ;; Query time: 0 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Tue Nov 16 20:44:35 CET 2021 ;; MSG SIZE rcvd: 112 This time, requests will only be forwarded to Unbound, but not external DNS resolvers.
   VLAN30_CLEAR: DNS Leak Test #  As we saw earlier, we expect the Quad9 and the Mullvad public IPs to leak. Here is the result of an extended test from dnsleaktest.com:
   VLAN40_GUEST: Test DNS #  Connect to VLAN40_GUEST.
Verify that dig opnsense.org @192.168.40.1 times out.
The Cloudflare DNS servers you configured in the DHCP settings of the Guest VLAN should show up when running the leak test:
   Thanks For Reading \u2764\uFE0F #  If you&rsquo;re here, I thank you for reading all this! Any feedback is highly appreciated.
When I decided to write this guide, I didn&rsquo;t think it would take soooo long. I worked on it intensively for the better part of a month. I spent like a week configuring Unbound to use WireGuard tunnels, only to find out that I couldn&rsquo;t get it working due to a bug. But it was all worth it and a fulfilling journey \u2014 I have learned so much OPNsense and networking. But I&rsquo;m also happy to be able to put this aside for a while. \u{1F389}
So what&rsquo;s next?
Let&rsquo;s Encrypt certificates and HAProxy to secure self-hosted services. I imagine configuring it should be pretty straightforward.
Me and possibly others want to be able to access my home network from the outside via WireGuard. I have a dynamic IP, so I thought I&rsquo;d have to resort to Dynamic DNS. But I think port forwarding with Mullvad is the better solution and doesn&rsquo;t require me to associate my public IP address with a public DNS record.
Traffic shaping and intrusion prevention is something I want to look into, too.
So yeah, OPNsense and I will be friends for a while. \u{1F46B}
`}).add({id:7,href:"/blog/install-qbittorrent-jackett-lidarr-radarr-sonarr-and-plex-inside-truenas-jails/",title:"Install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex inside TrueNAS Jails",description:`This post documents the steps required to install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex in TrueNAS jails version 12.0-U6.
`,content:`This post documents the steps required to install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex in TrueNAS jails version 12.0-U6.
The FN11.3 iocage jails - Plex, Tautulli, Sonarr, Radarr, Lidarr, Jackett, Transmission, Organizr guide inspired me to write this guide. I&rsquo;ll also briefly cover permissions.
Disclaimer #  From Wikipedia on BitTorrent Legislation:
 Although the protocol itself is legal, problems stem from using the protocol to traffic copyright infringing works, since BitTorrent is often used to download otherwise paid content, such as movies and video games.
 See also: Legal issues with BitTorrent (Wikipedia)
I&rsquo;ll only cover options that deviate from the defaults. I use DHCP reservations to manage my server IPs, so I use the dhcp=1 option to create jails.
Note that using hardlinks with the *arrs doesn&rsquo;t work with this setup. For hardlinking to work, the torrent client and *arrs must share the same dataset and jail mount points.
Goals #  The diagram above translates to the following requirements:
 Each service lives inside a separate jail Each service inside the jail runs as a different user Each jail owns a dataset for configuration data Jails share media datasets, but only one user has write permissions. E.g., Radarr can only read (ro) from the torrent dataset but write (rw) to the movies dataset.  Groups #  Navigate to Accounts \u2192  Groups and add the following groups.
         GID 850   Name qbittorrent   Samba Authentication unchecked             GID 354   Name jackett   Samba Authentication unchecked             GID 356   Name lidarr   Samba Authentication unchecked             GID 352   Name radarr   Samba Authentication unchecked             GID 351   Name sonarr   Samba Authentication unchecked             GID 972   Name plex   Samba Authentication unchecked    Users #  Navigate to Accounts \u2192  Users and add the following users.
         Full Name qBittorrent   Username qbittorrent   User ID 850   New Primary Group unchecked   Primary Group qbittorrent   Primary Group qbittorrent   Disable Password Yes   Shell nologin   Samba Authentication unchecked             Full Name Jackett   Username jackett   User ID 354   New Primary Group unchecked   Primary Group jackett   Disable Password Yes   Shell nologin   Samba Authentication unchecked             Full Name Lidarr   Username lidarr   User ID 356   New Primary Group unchecked   Primary Group lidarr   Disable Password Yes   Shell nologin   Samba Authentication unchecked             Full Name Radarr   Username radarr   User ID 352   New Primary Group unchecked   Primary Group radarr   Disable Password Yes   Shell nologin   Samba Authentication unchecked             Full Name Sonarr   Username sonarr   User ID 351   New Primary Group unchecked   Primary Group sonarr   Disable Password Yes   Shell nologin   Samba Authentication unchecked             Full Name Plex   Username plex   User ID 972   New Primary Group unchecked   Primary Group plex   Disable Password Yes   Shell nologin   Samba Authentication unchecked    Datasets #  Navigate to Storage \u2192  Pools and add the datasets and permissions. We&rsquo;ll use default 755 permissions for all datasets.
Jail Config Datasets #           Path /mnt/vault0/apps/qbittorrent   User qbittorrent   Apply User checked   Group qbittorrent   Apply Group checked             Path /mnt/vault0/apps/jackett   User jackett   Apply User checked   Group jackett   Apply Group checked             Path /mnt/vault0/apps/lidarr   User lidarr   Apply User checked   Group lidarr   Apply Group checked             Path /mnt/vault0/apps/radarr   User radarr   Apply User checked   Group radarr   Apply Group checked             Path /mnt/vault0/apps/sonarr   User sonarr   Apply User checked   Group sonarr   Apply Group checked             Path /mnt/vault0/apps/plex   User plex   Apply User checked   Group plex   Apply Group checked    Media Datasets #           Path /mnt/vault0/media/torrents   User qbittorrent   Apply User checked   Group qbittorrent   Apply Group checked             Path /mnt/vault0/media/music   User lidarr   Apply User checked   Group lidarr   Apply Group checked             Path /mnt/vault0/media/movies   User radarr   Apply User checked   Group radarr   Apply Group checked             Path /mnt/vault0/media/series   User sonarr   Apply User checked   Group sonarr   Apply Group checked    Jails #  Connect to TrueNAS via SSH or similar.
qBittorrent Jail #  # Create jail iocage create --name qbittorrent --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec qbittorrent mkdir /mnt/config iocage fstab --add qbittorrent /mnt/vault0/apps/qbittorrent /mnt/config nullfs rw 0 0 # Mount media dataset iocage exec qbittorrent mkdir /mnt/torrents iocage fstab --add qbittorrent /mnt/vault0/media/torrents /mnt/torrents nullfs rw 0 0 # Update packages iocage exec qbittorrent &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install qBittorrent without GUI iocage exec qbittorrent pkg install qbittorrent-nox # Enable qBittorrent service iocage exec qbittorrent sysrc qbittorrent_enable=YES # Configure config directory iocage exec qbittorrent sysrc qbittorrent_conf_dir=/mnt/config # Start the service iocage exec qbittorrent service qbittorrent start Navigate to http://&lt;jail IP&gt;:8080 in your browser and login with the default credentials.
   Username Password     admin adminadmin    Navigate to Tools \u2192  Options... and change the Default Save Path to /mnt/torrents:
   Jackett Jail #  # Create jail iocage create --name jackett --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec jackett mkdir /mnt/config iocage fstab --add jackett /mnt/vault0/apps/jackett /mnt/config nullfs rw 0 0 # Change pkg repository set from \`quarterly\` to \`latest\` # At the time of this writing, \`jackett\` from the quarterly repo wasn&#39;t working iocage exec jackett sed -i &#39;&#39; &#39;s/quarterly/latest/g&#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec jackett &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install iocage exec jackett pkg install jackett # Enable service iocage exec jackett sysrc jackett_enable=YES # Configure config directory iocage exec jackett sysrc jackett_data_dir=/mnt/config # Start the service iocage exec jackett service jackett start Navigate to http://&lt;jail IP&gt;:9117 in your browser to use Jackett.
Lidarr Jail #  # Create jail iocage create --name lidarr --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec lidarr mkdir /mnt/config iocage fstab --add lidarr /mnt/vault0/apps/lidarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec lidarr mkdir /mnt/music iocage fstab --add lidarr /mnt/vault0/media/music /mnt/music nullfs rw 0 0 iocage exec lidarr mkdir /mnt/torrents iocage fstab --add lidarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from \`quarterly\` to \`latest\` iocage exec lidarr sed -i &#39;&#39; &#39;s/quarterly/latest/g&#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec lidarr &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install iocage exec lidarr pkg install lidarr # Enable service iocage exec lidarr sysrc lidarr_enable=YES # Configure config directory iocage exec lidarr sysrc lidarr_data_dir=/mnt/config # Start the service iocage exec lidarr service lidarr start Navigate to http://&lt;jail IP&gt;:8686 in your browser to use Lidarr.
Radarr Jail #  # Create jail iocage create --name radarr --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec radarr mkdir /mnt/config iocage fstab --add radarr /mnt/vault0/apps/radarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec radarr mkdir /mnt/movies iocage fstab --add radarr /mnt/vault0/media/movies /mnt/movies nullfs rw 0 0 iocage exec radarr mkdir /mnt/torrents iocage fstab --add radarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from \`quarterly\` to \`latest\` iocage exec radarr sed -i &#39;&#39; &#39;s/quarterly/latest/g&#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec radarr &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install iocage exec radarr pkg install radarr # Enable service iocage exec radarr sysrc radarr_enable=YES # Configure config directory iocage exec radarr sysrc radarr_data_dir=/mnt/config # Start the service iocage exec radarr service radarr start Navigate to http://&lt;jail IP&gt;:7878 in your browser to use Radarr.
Sonarr Jail #  # Create jail iocage create --name sonarr --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec sonarr mkdir /mnt/config iocage fstab --add sonarr /mnt/vault0/apps/sonarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec sonarr mkdir /mnt/series iocage fstab --add sonarr /mnt/vault0/media/series /mnt/series nullfs rw 0 0 iocage exec sonarr mkdir /mnt/torrents iocage fstab --add sonarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from \`quarterly\` to \`latest\` iocage exec sonarr sed -i &#39;&#39; &#39;s/quarterly/latest/g&#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec sonarr &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install iocage exec sonarr pkg install sonarr # Enable service iocage exec sonarr sysrc sonarr_enable=YES # Configure config directory iocage exec sonarr sysrc sonarr_data_dir=/mnt/config # Start the service iocage exec sonarr service sonarr start Navigate to http://&lt;jail IP&gt;:8989 in your browser to use Sonarr.
Plex Jail #  # Create jail iocage create --name plex --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec plex mkdir /mnt/config iocage fstab --add plex /mnt/vault0/apps/plex /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec plex mkdir /mnt/music iocage fstab --add plex /mnt/vault0/media/music /mnt/music nullfs ro 0 0 iocage exec plex mkdir /mnt/series iocage fstab --add plex /mnt/vault0/media/series /mnt/series nullfs ro 0 0 iocage exec plex mkdir /mnt/movies iocage fstab --add plex /mnt/vault0/media/movies /mnt/movies nullfs ro 0 0 # Update packages iocage exec plex &#34;pkg update &amp;&amp; pkg upgrade&#34; # Install iocage exec plex pkg install plexmediaserver # Enable service iocage exec plex sysrc plexmediaserver_enable=YES # Configure config directory iocage exec plex sysrc plexmediaserver_support_path=/mnt/config # Start the service iocage exec plex service plexmediaserver start Navigate to http://&lt;jail IP&gt;:32400/web in your browser to use Plex.
`}).add({id:8,href:"/blog/router-on-a-stick-vlan-configuration-with-swos-on-the-mikrotik-crs328-24p-4s+rm-switch/",title:"Router on a Stick VLAN Configuration with SwOS on the Mikrotik CRS328-24P-4S+RM Switch",description:`My homelab grew quite a bit over the past years. And with that, my networking needs also changed: stricter firewall rules, segregating untrusted IoT devices into separate networks, traffic prioritization, and more. I wanted to document my switch and VLAN configuration. And maybe this is useful for someone else, too.
`,content:`My homelab grew quite a bit over the past years. And with that, my networking needs also changed: stricter firewall rules, segregating untrusted IoT devices into separate networks, traffic prioritization, and more. I wanted to document my switch and VLAN configuration. And maybe this is useful for someone else, too.
Mikrotik CRS328-24P-4S+RM #  The Mikrotik CRS328-24P-4S+RM is a beefy Layer 3 (L3) switch. It features 24 Gigabit Power over Ethernet (PoE) ports and four 10 Gbps SFP+ ports. It has a 500W power supply, so it&rsquo;ll be able to serve as a core switch for my homelab for a long time. It&rsquo;s rack-mountable and was significantly cheaper than comparable Ubiquiti gear I was considering at the time of the purchase. I also replaced its stock fans with Noctua NF-A4x20s, making it completely silent. As long as I don&rsquo;t use too many PoE devices and keep an eye on the temperatures, these fans will dissipate enough heat for the time being. For routing, I use OPNsense, so I only need the L2 capabilities of the Mikrotik switch, which is why I run it on SwOS instead of RouterOS. So, instructions in this guide also refer to SwOS.
Terminology #  From Wikipedia:
 In computing, a router on a stick, also known as a one-armed router, is a router that has a single physical or logical connection to a network. It is a method of inter-VLAN (virtual local area networks) routing where one router is connected to a switch via a single cable.
 When configuring VLANs, we usually encounter three types of port configurations (Cisco lingo):
 Access Port
Port carrying untagged traffic for one VLAN Trunk Port
Port carrying tagged traffic for multiple VLANs Hybrid Port
Port carrying tagged and untagged traffic for multiple VLANs  The traffic of the native VLAN may traverse a trunk port.
VLAN Overview #  I like the convention of matching the third octet of the IP with the VLAN ID. I.e., assigning the VLAN with the ID 10 the address 192.168.10.0/24. Here is an overview of the VLANs I use:
   Description VLAN ID Subnet     Native 1 192.168.1.0/24   Management 10 192.168.10.0/24   VPN 20 192.168.20.0/24   Clear 30 192.168.30.0/24   Guest 40 192.168.40.0/24    Port Mapping #  Here is how we&rsquo;re going to use the ports of the switch:
   Port Description VLANs     1 Trunk port connecting OPNsense 1 (untagged), 10, 20, 30, 40   2-3 Not used    4 Hybrid port to Ubiquiti Unifi AP 10 (untagged), 20, 30, 40   5-8 Access ports connecting Management VLAN 10   9-12 Access ports connecting VPN VLAN 20   13-16 Access ports connecting Clear VLAN 30   17-20 Access ports connecting Guest VLAN 40   21-24 Access ports connecting native VLAN 1 (untagged)   SFP1-SFP4 Not used     Note the following:
 The trunk port carries all tagged VLAN traffic from the switch to OPNsense The hybrid port carries the tagged traffic of VLANs 20, 30, and 40 made available by the UniFi access point (AP) via WiFi. We also allow untagged VLAN 10 traffic because UniFi devices must communicate over an untagged network to be adopted by a UniFi controller. We could use VLAN 1 for this, but I like to use VLAN 1 for debugging and instead adopt UniFi devices over the management network. For security reasons, connecting to the management network is only allowed via physical cable connection The guest network is only available via WiFi  Configure SwOS #  The only thing missing now is configuring the VLANs. First, we enable Independent VLAN Lookup (IVL) on the System tab. We then add our VLANs and configure port isolation on the VLANs tab:
   On the VLAN tab, we configure trunk, hybrid, and access ports:
   Easy as that! Check the official Mikrotik docs for more VLAN example configurations.
`}).add({id:9,href:"/blog/install-the-ubiquiti-unifi-controller-software-v6-inside-a-truenas-jail/",title:"Install the Ubiquiti Unifi Controller Software v6 Inside a TrueNAS Jail",description:`To manage Ubiquiti UniFi devices, a UniFi controller is required. Over a year ago, I initially installed the controller software inside a Ubuntu VirtualBox VM. Now that version 6 of the UniFi controller software is released, it&rsquo;s time to upgrade. I decided to reinstall the controller inside a TrueNAS jail instead of a VirtualBox VM. When searching the interwebs, I only found lots of outdated instructions. It turns out that it&rsquo;s very straightforward, so here are my quick notes on how to do it.
`,content:`To manage Ubiquiti UniFi devices, a UniFi controller is required. Over a year ago, I initially installed the controller software inside a Ubuntu VirtualBox VM. Now that version 6 of the UniFi controller software is released, it&rsquo;s time to upgrade. I decided to reinstall the controller inside a TrueNAS jail instead of a VirtualBox VM. When searching the interwebs, I only found lots of outdated instructions. It turns out that it&rsquo;s very straightforward, so here are my quick notes on how to do it.
I tested the following on TrueNAS version 12.0-U6.
Install UniFi #  Connect to a shell via Web GUI or SSH and fetch the latest FreeBSD release available. At the time of this writing, it was 12.2-RELEASE.
 iocage fetch   Connect to TrueNAS via shell and create the jail.
 iocage create --name unifi --release 12.2-RELEASE dhcp=1 boot=1   I use DHCP reservations to manage my server IPs. Setting dhcp=1 also sets vnet=1 and bpf=1. Network configuration is out of scope for this guide. Please consult the iocage manual (man iocage) or the TrueNAS jails documentation for more info. Setting boot=1 enables auto-start at boot time.
Connect to the jail.
 iocage console unifi   Once connected, run the following commands.
 pkg update && pkg upgrade -y # install updates pkg install unifi6 # install unifi sysrc unifi_enable=YES # auto-start at boot service unifi start # start unifi   Connect to the UniFi controller at https://&lt;jail IP&gt;:8443.
Updating #  Updating requires very few steps. Please backup the jail before updating to be able to roll back if something goes wrong.
 iocage stop unifi iocage snapshot unifi # OR # iocage export unifi iocage start unifi   Connect to the jail.
 iocage console unifi   Once connected, run the following commands.
 service unifi stop # stop unifi pkg update && pkg upgrade -y # install updates service unifi start # start unifi   And that&rsquo;s it!
`}).add({id:10,href:"/blog/use-custom-dns-servers-with-mullvad-and-any-wireguard-client/",title:"Use Custom DNS Servers With Mullvad And Any WireGuard Client",description:`I&rsquo;ve been using Mullvad VPN for a while now but only ever used it with the official client on my workstation. I use DNS extensively in my home network, so as soon as I activate Mullvad, I can&rsquo;t resolve DNS names locally. Of course, this is by design and expected. I own an OPNsense appliance, so the natural solution is to move the tunnel there.
TL;DR #  Use the following shell command to request an IP with no DNS hijacking:`,content:`I&rsquo;ve been using Mullvad VPN for a while now but only ever used it with the official client on my workstation. I use DNS extensively in my home network, so as soon as I activate Mullvad, I can&rsquo;t resolve DNS names locally. Of course, this is by design and expected. I own an OPNsense appliance, so the natural solution is to move the tunnel there.
TL;DR #  Use the following shell command to request an IP with no DNS hijacking:
curl -sSL https://api.mullvad.net/app/v1/wireguard-keys \\  -H &#34;Content-Type: application/json&#34; \\  -H &#34;Authorization: Token YOURMULLVADACCOUNTNUMBER&#34; \\  -d &#39;{&#34;pubkey&#34;:&#34;YOURWIREGUARDPUBLICKEY&#34;}&#39; Mullvad Hijacks DNS Queries Over WireGuard Tunnels #  Instead of using the OpenVPN protocol, I decided to go with the latest and greatest: WireGuard. OPNsense is a fork of FreeBSD but lacks a kernel implementation of WireGuard, requiring a plugin. It&rsquo;s good enough for me to try, and I hope WireGuard will be natively supported soon.
During my research on how to best configure this, there seemed to be the caveat of Mullvad hijacking DNS traffic going through WireGuard tunnels, redirecting it to their DNS servers. It decreases the likelihood of DNS leaks, but power users like you and I might not want that. What if I want to query DNS root servers through the VPN tunnel because I use my own DNS resolver? Mullvad hijacking DNS queries would make my DNS resolver trip up.
What a bummer, right? Looking at the Mullvad FAQ, it seemed the only solution was to resort to OpenVPN:
 Ports 1400 UDP and 1401 TCP do not have DNS hijacking enabled, which might work better for pfSense users
 But Mullvad launched support for custom DNS servers on the Mullvad VPN app back in April 2021. It also works for WireGuard, so what&rsquo;s the secret?
Reverse-engineering the Mullvad App #  Searching through the docs, I found the WireGuard on a router article explaining how to get an IP to use with Mullvad via API:
curl https://api.mullvad.net/wg/ -d account=YOURMULLVADACCOUNTNUMBER --data-urlencode pubkey=YOURPUBLICKEY Next, let&rsquo;s look at how the app requests IPs. Fortunately it&rsquo;s open-source and available on GitHub, so the only reverse-engineering we&rsquo;re going to be doing is reading some code. It turns out that the app uses a different API to request IPs, found in the push_wg_key function: https://api.mullvad.net/app/v1/wireguard-keys.
Testing Both APIs #  For testing, we&rsquo;ll be using the official WireGuard client. Let&rsquo;s open the client, click Add empty tunnel..., and give it a name:
   The tunnel will initially look like this:
   Copy the public key and execute the following to request our Mullvad IPs:
curl https://api.mullvad.net/wg/ -d account=YOURMULLVADACCOUNTNUMBER --data-urlencode pubkey=YOURPUBLICKEY The response will return an IPv4 and IPv6 address. Add the following to the configuration file:
[Interface] PrivateKey = &lt;PRIVATE KEY&gt; Address = &lt;IPv4 ADRESS&gt; DNS = 9.9.9.9 [Peer] PublicKey = 9hIGjit4ApkNGuEWYBLpahxokEoP0cT9CMZ+ELEygzo= AllowedIPs = 0.0.0.0/0 Endpoint = 194.36.25.18:51820 We use the de24-wireguard Mullvad server as peer and Quad9 as DNS server.
Let&rsquo;s activate the tunnel and browse to Mullvad&rsquo;s connection check:
   As expected, the Quad9 DNS server is not leaking through because Mullvad hijacks our DNS requests and redirects them to their DNS servers.
Next, we use the API the app uses to request the Mullvad IPs. We expect to get a different IP for which DNS hijacking is disabled. Before we can do this, we need to revoke the WireGuard key on the Mullvad website because we already requested an IP for this public key:
   After revoking the key, we run the following command:
curl -sSL https://api.mullvad.net/app/v1/wireguard-keys -H &#34;Content-Type: application/json&#34; -H &#34;Authorization: Token YOURMULLVADACCOUNTNUMBER&#34; -d &#39;{&#34;pubkey&#34;:&#34;YOURPUBLICKEY&#34;}&#39; Next, we replace the IP in Address field of the WireGuard config with the new IP we received. Then we re-activate the tunnel and visit Mullvad&rsquo;s connection check:
   Hooray, the Quad9 DNS servers leak through, so Mullvad is not hijacking our DNS traffic for this tunnel!
`}).add({id:11,href:"/blog/use-the-trezor-hardware-wallet-anonymously-inside-a-virtualbox-whonix-vm-with-external-wallets-like-adalite-and-monero-gui/",title:"Use the Trezor Hardware Wallet Anonymously Inside a VirtualBox Whonix VM With External Wallets Like Adalite and Monero GUI",description:"In the past, I used an old laptop running Qubes OS for any cryptocurrency-related stuff, and it worked great. It&rsquo;s where I first learned about Whonix, a desktop operating system designed to protect your privacy online. Unfortunately, Qubes OS is a bit picky about the hardware it runs on. My old laptop only has four gigs of RAM, and I could barely run two instances of MyEtherWallet in two separate qubes without the system running out of memory.",content:`In the past, I used an old laptop running Qubes OS for any cryptocurrency-related stuff, and it worked great. It&rsquo;s where I first learned about Whonix, a desktop operating system designed to protect your privacy online. Unfortunately, Qubes OS is a bit picky about the hardware it runs on. My old laptop only has four gigs of RAM, and I could barely run two instances of MyEtherWallet in two separate qubes without the system running out of memory.
The Final Straw #  When initially configuring my Qubes OS environment for Monero, I decided to go with the setup described in a user guide from the official Monero website:
 With Qubes + Whonix you can have a Monero wallet that is without networking and running on a virtually isolated system from the Monero daemon which has all of its traffic forced over Tor.
 Out of the blue, after more than a year of using this setup, I couldn&rsquo;t start the daemon and the wallet GUI simultaneously anymore. I only had enough RAM to launch one, but not both, which made the setup unusable. So I had to buy new hardware.
Reducing The Complexity #  I would have loved to get a NitroPad X230, a Qubes OS-certified laptop to tinker around with Qubes OS some more. Also, the guys at Nitrokey are doing a great job in the open-source space and deserve any support they get. But I ultimately decided to reduce the complexity of my crypto setup and went with a hardware wallet instead \u2014 the Trezor Model T.
I&rsquo;ll guide you through my new &ldquo;Whonix on VirtualBox&rdquo; setup and the steps required to configure it.
Import the VirtualBox Whonix Appliance #  First, we download the VirtualBox appliance provided by Whonix and make sure to verify the binary. This requires different steps depending on the OS you are using.
Next, we import the appliance into VirtualBox by clicking File \u2192 Import Appliance&hellip;, creating two VMs \u2014 the gateway and the workstation. The gateway connects us to the Tor network. The workstation will only connect through that gateway, so both VMs must run if we want to connect to the outside world from within the workstation VM.
Make sure to read the Post-installation Security Advice from the official Whonix docs. We change the default user password and install the latest updates for the gateway at the very least. After, we repeat the same for the workstation VM.
At this point, we can optionally clone the workstation VM to have a clean Whonix image if we ever want to start over from scratch.
Install the Trezor Dependencies #  We can now connect the Trezor device to our computer. We then right-click on the workstation VM and select Settings&hellip;. Under USB, we check the Enable USB Controller and add the Trezor device. This way, it will be automatically attached to the workstation VM.
   Let&rsquo;s start the VM, open a terminal, and import the udev rule for Trezor to enable communication with the Trezor device via Linux kernel:
sudo curl https://data.trezor.io/udev/51-trezor.rules -o /etc/udev/rules.d/51-trezor.rules Next, we download the SatoshiLabs 2021 Signing Key:
wget https://trezor.io/security/satoshilabs-2021-signing-key.asc We verify the authenticity of the key by running:
gpg --with-fingerprint ./satoshilabs-2021-signing-key.asc The value of the fingerprint is EB48 3B26 B078 A4AA 1B6F 425E E21B 6950 A2EC B65C. If it&rsquo;s not, something is very wrong \u2014 do NOT continue! If everything looks good, we import the key:
gpg --import ./satoshilabs-2021-signing-key.asc Then we download the Trezor Suite and corresponding signature by running the following in the terminal (replace XX.XX.X with the latest version):
wget https://suite.trezor.io/web/static/desktop/Trezor-Suite-XX.XX.X-linux-x86_64.AppImage wget https://suite.trezor.io/web/static/desktop/Trezor-Suite-XX.XX.X-linux-x86_64.AppImage.asc We verify the binaries by running:
gpg --verify ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage.asc The output should contain:
Good signature from &#34;SatoshiLabs 2021 Signing Key&#34; Next, we make the binary executable:
chmod +x ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage To launch the Trezor suite and connect to our hardware wallet, we run:
./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage At this point, we can shut down the VM and take a snapshot so we can roll back later if we need to. I like to clone this VM again for each cryptocurrency requiring an external wallet. It increases the security and maintainability of the setup.
Using the Monero GUI #  Besides giving the workstation VM a little more RAM, Monero doesn&rsquo;t require additional setup since Whonix includes the Monero GUI. We launch the app and wait for the Monero daemon to sync, which might take a long time using Tor. After that, we&rsquo;re ready to go!
Setting Up Adalite #  Adalite is an open-source, in-browser wallet, and only a few steps are required to get it working.
First, we open the Tor browser and allow pop-up windows from https://adalite.io by adding an exception for it under Settings \u2192 Preferences \u2192 Privacy &amp; Security.
Next, we launch the Trezor Suite by running ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage. It starts the included Trezor Bridge, which Adalite requires to communicate with the Trezor device. We can verify the Trezor Bridge is running by navigating to http://127.0.0.1:21325/status/ in the browser.
The final step is to change two advanced settings of the Tor browser by navigating to about:config and clicking Accept the Risk and Continue. We set the network.proxy.no_proxies_on to 127.0.0.1:21325, so traffic to the Trezor Bridge is not proxied through the Tor network. We also need to disable First-Party Isolation by setting privacy.firstparty.isolate to false.
We can now use Adalite by navigating to https://adalite.io. It might be a good idea to bookmark that address, so you don&rsquo;t fall victim to a phishing attack in case of a typo.
Closing Thoughts #  For my purposes, this setup provides more than enough anonymity. Depending on your needs, it might be overkill or might not be anonymous or secure enough. It always depends on your threat model.
I don&rsquo;t like that SatoshLabs publishes a new signing key every year. Just one more thing to keep in mind come next year.
`}).add({id:12,href:"/blog/create-a-modern-css-only-fold-out-burger-menu/",title:"Create a Modern CSS-only Fold-Out Burger Menu",description:`For the last couple of months, I have been working on a custom Hugo theme in my free time. Most recently, I implemented a CSS-only burger fold-out menu to increase its responsiveness. I based the implementation on Erik Terwan&rsquo;s nifty pure CSS Hamburger fold-out menu which is pretty popular on CodePen. I modernized it by utilizing SVG and newer CSS selectors to make the code more declarative and scalable. It comes with the price of not supporting as many browsers, but honestly, who cares about Internet Explorer users?
`,content:`For the last couple of months, I have been working on a custom Hugo theme in my free time. Most recently, I implemented a CSS-only burger fold-out menu to increase its responsiveness. I based the implementation on Erik Terwan&rsquo;s nifty pure CSS Hamburger fold-out menu which is pretty popular on CodePen. I modernized it by utilizing SVG and newer CSS selectors to make the code more declarative and scalable. It comes with the price of not supporting as many browsers, but honestly, who cares about Internet Explorer users?
 .example { background: #3c3836; color: #ebdbb2; height: 200px; overflow: hidden; position: relative; width: 300px; }  What the Result Looks Like #  Have a look at the result (it&rsquo;s interactive):
 #example-result .menu-burger  * { position: absolute; } #example-result .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } #example-result .menu-burger svg { height: 32px; width: 32px; z-index: 2; } #example-result ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } #example-result input:checked ~ .menu-burger__item-list { transform: none; } #example-result .menu-burger input:checked ~ svg line { stroke: #3c3836; } #example-result .menu-burger svg line:nth-of-type(1) { transform-origin: center 6px; } #example-result .menu-burger svg line:nth-of-type(2) { transform-origin: center 12px; } #example-result .menu-burger svg line:nth-of-type(3) { transform-origin: center 18px; } #example-result .menu-burger svg line { transition-duration: 0.5s; transition-property: stroke, opacity, transform; transition-timing-function: cubic-bezier(0.9, 0, 0.1, 1); } #example-result .menu-burger input:checked ~ svg line:nth-of-type(2) { opacity: 0; transform: scale(0.2); } #example-result .menu-burger input:checked ~ svg line:nth-of-type(1) { transform: translate(0, 6px) rotate(45deg); } #example-result .menu-burger input:checked ~ svg line:nth-of-type(3) { transform: translate(0, -6px) rotate(-45deg); }   Item 1 Item 2 Item 3    All of the illustrating examples use the following basic styling:
.example { background: #3c3836; color: #ebdbb2; height: 200px; overflow: hidden; position: relative; width: 300px; } Burger Anatomy #  As a starting point, I chose the menu-2 icon of the excellent, MIT-licensed Tabler Icon suite:
&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; class=&#34;icon icon-tabler icon-tabler-menu-2&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt; &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34;/&gt; &lt;line x1=&#34;4&#34; y1=&#34;6&#34; x2=&#34;20&#34; y2=&#34;6&#34; /&gt; &lt;line x1=&#34;4&#34; y1=&#34;12&#34; x2=&#34;20&#34; y2=&#34;12&#34; /&gt; &lt;line x1=&#34;4&#34; y1=&#34;18&#34; x2=&#34;20&#34; y2=&#34;18&#34; /&gt; &lt;/svg&gt; The icon looks like this:
 Here is a breakdown of the arrangement of the burger&rsquo;s three &lt;line /&gt; elements inside the SVG:
   When we later animate the lines, this will be important to know.
Menu Skeleton #  &lt;div class=&#34;menu-burger&#34;&gt; &lt;input type=&#34;checkbox&#34; /&gt; &lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; class=&#34;icon icon-tabler icon-tabler-menu-2&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;2&#34; stroke=&#34;currentColor&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; &gt; &lt;path stroke=&#34;none&#34; d=&#34;M0 0h24v24H0z&#34; fill=&#34;none&#34; /&gt; &lt;line x1=&#34;4&#34; y1=&#34;6&#34; x2=&#34;20&#34; y2=&#34;6&#34; /&gt; &lt;line x1=&#34;4&#34; y1=&#34;12&#34; x2=&#34;20&#34; y2=&#34;12&#34; /&gt; &lt;line x1=&#34;4&#34; y1=&#34;18&#34; x2=&#34;20&#34; y2=&#34;18&#34; /&gt; &lt;/svg&gt; &lt;ul class=&#34;menu-burger__item-list&#34;&gt; &lt;li&gt;Item 1&lt;/li&gt; &lt;li&gt;Item 2&lt;/li&gt; &lt;li&gt;Item 3&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; The menu contains the burger and a list of menu items inside .menu-burger__item-list, which initially is not displayed. The &lt;input type=&quot;checkbox&quot; /&gt; is used as an invisible, &ldquo;CSS-only click handler&rdquo; indicating the menu state. Let&rsquo;s add some initial CSS next:
.menu-burger &gt; * { position: absolute; } .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } .menu-burger svg { height: 32px; width: 32px; z-index: 2; } ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } input:checked ~ .menu-burger__item-list { transform: none; } .menu-burger input:checked ~ svg line { stroke: #3c3836; } The most important things going on here are:
 Any direct descendant of .menu-burger is positioned absolutely menu-burger__item-list, svg, and checkbox input are stacked on top of each other, ordered by z-index The dimensions of the input checkbox and svg exactly match To initially hide menu-burger__item-list, it&rsquo;s translated out of view A transition animation with a custom cubic-bezier easing-function is added to the transform property, so the fold-out of the menu looks nice. You can play with it on cubic-bezier.com. The &ldquo;click handler&rdquo; CSS magic is happening in input:checked ~ .menu-burger__item-list. ~ is the subsequent-sibling combinator. In this case it matches .menu-burger__item-list siblings of a :checked input checkbox and undoes the initial hiding transition. The same &ldquo;click handler&rdquo; logic is used to change the colors of the line elements from dark to light  The rest is self-explanatory, generic CSS styling. Here is what we have so far:
 #example-skeleton .menu-burger  * { position: absolute; } #example-skeleton .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } #example-skeleton .menu-burger svg { height: 32px; width: 32px; z-index: 2; } #example-skeleton ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } #example-skeleton input:checked ~ .menu-burger__item-list { transform: none; } .menu-burger input:checked ~ svg line { stroke: #3c3836; }   Item 1 Item 2 Item 3    Now that we finished building the foundation, we can continue with the fun part!
Animate the Burger #  First, we style each &lt;line /&gt; by using declarative nth-of-type CSS selectors:
.collapsible__menu svg line:nth-of-type(1) { stroke: red; } .collapsible__menu svg line:nth-of-type(2) { stroke: green; } .collapsible__menu svg line:nth-of-type(3) { stroke: blue; } This styles the first, second and third line of svg like so:
 #example-rgb svg line:nth-of-type(1) { stroke: red; } #example-rgb svg line:nth-of-type(2) { stroke: green; } #example-rgb svg line:nth-of-type(3) { stroke: blue; }    Let&rsquo;s take a step back and think about what the animation should do:
 Fade out the middle line by scaling it down and changing its opacity to 0 Rotate the top and bottom lines to form an X. One way to do this is to vertically center both lines and rotate them by 45 degrees in opposing directions around their center.  Before we get started, we have to understand how transformation origins work in CSS:
 The transform origin is the point around which a transformation is applied. For example, the transform origin of the rotate() function is the center of rotation.
 In our case, the default transform-origin is (0,0). So before we apply transformations, we have to move the transform-origins of every line to their respective center:
   .menu-burger svg line:nth-of-type(1) { transform-origin: center 6px; } .menu-burger svg line:nth-of-type(2) { transform-origin: center 12px; } .menu-burger svg line:nth-of-type(3) { transform-origin: center 18px; } Next, use the cubic-bezier function again and apply it to every line property to be animated:
.menu-burger svg line { transition-duration: 0.5s; transition-property: stroke, opacity, transform; transition-timing-function: cubic-bezier(0.9, 0, 0.1, 1); } When clicked, we fade out the middle line of the burger:
.menu-burger input:checked ~ svg line:nth-of-type(2) { opacity: 0; transform: scale(0.2); } Finally, we vertically center and then rotate the top and bottom lines:
.menu-burger input:checked ~ svg line:nth-of-type(1) { transform: translate(0, 6px) rotate(45deg); } .menu-burger input:checked ~ svg line:nth-of-type(3) { transform: translate(0, -6px) rotate(-45deg); } Wrapping Up #  What do you think? Let me know in the comments or at me on Twitter.
I created a JSFiddle for you to play around with the code. Here is the entire CSS for reference:
.example { background: #3c3836; color: #ebdbb2; height: 200px; overflow: hidden; position: relative; width: 300px; } .menu-burger &gt; * { position: absolute; } .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } .menu-burger svg { height: 32px; width: 32px; z-index: 2; } ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } input:checked ~ .menu-burger__item-list { transform: none; } .menu-burger input:checked ~ svg line { stroke: #3c3836; } .menu-burger svg line:nth-of-type(1) { transform-origin: center 6px; } .menu-burger svg line:nth-of-type(2) { transform-origin: center 12px; } .menu-burger svg line:nth-of-type(3) { transform-origin: center 18px; } .menu-burger svg line { transition-duration: 0.5s; transition-property: stroke, opacity, transform; transition-timing-function: cubic-bezier(0.9, 0, 0.1, 1); } .menu-burger input:checked ~ svg line:nth-of-type(2) { opacity: 0; transform: scale(0.2); } .menu-burger input:checked ~ svg line:nth-of-type(1) { transform: translate(0, 6px) rotate(45deg); } .menu-burger input:checked ~ svg line:nth-of-type(3) { transform: translate(0, -6px) rotate(-45deg); } `}).add({id:13,href:"/blog/use-sieve-filters-to-auto-sort-your-protonmail-inbox-into-subfolders/",title:"Use Sieve Filters to Auto-Sort Your ProtonMail Inbox into Subfolders",description:`Sieve is a programming language used for email filtering. Today, I show you how I automatically sort my ProtonMail inbox into folders and subfolders using custom sieve filters. My setup uses the catch-all feature requiring at least a ProtonMail Professional subscription and a properly configured custom domain.
`,content:`Sieve is a programming language used for email filtering. Today, I show you how I automatically sort my ProtonMail inbox into folders and subfolders using custom sieve filters. My setup uses the catch-all feature requiring at least a ProtonMail Professional subscription and a properly configured custom domain.
The ProtonMail Bridge has been supporting subfolders for a while now. With the release of the re-designed ProtonMail for web, subfolders are now officially supported. So now is a great time to share my setup with you guys!
The Goal #  I only want emails from unknown senders to land in my inbox, e.g., non-automated messages from people directly contacting me or emails from unknown websites. Sieve filters sort emails from well-known senders into folders and subfolders, like webshops or social media for which I have signed up.
I use root-level folders to categorize my email. Subfolders are for specific websites belonging to a category:
Inbox/ Webshops/ \u251C\u2500 Amazon/ \u251C\u2500 Ex Libris/ Entertainment/ \u251C\u2500 Netflix/ \u251C\u2500 Spotify/ Here, Webshops and Entertainment are the categories, and Amazon and Spotify are specific websites. For each website, I use a separate email address that maps to a pre-created folder. The generic structure of the email address I use to sign up for websites is the following:
category+website@schnerring.net For the example folder structure above, the email-to-folder mapping looks like this:
 webshops+amazon@schnerring.net \u2194 Webshops/Amazon/ webshops+exlibris@schnerring.net \u2194 Webshops/Ex Libris/ entertainment+netflix@schnerring.net \u2194 Entertainment/Netflix/ entertainment+spotify@schnerring.net \u2194 Entertainment/Spotify/ *@schnerring.net \u2194 Inbox/ (catch-all)  The Solution #  You can add custom sieve filters in the ProtonMail web client by navigating to Settings \u2192 Filters \u2192 Add sieve filter. The following snippet meets all of my requirements:
require [&#34;include&#34;, &#34;variables&#34;, &#34;fileinto&#34;, &#34;envelope&#34;]; if envelope :localpart :matches &#34;To&#34; &#34;*+*&#34; { set :lower &#34;category&#34; &#34;\${1}&#34;; set :lower &#34;website&#34; &#34;\${2}&#34;; } else { return; } if string :is &#34;\${website}&#34; &#34;exlibris&#34; { fileinto &#34;\${category}/Ex Libris&#34;; } else { fileinto &#34;\${category}&#34;; fileinto &#34;\${category}/\${website}&#34;; } The require command in the first line is used to load extensions that provide functionality, like fileinto to file messages into folders or variables to declare variables.
Next, the if-conditional checks the :localpart, the part before the @ symbol, of the To address. If it matches the pattern *+* (category+website):
 the value of the category variable is set to the match left of + symbol (\${1}) the value of the website variable is set to the match right of the + symbol (\${2}).  If :localpart does not match the pattern (else), the sieve filter exits (return).
Next, the actual sorting happens. Let&rsquo;s look into the else case first:
} else { fileinto &#34;\${category}&#34;; fileinto &#34;\${category}/\${website}&#34;; } Most of the magic happens here. The email is first moved to the category folder and then moved to the more specific website folder. This way, if the website subfolder does not (yet) exist, the email is at least moved into the root-level category folder. I find this useful when only ordering at a webshop once via guest checkout instead of signing up. This way, I don&rsquo;t need to bother creating an extra subfolder that will only ever contain one or two emails.
Lastly, let&rsquo;s cover the exception for the exlibris webshop in the if-part:
if string :is &#34;\${website}&#34; &#34;exlibris&#34; { fileinto &#34;\${category}/Ex Libris&#34;; } It is just a cosmetic exception. When I register at a webshop with webshops+exlibris@schnerring.net but want to map it to a folder called Ex Libris, an explicit mapping is required. Note that fileinto is not case-sensitive, so it is valid if the email address is webshops+amazon@schnerring.net, but the folder name is Webshops/Amazon/. It also is easily extensible by adding more elseif conditions.
For more details on advanced custom sieve filtering, check the official ProtonMail documentation.
`}).add({id:14,href:"/blog/set-up-azure-active-directory-domain-services-aadds-with-terraform/",title:"Set Up Azure Active Directory Domain Services (AADDS) with Terraform",description:`Update 2022-02-20 #  This guide is outdated! Please click here to go to the new guide I wrote.
`,content:`Update 2022-02-20 #  This guide is outdated! Please click here to go to the new guide I wrote.
Update 2021-08-03 #  With v2.69.0 of the official Terraform azurerm provider released two weeks ago, the active_directory_domain_service resource is now available. If you are freshly adding AADDS, there is no point in reading any further \u2014 use the official resources.
 Bringing traditional Active Directory Domain Services (AD DS) to the cloud, typically required to set up, secure, and maintain domain controllers (DCs). Azure Active Directory Domain Services (AADDS or Azure AD DS) is a Microsoft-managed solution, providing a subset of traditional AD DS features without the need to self-manage DCs.
One such service that requires AD DS features is Azure Virtual Desktop (WVD). I have successfully deployed WVD with Terraform, but until recently, I struggled to do the same with AADDS. Today, I show you how to deploy AADDS with Terraform.
If you are lazy, you can skip to the end and use the custom Terraform module I published to the Terraform Registry.
Prerequisites #  Before getting started, we need the following things:
 Active Azure subscription Azure Active Directory (Azure AD or AAD) tenant Install Terraform  Building Blocks #  So how do we figure out what the required resources are to deploy AADDS? By reverse-engineering the AADDS configuration wizard in the Azure Portal! We launch it by adding a new managed domain after navigating to Azure AD Domain Services.
What we find is that the following resources are required:
 Resource group Virtual network and subnet AAD DC Administrators user group  Using default values for the remaining configuration options, we can download an Azure Resource Manager (ARM) template in the final review step of the wizard.
Analyzing the ARM template reveals that besides the resource group, virtual network, and subnet, a Network Security Group (NSG) with the security rules required for AADDS is added.
The ARM template also contains the Microsoft.AAD/domainServices resource, with its parameters set to the configuration options from the wizard. The following is version 2021-03-01 of the ARM template format from the official Azure Template documentation:
{ &#34;name&#34;: &#34;string&#34;, &#34;type&#34;: &#34;Microsoft.AAD/domainServices&#34;, &#34;apiVersion&#34;: &#34;2021-03-01&#34;, &#34;location&#34;: &#34;string&#34;, &#34;tags&#34;: {}, &#34;properties&#34;: { &#34;domainName&#34;: &#34;string&#34;, &#34;replicaSets&#34;: [ { &#34;location&#34;: &#34;string&#34;, &#34;subnetId&#34;: &#34;string&#34; } ], &#34;ldapsSettings&#34;: { &#34;ldaps&#34;: &#34;string&#34;, &#34;pfxCertificate&#34;: &#34;string&#34;, &#34;pfxCertificatePassword&#34;: &#34;string&#34;, &#34;externalAccess&#34;: &#34;string&#34; }, &#34;resourceForestSettings&#34;: { &#34;settings&#34;: [ { &#34;trustedDomainFqdn&#34;: &#34;string&#34;, &#34;trustDirection&#34;: &#34;string&#34;, &#34;friendlyName&#34;: &#34;string&#34;, &#34;remoteDnsIps&#34;: &#34;string&#34;, &#34;trustPassword&#34;: &#34;string&#34; } ], &#34;resourceForest&#34;: &#34;string&#34; }, &#34;domainSecuritySettings&#34;: { &#34;ntlmV1&#34;: &#34;string&#34;, &#34;tlsV1&#34;: &#34;string&#34;, &#34;syncNtlmPasswords&#34;: &#34;string&#34;, &#34;syncKerberosPasswords&#34;: &#34;string&#34;, &#34;syncOnPremPasswords&#34;: &#34;string&#34;, &#34;kerberosRc4Encryption&#34;: &#34;string&#34;, &#34;kerberosArmoring&#34;: &#34;string&#34; }, &#34;domainConfigurationType&#34;: &#34;string&#34;, &#34;sku&#34;: &#34;string&#34;, &#34;filteredSync&#34;: &#34;string&#34;, &#34;notificationSettings&#34;: { &#34;notifyGlobalAdmins&#34;: &#34;string&#34;, &#34;notifyDcAdmins&#34;: &#34;string&#34;, &#34;additionalRecipients&#34;: [&#34;string&#34;] } } } The docs also reference the Azure Resource Manager QuickStart Template on GitHub. Its README confirms our previous findings but shows that the configuration wizard also must perform the following steps under the hood:
 Register the Azure Active Directory Application Service Principal 2565bd9d-da50-47d4-8b85-4c97f669dc36 Register the Microsoft.AAD Resource Provider  With everything figured out, we can continue with the fun part: Terraform!
Putting Everything Together #  Let&rsquo;s register the service principal and resource provider first:
resource &#34;azuread_service_principal&#34; &#34;aadds&#34; { application_id = &#34;2565bd9d-da50-47d4-8b85-4c97f669dc36&#34; } resource &#34;azurerm_resource_provider_registration&#34; &#34;aadds&#34; { name = &#34;Microsoft.AAD&#34; } Next, we add the AAD DC Administrators user group:
resource &#34;azuread_group&#34; &#34;aadds&#34; { display_name = &#34;AAD DC Administrators&#34; description = &#34;Delegated group to administer Azure AD Domain Services&#34; } Adding the resource group, virtual network, subnet, and NSG is pretty straightforward:
resource &#34;azurerm_resource_group&#34; &#34;aadds&#34; { name = &#34;aadds-rg&#34; location = &#34;Switzerland North&#34; } resource &#34;azurerm_virtual_network&#34; &#34;aadds&#34; { name = &#34;aadds-vnet&#34; resource_group_name = azurerm_resource_group.aadds.name location = &#34;Switzerland North&#34; address_space = [&#34;10.0.0.0/16&#34;]# AADDS DCs  dns_servers = [&#34;10.0.0.4&#34;, &#34;10.0.0.5&#34;] } resource &#34;azurerm_subnet&#34; &#34;aadds&#34; { name = &#34;aadds-snet&#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [&#34;10.0.0.0/24&#34;] } resource &#34;azurerm_network_security_group&#34; &#34;aadds&#34; { name = &#34;aadds-nsg&#34; location = &#34;Switzerland North&#34; resource_group_name = azurerm_resource_group.aadds.name security_rule { name = &#34;AllowRD&#34; access = &#34;Allow&#34; priority = 201 direction = &#34;Inbound&#34; protocol = &#34;Tcp&#34; source_address_prefix = &#34;CorpNetSaw&#34; source_port_range = &#34;*&#34; destination_address_prefix = &#34;*&#34; destination_port_range = &#34;3389&#34; } security_rule { name = &#34;AllowPSRemoting&#34; access = &#34;Allow&#34; priority = 301 direction = &#34;Inbound&#34; protocol = &#34;Tcp&#34; source_address_prefix = &#34;AzureActiveDirectoryDomainServices&#34; source_port_range = &#34;*&#34; destination_address_prefix = &#34;*&#34; destination_port_range = &#34;5986&#34; } } resource &#34;azurerm_subnet_network_security_group_association&#34; &#34;aadds&#34; { subnet_id = azurerm_subnet.aadds.id network_security_group_id = azurerm_network_security_group.aadds.id } Make sure to set dns_servers to the IP addresses of the DCs. You can find them on the Overview page of the managed domain after the deployment succeeded.
The final step is to add the AADDS deployment. Define the ARM template as Terraform templatefile named aadds-arm-template.tpl.json:
{ &#34;$schema&#34;: &#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&#34;, &#34;contentVersion&#34;: &#34;1.0.0.0&#34;, &#34;resources&#34;: [ { &#34;name&#34;: &#34;\${domainName}&#34;, &#34;type&#34;: &#34;Microsoft.AAD/domainServices&#34;, &#34;apiVersion&#34;: &#34;2021-03-01&#34;, &#34;location&#34;: &#34;\${location}&#34;, &#34;tags&#34;: \${tags}, &#34;properties&#34;: { &#34;domainName&#34;: &#34;\${domainName}&#34;, &#34;replicaSets&#34;: [ { &#34;location&#34;: &#34;\${location}&#34;, &#34;subnetId&#34;: &#34;\${subnetId}&#34; } ], &#34;domainSecuritySettings&#34;: { &#34;ntlmV1&#34;: &#34;\${ntlmV1}&#34;, &#34;tlsV1&#34;: &#34;\${tlsV1}&#34;, &#34;syncNtlmPasswords&#34;: &#34;\${syncNtlmPasswords}&#34;, &#34;syncKerberosPasswords&#34;: &#34;\${syncKerberosPasswords}&#34;, &#34;syncOnPremPasswords&#34;: &#34;\${syncOnPremPasswords}&#34;, &#34;kerberosRc4Encryption&#34;: &#34;\${kerberosRc4Encryption}&#34;, &#34;kerberosArmoring&#34;: &#34;\${kerberosArmoring}&#34; }, &#34;domainConfigurationType&#34;: &#34;\${domainConfigurationType}&#34;, &#34;sku&#34;: &#34;\${sku}&#34;, &#34;filteredSync&#34;: &#34;\${filteredSync}&#34;, &#34;notificationSettings&#34;: { &#34;notifyGlobalAdmins&#34;: &#34;\${notifyGlobalAdmins}&#34;, &#34;notifyDcAdmins&#34;: &#34;\${notifyDcAdmins}&#34;, &#34;additionalRecipients&#34;: \${additionalRecipients} } } } ] } We then populate its values dynamically like so:
resource &#34;azurerm_resource_group_template_deployment&#34; &#34;aadds&#34; { name = &#34;aadds-deploy&#34; resource_group_name = azurerm_resource_group.aadds.name deployment_mode = &#34;Incremental&#34; template_content = templatefile( &#34;\${path.module}/aadds-arm-template.tpl.json&#34;, {# Basics  &#34;domainName&#34; = &#34;aadds.schnerring.net&#34; &#34;location&#34; = &#34;Switzerland North&#34; &#34;sku&#34; = &#34;Standard&#34; &#34;domainConfigurationType&#34; = &#34;FullySynced&#34;# Networking  &#34;subnetId&#34; = azurerm_subnet.aadds.id# Administration  &#34;notifyGlobalAdmins&#34; = &#34;Enabled&#34; &#34;notifyDcAdmins&#34; = &#34;Enabled&#34; &#34;additionalRecipients&#34; = jsonencode([])# Synchronization  &#34;filteredSync&#34; = &#34;Enabled&#34;# Security  &#34;tlsV1&#34; = &#34;Enabled&#34; &#34;ntlmV1&#34; = &#34;Enabled&#34; &#34;syncNtlmPasswords&#34; = &#34;Enabled&#34; &#34;syncOnPremPasswords&#34; = &#34;Enabled&#34; &#34;kerberosRc4Encryption&#34; = &#34;Enabled&#34; &#34;syncKerberosPasswords&#34; = &#34;Enabled&#34; &#34;kerberosArmoring&#34; = &#34;Disabled&#34;# Tags  &#34;tags&#34; = jsonencode({}) } ) depends_on = [azurerm_resource_provider_registration.aadds] } Run terraform apply to deploy everything. It takes around 45 minutes to complete.
Wrapping Up #  I created a custom module wrapping the above functionality and published it to the Terraform Registry. You can also find the code on my GitHub along with some examples. I decided that the creation of network resources is out of the module&rsquo;s scope. Depending on what network topology you prefer, pre-provisioning the virtual network and subnet gives you more flexibility.
The module provides the same options as the Azure Portal configuration wizard. More advanced configuration options like LDAP and forests are not yet supported. Feel free to comment below, or open an issue or pull request on GitHub if you find something to improve.
A minimal deployment with the custom module looks like this:
resource &#34;azurerm_resource_group&#34; &#34;aadds&#34; { name = &#34;aadds-rg&#34; location = &#34;Switzerland North&#34; } resource &#34;azurerm_virtual_network&#34; &#34;aadds&#34; { name = &#34;aadds-vnet&#34; resource_group_name = azurerm_resource_group.aadds.name location = &#34;Switzerland North&#34; address_space = [&#34;10.0.0.0/16&#34;]# AADDS DCs  dns_servers = [&#34;10.0.0.4&#34;, &#34;10.0.0.5&#34;] } resource &#34;azurerm_subnet&#34; &#34;aadds&#34; { name = &#34;aadds-snet&#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [&#34;10.0.0.0/24&#34;] } module &#34;aadds&#34; { source = &#34;schnerring/aadds/azurerm&#34; version = &#34;0.1.1&#34; resource_group_name = azurerm_resource_group.aadds.name location = &#34;Switzerland North&#34; domain_name = &#34;aadds.schnerring.net&#34; subnet_id = azurerm_subnet.aadds.id } `}).add({id:15,href:"/blog/deploy-a-matrix-homeserver-to-azure-kubernetes-service-aks-with-terraform/",title:"Deploy a Matrix Homeserver to Azure Kubernetes Service (AKS) with Terraform",description:`Did you ever think about running a Matrix homeserver? In this post, we will set one up on the Azure Kubernetes Service (AKS). We will use the reference homeserver implementation, which is Synapse from the folks at matrix.org. This post focuses on the Kubernetes stuff, keeping Synapse configuration to a minimum. Things like federation, delegation and PostgreSQL are out of scope, because plenty of excellent guides and the official documentation exist covering that. The icing on the cake will be the Synapse Admin UI deployment with secure access to the administration endpoints to make management of our homeserver easier.
`,content:`Did you ever think about running a Matrix homeserver? In this post, we will set one up on the Azure Kubernetes Service (AKS). We will use the reference homeserver implementation, which is Synapse from the folks at matrix.org. This post focuses on the Kubernetes stuff, keeping Synapse configuration to a minimum. Things like federation, delegation and PostgreSQL are out of scope, because plenty of excellent guides and the official documentation exist covering that. The icing on the cake will be the Synapse Admin UI deployment with secure access to the administration endpoints to make management of our homeserver easier.
Preface #  Besides some basic knowledge about AKS, Kubernetes, and Terraform, we have to set up a couple of other things before getting started.
We need an AKS cluster with a configured Ingress Controller to be able to expose our homeserver to the world. I use Traefik 2 in combination with its Kubernetes Ingress implementation.
Synapse requires valid TLS certificates to work. It ships used to ship with functionality to automatically provision Let&rsquo;s Encrypt certificates. However, I use cert-manager as a certificate management solution for all my services. So we skip over that part of the configuration, too.
The last thing we have to set up is Terraform with a properly configured kubernetes provider. If you do not want to use Terraform, transforming the code to regular YAML manifests is trivial.
Depending on your needs, reverse proxy (ingress) functionality and certificate management is the part where your setup differs the most. If you are starting out from scratch, check out my previous post, covering much of the steps required to set up the AKS cluster.
Ingress #  We work from the outside to the inside. The Ingress is what exposes our Kubernetes Service to the public internet. We also add a Namespace for all our Matrix resources to a new Terraform file matrix.tf:
resource &#34;kubernetes_namespace&#34; &#34;matrix&#34; { metadata { name = &#34;matrix&#34; } } resource &#34;kubernetes_ingress&#34; &#34;matrix&#34; { metadata { name = &#34;matrix-ing&#34; namespace = kubernetes_namespace.matrix.metadata.0.name annotations = { &#34;cert-manager.io/cluster-issuer&#34; = &#34;letsencrypt-staging&#34; &#34;traefik.ingress.kubernetes.io/router.tls&#34; = &#34;true&#34; } } spec { rule { host = var.synapse_server_name http { path { path = &#34;/_matrix&#34; backend { service_name = &#34;matrix-svc&#34; service_port = 8008 } } path { path = &#34;/_synapse/client&#34; backend { service_name = &#34;matrix-svc&#34; service_port = 8008 } } path { path = &#34;/&#34; backend { service_name = &#34;matrix-admin-svc&#34; service_port = 8080 } } } } tls { secret_name = &#34;matrix-tls-secret&#34; hosts = [var.synapse_server_name] } } } To route external traffic to the Synapse Service (matrix-svc), we set up the /_matrix and /_synapse/client endpoints according to the official reverse proxy documentation. The / endpoint routes traffic to the Synapse Admin UI (matrix-admin-svc). We do NOT expose the /_synapse/admin endpoint. We will look at how to access this endpoint at the end of this post.
After we finish testing, we must change the certificate issuer from letsencrypt-staging to letsencrypt-production. Later on, we will also define the var.synapse_server_name Terraform input variable.
Services #  Services expose our Synapse and Synapse Admin UI deployments as network services inside our Kubernetes cluster:
resource &#34;kubernetes_service&#34; &#34;matrix&#34; { metadata { name = &#34;matrix-svc&#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { selector = { &#34;app&#34; = &#34;matrix&#34; } port { port = 8008 target_port = 8008 } } } resource &#34;kubernetes_service&#34; &#34;matrix_admin&#34; { metadata { name = &#34;matrix-admin-svc&#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { selector = { &#34;app&#34; = &#34;matrix-admin&#34; } port { port = 8080 target_port = 80 } } } Pretty self-explanatory. We map the service ports to container ports of the deployments that we add in a later step.
Configuration Files #  Before adding the deployment to our cluster, we generate the initial configuration files for Synapse:
 homeserver.yaml: Synapse configuration matrix.schnerring.net.log.config: logging configuration matrix.schnerring.net.signature.key: the key, Synapse signs messages with  We can do that with the generate command which is part of Synapse:
kubectl run matrix-generate-pod \\  --stdin \\  --rm \\  --restart=Never \\  --command=true \\  --image matrixdotorg/synapse:latest \\  --env=&#34;SYNAPSE_REPORT_STATS=yes&#34; \\  --env=&#34;SYNAPSE_SERVER_NAME=matrix.schnerring.net&#34; \\  -- bash -c &#34;/start.py generate &amp;&amp; sleep 300&#34; This command creates a pod, runs generate, sleeps for 300 seconds, and then cleans itself up. Five minutes should be enough time to copy the generated files from the container before it terminates. The following command copies the entire /data directory from the container to a local synapse-config directory:
kubectl cp matrix-generate-pod:/data synapse-config We then store the three configuration files as Secret on the cluster. But before, make sure to change the value of handlers.file.filename from /homeserver.log to /data/homeserver.log inside the *.log.config file. Otherwise, you will run into a permission issue caused by the default logger configuration that I discovered.
locals { synapse_log_config = &#34;/data/\${var.synapse_server_name}.log.config&#34; synapse_signing_key_path = &#34;/data/\${var.synapse_server_name}.signing.key&#34; } resource &#34;kubernetes_secret&#34; &#34;matrix&#34; { metadata { name = &#34;matrix-secret&#34; namespace = kubernetes_namespace.matrix.metadata.0.name }# See also: https://github.com/matrix-org/synapse/blob/master/docker/README.md#generating-a-configuration-file  data = { &#34;homeserver.yaml&#34; = templatefile( &#34;\${path.module}/synapse-config/homeserver.tpl.yaml&#34;, { &#34;server_name&#34; = var.synapse_server_name &#34;report_stats&#34; = var.synapse_report_stats &#34;log_config&#34; = local.synapse_log_config &#34;signing_key_path&#34; = local.synapse_signing_key_path &#34;registration_shared_secret&#34; = var.synapse_registration_shared_secret &#34;macaroon_secret_key&#34; = var.synapse_macaroon_secret_key &#34;form_secret&#34; = var.synapse_form_secret } ) &#34;log.config&#34; = templatefile( &#34;\${path.module}/synapse-config/log.tpl.config&#34;, { &#34;log_filename&#34; = &#34;/data/homeserver.log&#34; &#34;log_level&#34; = &#34;INFO&#34; } ) &#34;signing.key&#34; = var.synapse_signing_key } } To protect the signing key and the secrets contained inside the homeserver.yaml file, we use the Terraform templatefile() function, which allows us to put variable placeholders into the configuration files that are interpolated during terraform apply. This way, we can commit the configuration files to source control securely. To denote the files as template files, we change the file names to homeserver.tpl.yaml and log.tpl.config. Here is an example of how to define interpolation sequences inside homeserver.yaml:
server_name: &#34;\${server_name}&#34; # ... # a secret which is used to sign access tokens. If none is specified, # the registration_shared_secret is used, if one is given; otherwise, # a secret key is derived from the signing key. # macaroon_secret_key: &#34;\${macaroon_secret_key}&#34; # a secret which is used to calculate HMACs for form values, to stop # falsification of values. Must be specified for the User Consent # forms to work. # form_secret: &#34;\${form_secret}&#34; Of course, you can also check out the config file templates on my GitHub.
To find the values inside the huge homeserver.yaml file, we use the following regular expression. It matches any line that is not a comment or whitespace:
^(?!^\\s*#)^(?!\\n).* All we have to do now is define the Terraform input variables and set them to the values we originally generated:
variable &#34;synapse_image_version&#34; { type = string description = &#34;Synapse image version.&#34; default = &#34;v1.33.2&#34; } variable &#34;synapse_server_name&#34; { type = string description = &#34;Public Synapse hostname.&#34; } variable &#34;synapse_report_stats&#34; { type = bool description = &#34;Enable anonymous statistics reporting.&#34; } variable &#34;synapse_signing_key&#34; { type = string description = &#34;Signing key Synapse signs messages with.&#34; sensitive = true } variable &#34;synapse_registration_shared_secret&#34; { type = string description = &#34;Allows registration of standard or admin accounts by anyone who has the shared secret.&#34; sensitive = true } variable &#34;synapse_macaroon_secret_key&#34; { type = string description = &#34;Secret which is used to sign access tokens.&#34; sensitive = true } variable &#34;synapse_form_secret&#34; { type = string description = &#34;Secret which is used to calculate HMACs for form values.&#34; sensitive = true } Persistent Volume Claim #  To be able to persist media, we create a PersistentVolumeClaim (PVC):
resource &#34;kubernetes_persistent_volume_claim&#34; &#34;matrix&#34; { metadata { name = &#34;matrix-pvc&#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { access_modes = [&#34;ReadWriteOnce&#34;] resources { requests = { &#34;storage&#34; = &#34;4Gi&#34; } } } } Note that the default AKS StorageClass has the ReclaimPolicy set to Delete . I recommend defining a custom storage class for production. Setting its reclaim policy to Retain makes accidentally purging the media PVC much harder.
Synapse #  Let us finally deploy Synapse. We just need to mount the configuration files and the PVC we just defined:
resource &#34;kubernetes_deployment&#34; &#34;matrix&#34; { metadata { name = &#34;matrix-deploy&#34; namespace = kubernetes_namespace.matrix.metadata.0.name labels = { &#34;app&#34; = &#34;matrix&#34; } } spec { replicas = 1 selector { match_labels = { &#34;app&#34; = &#34;matrix&#34; } } strategy { type = &#34;Recreate&#34; } template { metadata { labels = { &#34;app&#34; = &#34;matrix&#34; } } spec { hostname = &#34;matrix&#34; restart_policy = &#34;Always&#34; security_context { run_as_user = &#34;991&#34; run_as_group = &#34;991&#34; fs_group = &#34;991&#34; run_as_non_root = true } container { name = &#34;synapse&#34; image = &#34;matrixdotorg/synapse:\${var.synapse_image_version}&#34; security_context { read_only_root_filesystem = true } port { container_port = 8008 } volume_mount { mount_path = &#34;/data&#34; name = &#34;data-vol&#34; } volume_mount { name = &#34;secret-vol&#34; mount_path = &#34;/data/homeserver.yaml&#34; sub_path = &#34;homeserver.yaml&#34; read_only = true } volume_mount { name = &#34;secret-vol&#34; mount_path = local.synapse_log_config sub_path = &#34;log.config&#34; read_only = true } volume_mount { name = &#34;secret-vol&#34; mount_path = local.synapse_signing_key_path sub_path = &#34;signing.key&#34; read_only = true } } volume { name = &#34;data-vol&#34; persistent_volume_claim { claim_name = &#34;matrix-pvc&#34; } } volume { name = &#34;secret-vol&#34; secret { secret_name = &#34;matrix-secret&#34; } } } } } } It is good practice to lock down the container by making the root filesystem read-only, if possible. We also run the container as the 991 user and group, respectively, the default user used by Synapse.
To deploy everything, we run the following commands:
terraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan After the deployment succeeds, do not forget to change the cluster issuer to letsencrypt-production and terraform apply again.
Next, we register a new user. Synapse includes the register_new_matrix_user command. First, we query the name of the pod:
kubectl get pods --namespace matrix We then run the following to connect to the pod:
kubectl exec --stdin --tty --namespace matrix matrix-deploy-xxxxxxxxxx-xxxxx -- bash Now we register the user:
register_new_matrix_user \\  --admin \\  --user michael \\  --config /data/homeserver.yaml \\  http://localhost:8008 We use Element or any other Matrix client and sign in. Great!
Synapse Admin UI #  The Synapse API includes administration endpoints but lacks a UI for administration tasks. There is an open GitHub issue which seems to be inactive. However, awesometechnologies/synapse-admin is being actively developed and works well.
We already configured the ingress to route traffic to the matrix-admin-svc, so the only thing missing is the deployment:
resource &#34;kubernetes_deployment&#34; &#34;matrix_admin&#34; { metadata { name = &#34;matrix-admin-deploy&#34; namespace = kubernetes_namespace.matrix.metadata.0.name labels = { &#34;app&#34; = &#34;matrix-admin&#34; } } spec { replicas = 1 selector { match_labels = { &#34;app&#34; = &#34;matrix-admin&#34; } } strategy { type = &#34;Recreate&#34; } template { metadata { labels = { &#34;app&#34; = &#34;matrix-admin&#34; } } spec { hostname = &#34;matrix-admin&#34; restart_policy = &#34;Always&#34; container { name = &#34;synapse-admin&#34; image = &#34;awesometechnologies/synapse-admin:latest&#34; port { container_port = 80 } } } } } } No configuration is required because the Admin UI is a client-side application. After running terraform apply, we can browse to matrix.schnerring.net to access the app:
   Synapse Admin UI requires access to the _synapse/admin endpoint. But we do not want to expose that endpoint to the public internet, so we have to connect to it by other means. kubectl port-forward allows us to securely forward a local port to a port on a Kubernetes service:
kubectl port-forward service/matrix-svc --namespace matrix 8008:8008 We can now enter http://localhost:8008 as homeserver URL and login to the admin UI with the user we created earlier:
   What&rsquo;s Next? #  Regardless of what you plan on doing with your homeserver, replacing SQLite with PostgreSQL should be at the top of the priority list. Other than that, there is tons of other stuff to configure depending on your needs.
I am super happy with what we have built and curious about what you think. I would appreciate it if you shared your opinion in the comments below.
`}).add({id:16,href:"/blog/use-terraform-to-deploy-the-remark42-commenting-system-to-kubernetes-and-integrate-it-with-a-hugo-website/",title:"Use Terraform to Deploy the Remark42 Commenting System to Kubernetes and Integrate it with a Hugo Website",description:`Building upon our previous work, we will deploy Remark42 on Kubernetes with Terraform and integrate it with your existing Hugo website. Make sure to check out my previous posts about creating a Hugo Website and deploying an Azure Kubernetes Service cluster if you haven&rsquo;t already.
`,content:`Building upon our previous work, we will deploy Remark42 on Kubernetes with Terraform and integrate it with your existing Hugo website. Make sure to check out my previous posts about creating a Hugo Website and deploying an Azure Kubernetes Service cluster if you haven&rsquo;t already.
About Remark42 #   Remark42 is a self-hosted, lightweight, and simple (yet functional) commenting system, which doesn&rsquo;t spy on users.
 I like simplicity, I am a privacy enthusiast, and I build this blog with this in mind. More popular, hands-off solutions like Disqus offer easier integration and more sophisticated features, like automated spam moderation and advertising. But for my intents, it&rsquo;s too bloated and invasive. For low-traffic websites like mine, Remark42 is just the better fit.
Preparation #  Besides a Hugo website and a Kubernetes cluster, you will have to install the following software on your workstation:
 Terraform kompose (optional)  Converting the original docker-compose.yml #  The official Remark42 repository includes a docker-compose.yml that we can download:
curl -O https://raw.githubusercontent.com/umputun/remark42/master/docker-compose.yml We then run kompose convert to generate regular Kubernetes YAML manifests from the docker-compose.yml file:
Kubernetes file &#34;remark-deployment.yaml&#34; created Kubernetes file &#34;remark-claim0-persistentvolumeclaim.yaml&#34; created To figure out what resources we need to create, we use the generated manifests as a starting point.
Create a Namespace #  First, we create the Namespace where our Remark42 resources will reside. We add the following Terraform code to a new file named remark42.yml:
resource &#34;kubernetes_namespace&#34; &#34;remark42&#34; { metadata { name = &#34;remark42&#34; } } Deploy the changes by running:
terraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan Create the Persistent Volume Claim #  To enable persistence for Remark42, we create a PersistentVolumeClaim (PVC) resource. Using the generate remark-claim0-persistentvolumeclaim.yaml file as a blueprint, we can easily derive the Terraform equivalent from it and add it to the remark42.tf file:
resource &#34;kubernetes_persistent_volume_claim&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-pvc&#34; namespace = kubernetes_namespace.remark42.metadata.0.name labels = { &#34;app&#34; = &#34;remark42-pvc&#34; } } spec { access_modes = [&#34;ReadWriteOnce&#34;] storage_class_name = &#34;azurefile&#34; resources { requests = { &#34;storage&#34; = &#34;1Gi&#34; } } } } AKS comes pre-configured with multiple StorageClasses. Here, I use the azurefile storage class to dynamically provision a persistent volume with Azure Files. At the time of this writing, I use a B2ms-sized VM as a node which is limited to four data disks. Using Azure Files whenever possible helps me circumventing this limitation.
Create ConfigMap and Secret #  To store configuration parameters for our Remark42 deployment, we make use of Kubernetes ConfigMap and Secret resources.
To store sensitive values, we use Secret:
resource &#34;kubernetes_secret&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-secret&#34; namespace = kubernetes_namespace.remark42.metadata.0.name } data = { &#34;SECRET&#34; = random_password.remark42_secret.result } } For non-sensitive stuff, we use ConfigMap:
resource &#34;kubernetes_config_map&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-cm&#34; namespace = kubernetes_namespace.remark42.metadata.0.name } data = { &#34;REMARK_URL&#34; = &#34;https://\${cloudflare_record.remark42.hostname}&#34; &#34;SITE&#34; = &#34;schnerring.net&#34; } } For this post, I have kept the configuration to a minimum:
 REMARK_URL: URL to Remark42 server SITE: site name(s) SECRET: secret key  Check the official documentation or my code on GitHub for more configuration options.
You might have noticed that I reference a cloudflare_record in the REMARK_URL part. That&rsquo;s because I also manage my DNS records with Terraform. The DNS record for remark42.schnerring.net pointing to the DNS record of my cluster looks like this:
resource &#34;cloudflare_record&#34; &#34;remark42&#34; { zone_id = cloudflare_zone.schnerring_net.id name = &#34;remark42&#34; type = &#34;CNAME&#34; value = cloudflare_record.traefik.hostname proxied = true } Create the Deployment #  Next, we add the Deployment to the remark42.tf file, using the remark-deployment.yaml file as a model. We map the previously defined configuration parameters to environment variables and mount the PVC to /srv/var.
resource &#34;kubernetes_deployment&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-deploy&#34; namespace = kubernetes_namespace.remark42.metadata.0.name labels = { &#34;app&#34; = &#34;remark42&#34; } } spec { replicas = 1 selector { match_labels = { &#34;app&#34; = &#34;remark42&#34; } } strategy { type = &#34;Recreate&#34; } template { metadata { labels = { &#34;app&#34; = &#34;remark42&#34; } } spec { hostname = &#34;remark42&#34; restart_policy = &#34;Always&#34; container { name = &#34;remark42&#34; image = &#34;umputun/remark42:\${var.remark42_image_version}&#34; port { container_port = 8080 } env { name = &#34;REMARK_URL&#34; value_from { config_map_key_ref { key = &#34;REMARK_URL&#34; name = &#34;remark42-cm&#34; } } } env { name = &#34;SECRET&#34; value_from { secret_key_ref { key = &#34;SECRET&#34; name = &#34;remark42-secret&#34; } } } env { name = &#34;SITE&#34; value_from { config_map_key_ref { key = &#34;SITE&#34; name = &#34;remark42-cm&#34; } } } volume_mount { mount_path = &#34;/srv/var&#34; name = &#34;remark42-vol&#34; } } volume { name = &#34;remark42-vol&#34; persistent_volume_claim { claim_name = &#34;remark42-pvc&#34; } } } } } } Create the Service #  To expose our deployment as a network service, we create a Service resource by adding the following to the remark42.tf file:
resource &#34;kubernetes_service&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-svc&#34; namespace = kubernetes_namespace.remark42.metadata.0.name } spec { selector = { &#34;app&#34; = &#34;remark42&#34; } port { name = &#34;http&#34; port = 80 target_port = 8080 } } } Create the Ingress #  I use Traefik 2 as Ingress Controller in combination with the Traefik Kubernetes Ingress provider. To manage Let&rsquo;s Encrypt certificates, I use cert-manager. So depending on your cluster configuration, the following steps might differ.
Let us add an Ingress to the remark42.tf file, to expose our service to the world:
resource &#34;kubernetes_ingress&#34; &#34;remark42&#34; { metadata { name = &#34;remark42-ing&#34; namespace = kubernetes_namespace.remark42.metadata.0.name annotations = { &#34;cert-manager.io/cluster-issuer&#34; = &#34;letsencrypt-production&#34; &#34;traefik.ingress.kubernetes.io/router.tls&#34; = &#34;true&#34; } } spec { rule { host = cloudflare_record.remark42.hostname http { path { path = &#34;/&#34; backend { service_name = &#34;remark42-svc&#34; service_port = 80 } } } } tls { hosts = [cloudflare_record.remark42.hostname] secret_name = &#34;remark42-tls-secret&#34; } } } Now run terraform apply to deploy everything. Note that we are using letsencrypt-staging as cluster issuer. We will have to change this to letsencrypt-production once we finished testing.
Browse to the demo site at https://remark42.schnerring.net/web and check whether Remark42 works. If you want to post a comment on the demo site, make sure to add the remark site ID to the SITE environment variable, separated by a , (i.e., schnerring.net,remark).
Integrate Remark42 with Hugo #  To add the Remark42 comment widget to our Hugo site, we have to integrate it with our theme. At the time of this writing, I use the Hello Friend theme, which includes a partial template for the comment section. We add the Remark42 widget to the layouts/partials/comments.html file:
&lt;div id=&#34;remark42&#34;&gt;&lt;/div&gt; &lt;script&gt; var remark_config = { host: &#34;https://remark42.schnerring.net&#34;, site_id: &#34;schnerring.net&#34;, theme: getHelloFriendTheme(), show_email_subscription: false, }; &lt;/script&gt; &lt;script&gt; !(function (e, n) { for (var o = 0; o &lt; e.length; o++) { var r = n.createElement(&#34;script&#34;), c = &#34;.js&#34;, d = n.head || n.body; &#34;noModule&#34; in r ? ((r.type = &#34;module&#34;), (c = &#34;.mjs&#34;)) : (r.async = !0), (r.defer = !0), (r.src = remark_config.host + &#34;/web/&#34; + e[o] + c), d.appendChild(r); } })(remark_config.components || [&#34;embed&#34;], document); &lt;/script&gt; You can find more configuration options for the widget in the Remark42 GitHub README.
Next, we implement the getHelloFriendTheme() function, so Remark42 loads the correct theme. The Hello Friend theme stores the current theme in the local storage of the browser. Knowing that, implementing the function is pretty straight forward:
&lt;script&gt; const defaultTheme = &#34;light&#34;; // same as defaultTheme in config.toml  function getHelloFriendTheme() { const theme = localStorage &amp;&amp; localStorage.getItem(&#34;theme&#34;); if (!theme) { return defaultTheme; } else { return theme; } } &lt;/script&gt; The last thing we have to take care of is to also toggle the Remark42 theme, when clicking the theme toggle button of the Hello Friend theme. To do so, we register an additional click event handler to the .theme-toggle button and call the window.REMARK42.changeTheme() function:
&lt;script&gt; const themeToggle = document.querySelector(&#34;.theme-toggle&#34;); themeToggle.addEventListener(&#34;click&#34;, () =&gt; { setTimeout(() =&gt; window.REMARK42.changeTheme(getHelloFriendTheme()), 10); }); &lt;/script&gt; Note that we wait 10 ms before reading from local storage to avoid race conditions. All we have to do now is to enable comments by setting comments: true via Hugo Front Matter.
What Do You Think? #  You can find all the code on my GitHub. I also tagged the commits to make it easier to find the code for future reference:
 schnerring/infrastructure-core (Terraform, tag v0.2.0) schnerring/schnerring.github.io (Hugo, tag v1.1.0)  Any feedback in the comments below is appreciated.
`}).add({id:17,href:"/blog/use-terraform-to-deploy-an-azure-kubernetes-service-aks-cluster-traefik-2-cert-manager-and-lets-encrypt-certificates/",title:"Use Terraform to Deploy an Azure Kubernetes Service (AKS) Cluster, Traefik 2, cert-manager, and Let's Encrypt Certificates",description:`In this post, we will deploy a simple Azure Kubernetes Service (AKS) cluster from scratch. To expose our web services securely, we will install Traefik 2 and configure cert-manager to manage Let&rsquo;s Encrypt certificates. The best part about it: we will do everything with Terraform.
`,content:`In this post, we will deploy a simple Azure Kubernetes Service (AKS) cluster from scratch. To expose our web services securely, we will install Traefik 2 and configure cert-manager to manage Let&rsquo;s Encrypt certificates. The best part about it: we will do everything with Terraform.
Prerequisites #  Keep in mind that I have tested the steps that follow on Windows. So if you are using another OS, you might have to modify them slightly. I also omit some non-essential steps in between, so it helps if you are already familiar with Azure, Kubernetes, and Terraform.
To follow along, you will need the following things:
 An active Azure subscription Install Terraform Install kubectl A registered domain A DNS provider:  that supports cert-manager DNS01 challenge validation with API access, so we can manage DNS records with Terraform \u2014 I use Cloudflare    Overview #  Here is an outline of the steps required to build our solution:
 Setup Terraform Create the AKS cluster Deploy cert-manager Configure Let&rsquo;s Encrypt certificates Deploy Traefik Deploy a demo application  Step 1: Setup Terraform #  We will make use of Terraform providers to put everything together:
 azurerm to manage our AKS cluster helm to deploy cert-manager and Traefik kubernetes to manage namespaces and deploy our demo app cloudflare to manage DNS records  We add a provider.tf file with the following content:
terraform { required_version = &#34;= 0.14.9&#34; required_providers { azurerm = { source = &#34;azurerm&#34; version = &#34;=2.97.0&#34; } cloudflare = { source = &#34;cloudflare/cloudflare&#34; version = &#34;=3.9.1&#34; } helm = { source = &#34;helm&#34; version = &#34;=2.4.1&#34; } kubernetes = { source = &#34;kubernetes&#34; version = &#34;=2.8.0&#34; } } } provider &#34;azurerm&#34; { features {} } provider &#34;cloudflare&#34; {} For now, we only configured the azurerm and cloudflare providers. After setting up the AKS cluster, we will configure the helm and kubernetes providers.
I have opted to configure the azurerm provider with environment variables. You might want to choose a different approach depending on your needs.
To authenticate the cloudflare provider, I use a Cloudflare API Token with Edit Zone permissions.
After, we make sure to run terraform init to get started.
Step 2: Create the AKS cluster #  Creating a production-ready AKS cluster is out of scope for this post, which means that we will not delve too deep into AKS configuration. There are many things we are skipping over, like backups and monitoring. For a little more elaborate example, check out the official Terraform on Azure documentation.
We create a new file k8s.tf with the following content:
resource &#34;azurerm_resource_group&#34; &#34;k8s&#34; { name = &#34;k8s-rg&#34; location = var.location } resource &#34;random_id&#34; &#34;random&#34; { byte_length = 2 } resource &#34;azurerm_kubernetes_cluster&#34; &#34;k8s&#34; { name = &#34;k8s-aks&#34; resource_group_name = azurerm_resource_group.k8s.name location = var.location tags = var.tags dns_prefix = &#34;k8saks\${random_id.random.dec}&#34; default_node_pool { name = &#34;default&#34; node_count = 1 vm_size = &#34;Standard_B2ms&#34; } identity { type = &#34;SystemAssigned&#34; } } Note that I have defined the var.location and var.tags variables in a separate variables.tf file.
To be able to access the AKS cluster locally with kubectl, we define a Terraform output in the outputs.tf file:
output &#34;kube_config&#34; { value = azurerm_kubernetes_cluster.k8s.kube_config_raw description = &#34;kubeconfig for kubectl access.&#34; sensitive = true } We have to set sensitive = true so our credentials will not get leaked, which could happen if we later decide to run Terraform with GitHub Actions. We apply our configuration by running Terraform:
terraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan After the deployment completes, we set up kubectl to be able to access our cluster:
terraform output -raw kube_config &gt; ~/.kube/config We check the deployment by running kubectl get all:
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 3m54s Step 3: Deploy cert-manager #  To issue free Let&rsquo;s Encrypt certificates for the web services we provide, the first thing we have to deploy is cert-manager. We need to configure the helm provider first. While we are on it, we also configure the kubernetes provider:
provider &#34;helm&#34; { kubernetes { host = azurerm_kubernetes_cluster.k8s.kube_config.0.host client_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.cluster_ca_certificate) } } provider &#34;kubernetes&#34; { host = azurerm_kubernetes_cluster.k8s.kube_config.0.host client_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.cluster_ca_certificate) } Stacking the providers above with our managed Kubernetes cluster resources can lead to errors and should be avoided. I will mention this once more at the end of the post.
Next, we deploy cert-manager with Helm by adding the following Terraform code to the k8s.tf file:
resource &#34;kubernetes_namespace&#34; &#34;cert_manager&#34; { metadata { name = &#34;cert-manager&#34; } } resource &#34;helm_release&#34; &#34;cert_manager&#34; { name = &#34;cert-manager&#34; repository = &#34;https://charts.jetstack.io&#34; chart = &#34;cert-manager&#34; version = &#34;v1.7.1&#34; namespace = kubernetes_namespace.cert_manager.metadata.0.name set { name = &#34;installCRDs&#34; value = &#34;true&#34; } } We then run terraform apply to deploy cert-manager. We then check our work by running kubectl get all --namespace cert-manager, which should display something like this:
NAME READY STATUS RESTARTS AGE pod/cert-manager-7998c69865-vrvrd 1/1 Running 0 75s pod/cert-manager-cainjector-7b744d56fb-qb54k 1/1 Running 0 75s pod/cert-manager-webhook-7d6d4c78bc-svzq5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.0.141.59 &lt;none&gt; 9402/TCP 75s service/cert-manager-webhook ClusterIP 10.0.18.192 &lt;none&gt; 443/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 75s deployment.apps/cert-manager-cainjector 1/1 1 1 75s deployment.apps/cert-manager-webhook 1/1 1 1 75s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-7998c69865 1 1 1 75s replicaset.apps/cert-manager-cainjector-7b744d56fb 1 1 1 75s replicaset.apps/cert-manager-webhook-7d6d4c78bc 1 1 1 75s Step 4: Configure Let&rsquo;s Encrypt Certificates #  In Kubernetes, Issuers are Kubernetes resources representing certificate authorities able to generate certificates. We have to create a single ClusterIssuer, a cluster-wide Issuer, using DNS01 challenge validation with Let&rsquo;s Encrypt servers. As mentioned earlier, we will use Cloudflare, but many other DNS providers are supported.
First, we need to create a Cloudflare API Token on the Cloudflare website, at User Profile \u2192 API Tokens. The following permissions are required:
 Zone - DNS - Edit Zone - Zone - Read  To securely pass the token to Terraform, we create a sensitive variable. We also add a variable containing the email address where Let&rsquo;s Encrypt can notify us about expiring certificates:
variable &#34;letsencrypt_email&#34; { type = string description = &#34;Email address that Let&#39;s Encrypt will use to send notifications about expiring certificates and account-related issues to.&#34; sensitive = true } variable &#34;letsencrypt_cloudflare_api_token&#34; { type = string description = &#34;Cloudflare API token with Zone-DNS-Edit and Zone-Zone-Read permissions, which is required for DNS01 challenge validation.&#34; sensitive = true } With Terraform, we then add the secret containing the API token to our cluster. Since ClusterIssuer is a cluster-scoped resource, we need to make sure the secret is globally available by putting it in the cert-manager namespace:
resource &#34;kubernetes_secret&#34; &#34;letsencrypt_cloudflare_api_token_secret&#34; { metadata { name = &#34;letsencrypt-cloudflare-api-token-secret&#34; namespace = kubernetes_namespace.cert_manager.metadata.0.name } data = { &#34;api-token&#34; = var.letsencrypt_cloudflare_api_token } } Next, we add the staging and production ClusterIssuer cert-manager CRD resources that use Let&rsquo;s Encrypt servers. We will have to use regular Kubernetes YAML manifests since we cannot deploy CRDs with the kubernetes provider. Here, the kubernetes_manifest resource comes in. Together with the Terraform yamldecode() and templatefile() functions, we get a pretty nice solution.
Let&rsquo;s start by defining the letsencrypt-issuer.tpl.yaml template file:
apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: \${name} spec: acme: email: \${email} server: \${server} privateKeySecretRef: name: issuer-account-key-\${name} solvers: - dns01: cloudflare: apiTokenSecretRef: name: \${api_token_secret_name} key: \${api_token_secret_data_key} We then create the staging and production ClusterIssuers like so:
resource &#34;kubernetes_manifest&#34; &#34;letsencrypt_issuer_staging&#34; { manifest = yamldecode(templatefile( &#34;\${path.module}/letsencrypt-issuer.tpl.yaml&#34;, { &#34;name&#34; = &#34;letsencrypt-staging&#34; &#34;email&#34; = var.letsencrypt_email &#34;server&#34; = &#34;https://acme-staging-v02.api.letsencrypt.org/directory&#34; &#34;api_token_secret_name&#34; = kubernetes_secret.letsencrypt_cloudflare_api_token_secret.metadata.0.name &#34;api_token_secret_data_key&#34; = keys(kubernetes_secret.letsencrypt_cloudflare_api_token_secret.data).0 } )) depends_on = [helm_release.cert_manager] } resource &#34;kubernetes_manifest&#34; &#34;letsencrypt_issuer_production&#34; { manifest = yamldecode(templatefile( &#34;\${path.module}/letsencrypt-issuer.tpl.yaml&#34;, { &#34;name&#34; = &#34;letsencrypt-production&#34; &#34;email&#34; = var.letsencrypt_email &#34;server&#34; = &#34;https://acme-v02.api.letsencrypt.org/directory&#34; &#34;api_token_secret_name&#34; = kubernetes_secret.letsencrypt_cloudflare_api_token_secret.metadata.0.name &#34;api_token_secret_data_key&#34; = keys(kubernetes_secret.letsencrypt_cloudflare_api_token_secret.data).0 } )) depends_on = [helm_release.cert_manager] } Now we terraform apply the changes.
Step 5: Deploy Traefik #  To manage external access to our Kubernetes cluster, we need to configure Kubernetes Ingress resources. To satisfy an Ingress, we first need to configure an Ingress Controller. We will use Traefik for this.
To manage ingress, we could also use the Traefik IngressRoute CRD. At the time of this writing, cert-manager cannot directly interface with Traefik CRDs, so we would have to manage Certificate and Secret resources manually, which is cumbersome.
We add the following to the k8s.tf file:
resource &#34;kubernetes_namespace&#34; &#34;traefik&#34; { metadata { name = &#34;traefik&#34; } } resource &#34;helm_release&#34; &#34;traefik&#34; { name = &#34;traefik&#34; repository = &#34;https://helm.traefik.io/traefik&#34; chart = &#34;traefik&#34; version = &#34;10.14.2&#34; namespace = kubernetes_namespace.traefik.metadata.0.name set { name = &#34;ports.web.redirectTo&#34; value = &#34;websecure&#34; }# Trust private AKS IP range  set { name = &#34;additionalArguments&#34; value = &#34;{--entryPoints.websecure.forwardedHeaders.trustedIPs=10.0.0.0/8}&#34; } } Setting ports.web.redirectTo to websecure forces all HTTP traffic to be redirected to HTTPS.
To configure Traefik to trust forwarded headers from Azure, we set entryPoints.websecure.forwardedHeaders.trustedIPs=10.0.0.0/8.
After running terraform apply, we check the deployment by running kubectl get all --namespace traefik:
NAME READY STATUS RESTARTS AGE pod/traefik-6b6767d778-hxzzw 1/1 Running 0 68s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.0.247.4 51.103.157.225 80:32468/TCP,443:31284/TCP 69s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 69s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-6b6767d778 1 1 1 69s Next, we add a DNS record with the IP of our Traefik service. To obtain the external IP address of the service, we leverage the kubernetes_service Data Source of the kubernetes provider. We then add the DNS record k8s.schnerring.net pointing to the external IP of Traefik.
Let us update the k8s.tf file accordingly and terraform apply the changes:
data &#34;kubernetes_service&#34; &#34;traefik&#34; { metadata { name = helm_release.traefik.name namespace = helm_release.traefik.namespace } } resource &#34;cloudflare_record&#34; &#34;traefik&#34; { zone_id = cloudflare_zone.schnerring_net.id name = &#34;k8s&#34; type = &#34;A&#34; value = data.kubernetes_service.traefik.status.0.load_balancer.0.ingress.0.ip proxied = true } How awesome is that?
Step 6: Deploy a Demo Application #  We are almost at the finish line. All that is missing is reaping the fruit of our hard labor. To create a simple demo, we will use the nginxdemos/hello image and make it available at https://hello.k8s.schnerring.net/.
To make it happen, we add a kubernetes_namespace, kubernetes_deployment, kubernetes_service, and kubernetes_ingress resource to a new hello.tf file:
resource &#34;kubernetes_namespace&#34; &#34;hello&#34; { metadata { name = &#34;hello&#34; } } resource &#34;kubernetes_deployment&#34; &#34;hello&#34; { metadata { name = &#34;hello-deploy&#34; namespace = kubernetes_namespace.hello.metadata.0.name labels = { app = &#34;hello&#34; } } spec { replicas = 2 selector { match_labels = { app = &#34;hello&#34; } } template { metadata { labels = { app = &#34;hello&#34; } } spec { container { image = &#34;nginxdemos/hello&#34; name = &#34;hello&#34; port { container_port = 80 } } } } } } resource &#34;kubernetes_service&#34; &#34;hello&#34; { metadata { name = &#34;hello-svc&#34; namespace = kubernetes_namespace.hello.metadata.0.name } spec { selector = { app = kubernetes_deployment.hello.metadata.0.labels.app } port { port = 80 target_port = 80 } } } resource &#34;kubernetes_ingress&#34; &#34;hello&#34; { metadata { name = &#34;hello-ing&#34; namespace = kubernetes_namespace.hello.metadata.0.name annotations = { &#34;cert-manager.io/cluster-issuer&#34; = &#34;letsencrypt-staging&#34; &#34;traefik.ingress.kubernetes.io/router.tls&#34; = &#34;true&#34; } } spec { rule { host = &#34;hello.k8s.schnerring.net&#34; http { path { path = &#34;/&#34; backend { service_name = &#34;hello-svc&#34; service_port = 80 } } } } tls { hosts = [&#34;hello.k8s.schnerring.net&#34;] secret_name = &#34;hello-tls-secret&#34; } } } After running terraform apply again, we should be able to visit the demo site https://hello.k8s.schnerring.net/:
   To verify the HTTPS redirect works, we run curl -svDL http://hello.k8s.schnerring.net (PowerShell), or curl -sLD - http://hello.k8s.schnerring.net (Bash):
... &lt; HTTP/1.1 301 Moved Permanently &lt; Location: https://hello.k8s.schnerring.net/ ... To get rid of the certificate warning, set &quot;cert-manager.io/cluster-issuer&quot; = &quot;letsencrypt-production&quot;. But be aware of rate limits that apply to the Let&rsquo;s Encrypt production API!
One Last Thing #  If we want to tear down the cluster and rebuild, we cannot achieve this in one terraform apply operation. The reason is that the kubernetes provider requires an operational cluster during the terraform plan phase. On top of that, any CRDs we deploy have to be available during terraform plan, too.
So when rebuilding, we would first create the AKS cluster and deploy cert-manager and then apply the rest:
terraform destroy -target &#34;azurerm_resource_group.k8s&#34; terraform plan -out infrastructure.tfplan -target &#34;helm_release.cert_manager&#34; terraform apply infrastructure.tfplan terraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan I already mentioned this earlier. The need for the workaround above originates from stacking Kubernetes cluster infrastructure with Kubernetes resources which the official Kubernetes provider documentation discourages recommend against that. Adhering to the docs and separating cluster and Kubernetes resources into different modules will probably save you a headache!
Other than that, we created a pretty cool solution, fully managed by Terraform, did we not?
You can find all the code on GitHub in my schnerring/infrastructure-core repository, which is evolving continuously. After committing the code to the repo, I added the v0.4.0 tag. This way, in the future, we can easily find the code depicted in this post.
`}).add({id:18,href:"/blog/create-a-hugo-website-with-github-pages-github-actions-and-cloudflare/",title:"Create a Hugo Website with GitHub Pages, GitHub Actions, and Cloudflare",description:`In this beginner guide, you&rsquo;ll create a Hugo website from scratch and publish the website on GitHub Pages. You&rsquo;ll configure Cloudflare&rsquo;s DNS and utilize its caching capabilities to optimize page speeds. Finally, implementing automated deployments with GitHub Pages will enable you to publish new content on your site easily.
`,content:`In this beginner guide, you&rsquo;ll create a Hugo website from scratch and publish the website on GitHub Pages. You&rsquo;ll configure Cloudflare&rsquo;s DNS and utilize its caching capabilities to optimize page speeds. Finally, implementing automated deployments with GitHub Pages will enable you to publish new content on your site easily.
Motivation #  Over the last couple of years, I&rsquo;ve written documentation for private hobby projects, most of it in Markdown and managed with Git. It&rsquo;s all over the place, some parts quite elaborate, other stuff just bullet point lists.
I think some of it might be useful for others, so I started looking for ways to publish Markdown documentation and found Hugo. It&rsquo;s a simple to use, modern, and very popular static site generator that encourages the use of Markdown files. Perfect!
Having found Hugo, I started looking into how to best host static content. I&rsquo;ve been using GitHub forever, so obviously I chose GitHub Pages. Accompanied by Cloudflare&rsquo;s caching capabilities, a blazingly fast website is guaranteed.
The only thing missing was a way to automagically publish changes made to the Git repository on the website. This is where GitHub Actions come in.
With the technology figured out, in my very first guide, I&rsquo;ll show you step by step how I created this website in its first version. You can check out the code on my GitHub where I tagged the resulting commit with version v1.0.0.
The Plan #  Here is an overview of what you&rsquo;ll do:
 Register for third-party services and install the required software Prepare the Git repository Set up the development environment Create a Hugo site from scratch and run it locally Set up Cloudflare for a custom root (apex) domain Manually deploy the website to GitHub Pages Deploy the website automatically to GitHub Pages with GitHub Actions  I created this guide with Windows users in mind, but the workflow should be easily adaptable to other platforms. The steps are detailed and beginner-friendly, so if you&rsquo;re more experienced, you can skip through most parts of steps 1 to 3.
For this guide, I configured the schnerring.net domain and used schnerring.github.io as the GitHub Pages site. In the instructions, you&rsquo;ll need to replace these accordingly.
Step #1: Prerequisites #  First, you&rsquo;ll need to register for a couple of services and install some software. Everything mentioned is free, except registering a custom domain name. I&rsquo;m not affiliated with any of the products I recommend in this guide.
Sign Up at Third-Party Services #   Register a domain name with your registrar \u2014 I&rsquo;m a very happy Namecheap customer Sign up at Cloudflare Sign up at GitHub  Install Software for Development #  Install the following programs for local development on our workstation:
 Hugo \u2014 make sure to install hugo-extended to be able to use themes that utilize Sass/SCSS Git for Windows \u2014 source control management and shell Visual Studio Code \u2014 editor for coding Google Chrome \u2014 web browser for debugging  I used the Git Bash terminal to create the instructions for this guide. It&rsquo;s bundled with Git for Windows and provides UNIX-like commands like rm, touch, and more.
Step #2: Prepare Git #  Create a GitHub Repository #  Sign in to your GitHub account and create a new repository. To create a user type website with GitHub Pages, name the repository schnerring.github.io.
If you want to know more about GitHub Page types, you can find further information in the GitHub Docs.
Make sure to initialize the repository with a README and a LICENSE file. For this project, I chose the MIT license.
Clone the Newly Created Repository and Open It in VS Code #  If you haven&rsquo;t already, configure an SSH key to use with GitHub. You&rsquo;re now ready to clone the GitHub repository to your workstation. Open a Git Bash terminal and run the following commands:
git clone git@github.com:schnerring/schnerring.github.io.git cd schnerring.github.io code . Step #3: Prepare VS Code #  To make it easier to write clean and consistent code, install the following extensions:
EditorConfig
 EditorConfig helps maintain consistent coding styles for multiple developers working on the same project across various editors and IDEs.
 markdownlint
 markdownlint is a static analysis tool for Node.js with a library of rules to enforce standards and consistency for Markdown files
 shellcheck
 ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts
 Add the .vscode/extensions.json File #  Workspace recommended extensions for VS Code make it easy to share a set of extensions easily across development environments. When you open a repository folder in VS Code and some recommended extensions are missing, you&rsquo;ll be notified, and just one click away from installing them.
Here&rsquo;s what the file looks like:
{ &#34;recommendations&#34;: [ &#34;editorconfig.editorconfig&#34;, &#34;davidanson.vscode-markdownlint&#34;, &#34;omartawfik.github-actions-vscode&#34;, &#34;timonwong.shellcheck&#34; ] } Add the .vscode/launch.json File #  When running hugo server locally during development, it&rsquo;s nice to be able to open http://localhost:1313 in Chrome from within VS Code by just hitting the F5 key. For this, you need to add a launch configuration. VS Code stores launch configurations in the launch.json file located in the .vscode/ folder.
For Windows, the configuration looks like this:
{ // Use IntelliSense to learn about possible attributes.  // Hover to view descriptions of existing attributes.  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387  &#34;version&#34;: &#34;0.2.0&#34;, &#34;configurations&#34;: [ { &#34;type&#34;: &#34;pwa-chrome&#34;, &#34;request&#34;: &#34;launch&#34;, &#34;name&#34;: &#34;Launch Chrome against localhost&#34;, &#34;url&#34;: &#34;http://localhost:1313&#34;, &#34;webRoot&#34;: &#34;\${workspaceFolder}&#34; } ] } Check out the official documentation to learn more about debugging in VS Code.
Add the EditorConfig Configuration #  Add the .editorconfig file to the root folder of the repository:
# top-most EditorConfig file root = true # every file [*] charset = utf-8 indent_size = 2 indent_style = space insert_final_newline = true trim_trailing_whitespace = true # Markdown files [*.md] trim_trailing_whitespace = false Check out EditorConfig if you want to learn more, it&rsquo;s awesome!
Add the .gitignore File #  To prevent cluttering of the repository with files generated by VS Code, add a .gitignore file to the root of the repository. To generate the .gitignore file, I like to use gitignore.io. Hugo adds a debug.log file from time to time which I manually add, as well.
debug.log # Created by https://www.toptal.com/developers/gitignore/api/hugo,vscode # Edit at https://www.toptal.com/developers/gitignore?templates=hugo,vscode ### Hugo ### # Generated files by hugo /public/ /resources/_gen/ hugo_stats.json # Executable may be added to repository hugo.exe hugo.darwin hugo.linux ### vscode ### .vscode/* !.vscode/settings.json !.vscode/tasks.json !.vscode/launch.json !.vscode/extensions.json *.code-workspace # End of https://www.toptal.com/developers/gitignore/api/hugo,vscode Push Development Environment Settings to GitHub #  The folder structure now looks like this:
.vscode/ \u251C\u2500 extensions.json \u251C\u2500 launch.json .editorconfig .gitignore LICENSE README.md Push the changes to GitHub:
git add --all git commit --message=&#34;Add vscode launch.json / extensions.json, .editorconfig, .gitignore&#34; git push Step #4: Enter Hugo #  Only a few steps are required to create a functional Hugo site.
Create a New Site #  To create a Hugo site in your current working folder, use the Hugo CLI:
hugo new site . --force With --force, Hugo won&rsquo;t complain about the folder containing files.
Add a Theme #  To get started quickly, add the Hello Friend theme as a Git Submodule. Check out themes.gohugo.io for more themes.
git submodule add https://github.com/panr/hugo-theme-hello-friend.git themes/hello-friend Update the Hugo Configuration #  The Hello Friend theme requires minimal configuration steps to work. As a baseline, use the sample provided in the themes' README and make some straightforward changes to it.
The resulting config.toml looks like this:
baseURL = &#34;https://schnerring.net&#34; languageCode = &#34;en-us&#34; theme = &#34;hello-friend&#34; paginate = 5 ignoreFiles = [ &#34;LICENSE$&#34; ] [params] # dir name of your blog content (default is \`content/posts\`). # the list of set content will show up on your index page (baseurl). #contentTypeName = &#34;posts&#34; # &#34;light&#34; or &#34;dark&#34; defaultTheme = &#34;dark&#34; # if you set this to 0, only submenu trigger will be visible showMenuItems = 2 # Show reading time in minutes for posts showReadingTime = false # Show table of contents at the top of your posts (defaults to false) # Alternatively, add this param to post front matter for specific posts # toc = true # Show full page content in RSS feed items #(default is Description or Summary metadata in the front matter) # rssFullText = true [languages] [languages.en] title = &#34;Michael Schnerring&#34; subtitle = &#34;Coder and Computer Enthusiast&#34; keywords = &#34;&#34; description = &#34;Michael Schnerring is a coder and computer enthusiast.&#34; copyright = &#34;&lt;p&gt;Copyright \xA9 2020 Michael Schnerring&lt;/p&gt;&lt;p xmlns:dct=&#39;http://purl.org/dc/terms/&#39; xmlns:cc=&#39;http://creativecommons.org/ns#&#39; class=&#39;license-text&#39;&gt;&lt;a rel=&#39;cc:attributionURL&#39; property=&#39;dct:title&#39; href=&#39;https://github.com/schnerring/schnerring.github.io/tree/main/content&#39;&gt;Content&lt;/a&gt; licensed under &lt;a rel=&#39;license&#39; href=&#39;https://creativecommons.org/licenses/by/4.0&#39;&gt;CC BY 4.0&lt;/a&gt;&lt;/p&gt;&#34; menuMore = &#34;Show more&#34; writtenBy = &#34;Written by&#34; readMore = &#34;Read more&#34; readOtherPosts = &#34;Read other posts&#34; newerPosts = &#34;Newer posts&#34; olderPosts = &#34;Older posts&#34; minuteReadingTime = &#34;min read&#34; dateFormatSingle = &#34;2006-01-02&#34; dateFormatList = &#34;2006-01-02&#34; # leave empty to disable, enter display text to enable #lastModDisplay = &#34;modified&#34; [languages.en.params.logo] logoText = &#34;schnerring.net&#34; logoHomeLink = &#34;/&#34; # or # # path = &#34;/img/your-example-logo.svg&#34; # alt = &#34;Your example logo alt text&#34; [languages.en.menu] [[languages.en.menu.main]] identifier = &#34;about&#34; name = &#34;About&#34; url = &#34;/about&#34; The themes' GitHub repository features another config.toml example. You can read more about how to configure Hugo in the official documentation.
Add Some Content #  With the help of the Hugo CLI, add an about page and the first blog post:
hugo new about.md hugo new posts/hello-world.md about.md #  --- title: &#34;About&#34; draft: false --- I code by day and toy around with computers by night... posts/hello-world.md #  --- title: &#34;Hello World&#34; description: &#34;The first post of this blog&#34; date: 2021-03-14T15:00:21+01:00 draft: false tags: - &#34;csharp&#34; - &#34;hello world&#34; --- I&#39;m a .NET developer by trade, so let&#39;s say hello in C#! \`\`\`csharp using System; class Program { public static void Main(string[] args) { Console.WriteLine(&#34;Hello, world!&#34;); } } \`\`\` Note the post&rsquo;s default status draft: true, so newly created content has to be published manually. You can read more about it in the official documentation. Make sure to set this to false before deployment or the post won&rsquo;t be displayed.
Run the Site Locally #  To finally test the changes locally, run the following in a Git Bash terminal. The --buildDrafts (or -D) option enables you to also view content that has set draft: true.
hugo server --buildDrafts Hit the F5 button in VS Code to open the page in Chrome.
Push the Changes #  Git ignores empty folders, so if you push the current changes and re-clone the repository, the Hugo folder structure would be partially gone. A common practice is to add empty .gitkeep marker files to empty folders to prevent this:
touch {data,layouts,static}/.gitkeep Your folder structure should look like this, excluding resources or public folders, since those are .gitignored:
.vscode/ \u251C\u2500 extensions.json \u251C\u2500 launch.json archetypes/ \u251C\u2500 default.md content/ \u251C\u2500 posts/ \u2502 \u251C\u2500 hello-world.md \u251C\u2500 about.md data/ \u251C\u2500 .gitkeep layouts/ \u251C\u2500 .gitkeep static/ \u251C\u2500 .gitkeep themes/ \u251C\u2500 hello-friend/ (submodule) .editorconfig .gitignore .gitmodules config.toml LICENSE README.md Commit and push everything:
git add -A git commit -m &#34;Add Hugo site, hello-friend theme, about page and hello-world post&#34; git push Step #5: Configure Cloudflare #  Add the Site to Cloudflare #   Sign in to your Cloudflare account Click + Add site in the navigation bar and add schnerring.net Select Free plan Navigate to the Overview page of the newly created site Take note of Cloudflare&rsquo;s nameservers, in my case carol.ns.cloudflare.com and cody.ns.cloudflare.com  Change the Nameservers for Your Domain #  Sign in to the administrator account of your domain registrar and change the nameservers. My Namecheap nameserver configuration for schnerring.net looks like this after adding the Cloudflare nameservers:
   Add the CNAME Records #  A CNAME record is used to map one domain name to another. Go to the DNS management at Cloudflare and add the following records to point your domain to GitHub Pages:
   Make sure the \u201Corange cloud\u201D is enabled, so you can define rules and cache static content with Cloudflare.
Enable Full SSL/TLS Encryption Mode #  Go to your site&rsquo;s SSL/TLS settings and set encryption to Full:
   Configure the Browser Cache TTL #  Go to Cache \u2192 Configuration and choose 2 months or higher, depending on how often you think your already published static content changes.
   Add Page Rules #  The Free Tier lets you create up to three page rules. Go to your site&rsquo;s Page Rules settings and click Create Page Rule.
Enforce HTTPS #     Add Forward from www.schnerring.net Subdomain to schnerring.net Root Domain #  If you use a subdomain, skip this step.
   Cache All Static Content to Speed Up the Website #     Page Rules Overview #     Note that as of May 2018 GitHub Pages supports HTTPS for custom domains out of the box. They utilize Let&rsquo;s Encrypt certificates which are better than the shared certificates you get with Cloudflare. But to issue Let&rsquo;s Encrypt certificates for both www.schnerring.net and schnerring.net, you&rsquo;d have to resort this, in my opinion hacky, solution.
Step #6: Deploy the Site to GitHub Pages #  You&rsquo;ll deploy the website to GitHub Pages on a separate, parallel gh-pages branch since published artifacts and source code are segregated this way. You can learn more about other options in the official Hugo documentation.
Initialize gh-pages branch #  Create an orphan branch which has a git init like state with no history:
git checkout --orphan gh-pages git reset --hard git commit --allow-empty -m &#34;Init gh-pages branch&#34; git push origin gh-pages git checkout main Checkout the gh-pages Branch in public/ #  To build the site, use the hugo command. The build artifacts will be placed in the public/ folder, the contents of which need to be published on the gh-pages branch. To be able to checkout multiple branches, use Git&rsquo;s worktree feature:
git worktree add -B gh-pages public origin/gh-pages Add CNAME File to static/ Folder #  Earlier, you configured Cloudflare to properly map www.schnerring.net and schnerring.net to schnerring.github.io&rsquo;s destination. However, you also need a redirect from schnerring.github.io to schnerring.net. You accomplish this by adding a static/CNAME file to the repo, containing your custom domain. When generating the site with hugo, the published site will contain the CNAME file at its root:
echo &#34;schnerring.net&#34; &gt; static/CNAME rm static/.gitkeep git add -A git commit -m &#34;Add CNAME file&#34; git push Build the Hugo Site #  Make sure to set draft: false to publish the hello-world.md post. Push the change to GitHub by running git commit -am &quot;Publish hello-world.md&quot; &amp;&amp; git push.
Then run hugo.
Push the Changes on the gh-pages Branch #  cd public git add -A git commit -m &#34;Publish to gh-pages&#34; git push cd .. Set gh-pages as Publishing Branch #  Navigate to your repository on GitHub
 Go to Settings \u2192 Pages Under Source, select Branch: gh-pages and click Save  It should look like this:
   Shortly after you perform these steps, your website should be available at schnerring.net.
Subsequent Deployments #  For future deployments, several steps are required:
 Delete contents of public/ folder, since hugo does not remove generated files before building Generate the site with hugo Push changes to the gh-pages branch Purge Cloudflare cache  This is an error-prone and tedious process if repeated frequently, so let&rsquo;s automate these steps with GitHub Pages next.
Step #7: Automate Deployments with GitHub Actions #  Configure GitHub Actions #  GitHub Actions Workflows are configured mostly through configuration files in the .github/workflows folder. This allows your configuration to be version controlled and flexible.
Add the following .github/workflows/hugo.yml file to the repository:
name: Hugo on: push: branches: - main # Allows to run workflow manually from Actions tab workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository and update Hugo themes uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Install Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: &#34;0.81.0&#34; extended: true - name: Build Hugo run: hugo --minify - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v3 with: github_token: \${{ secrets.GITHUB_TOKEN }} publish_dir: ./public - name: Purge Cloudflare Cache env: CLOUDFLARE_ZONE_ID: \${{ secrets.CLOUDFLARE_ZONE_ID }} CLOUDFLARE_API_TOKEN: \${{ secrets.CLOUDFLARE_API_TOKEN }} GITHUB_TOKEN: \${{ secrets.GITHUB_TOKEN }} run: |chmod +x ./purge_cloudflare_cache.sh ./purge_cloudflare_cache.sh Name the GitHub Actions Workflow Hugo which is triggered when pushed to the main branch. As OS, the latest version of ubuntu is used.
We only run one job, build, and perform the following steps:
 Checkout the repository and update Hugo themes Install Hugo extended since the theme uses SCSS. Build Hugo and --minify Deploy to GitHub Pages. The default deployment branch is gh-pages. Choose the ./public folder to be published. Purge the Cloudflare Cache by running the ./purge_cloudflare_cache.sh script that you&rsquo;ll create in the next steps.  The above is straight forward, thanks to the free Actions available at the GitHub Marketplace. Thanks to peaceiris for the awesome work:
 GitHub Actions for Hugo GitHub Actions for GitHub Pages  Note the mapping of the environment variables required in step 4 and 5. You&rsquo;ll later configure the GitHub secrets that will be injected as environment variables into the context of the respective steps. The GITHUB_TOKEN doesn&rsquo;t need to be configured for GitHub Actions, since GitHub automatically creates it to use in your workflow. For local debugging though, you&rsquo;ll have to create a token, anyway.
Create a GitHub Personal Access Token #   Personal access tokens (PAT) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line.
 Go to your GitHub profile&rsquo;s Settings \u2192 Developer settings \u2192 Personal access tokens. Click Generate new token, select only the public_repo scope and enter the note schnerring.github.io, to be able to later recognize what it&rsquo;s used for.
You only get one chance to copy the token&rsquo;s value, so add a new environment variable named GITHUB_TOKEN to your local system:
   If you lose the token, just delete and then recreate it.
Create the purge_cloudflare_cache.sh Script #  Purge the Cloudflare Cache #  Use Cloudflare&rsquo;s API to purge all cached files with curl after successful deployment:
curl \\  --silent \\  --request POST \\  --header &#34;Authorization: Bearer \${CLOUDFLARE_API_TOKEN}&#34; \\  --header &#34;Content-Type: application/json&#34; \\  --data &#39;{&#34;purge_everything&#34;:true}&#39; \\  &#34;https://api.cloudflare.com/client/v4/zones/\${CLOUDFLARE_ZONE_ID}/purge_cache&#34; Before you can run the command above, set CLOUDFLARE_ZONE_ID and CLOUDFLARE_API_TOKEN as environment variables in your GitHub repository and your workstation. You can add secrets in your GitHub repository&rsquo;s Settings \u2192 Secrets \u2192 New repository secret:
  Set CLOUDFLARE_ZONE_ID to your Cloudflare site&rsquo;s Zone ID which you can find it on your site&rsquo;s Overview page.
  Set CLOUDFLARE_API_TOKEN to an API Token with Zone.Cache Purge permissions. Create one at My Profile \u2192 API Tokens \u2192 Create Token \u2192 Create Custom Token \u2192 Get started:
     You should then be able to locally run the curl snippet above which should output &quot;success&quot;: true:
{ &#34;result&#34;: { &#34;id&#34;: &#34;********************************&#34; }, &#34;success&#34;: true, &#34;errors&#34;: [], &#34;messages&#34;: [] } Polling the GitHub Pages Build Status #  Before purging the site&rsquo;s cache on Cloudflare, make sure that the GitHub Pages build completed successfully. Via GitHub&rsquo;s API, query the latest GitHub Pages build with curl:
curl \\  --silent \\  --user &#34;schnerring:\${GITHUB_TOKEN}&#34; \\  --header &#34;Accept: application/vnd.github.v3+json&#34; \\  &#34;https://api.github.com/repos/schnerring/schnerring.github.io/pages/builds/latest&#34; The GITHUB_TOKEN you configured earlier is used here. Executing the snippet locally will output &quot;status&quot;: &quot;built&quot; if the GitHub Pages build succeeded:
{ &#34;url&#34;: &#34;https://api.github.com/repos/schnerring/schnerring.github.io/pages/builds/123456789&#34;, &#34;status&#34;: &#34;built&#34;, &#34;error&#34;: { &#34;message&#34;: null }, ... } Put It All Together #  #!/usr/bin/env bash  readonly DELAY_STEP_SECONDS=15 readonly INTERVAL_SECONDS=5 readonly TIMEOUT_SECONDS=120 readonly GITHUB_USER=schnerring readonly GITHUB_REPO=schnerring.github.io ################################################## # Poll status of latest GitHub Pages build every INTERVAL_SECONDS seconds for up # to TIMEOUT_SECONDS seconds. # Globals: # GITHUB_REPO # GITHUB_TOKEN # GITHUB_USER # INTERVAL_SECONDS # TIMEOUT_SECONDS # Arguments: # None # Outputs: # Success message to stdout or error message to stderr. # Returns: # 0 on success, 1 otherwise. ################################################## function poll_build_status() { echo &#34;Awaiting completion of latest GitHub Pages build ...&#34; local waited_seconds=0 while [[ &#34;\${waited_seconds}&#34; -lt &#34;\${TIMEOUT_SECONDS}&#34; ]]; do if curl \\  --silent \\  --user &#34;\${GITHUB_USER}:\${GITHUB_TOKEN}&#34; \\  --header &#34;Accept: application/vnd.github.v3+json&#34; \\  &#34;https://api.github.com/repos/\${GITHUB_USER}/\${GITHUB_REPO}/pages/builds/latest&#34; \\  | grep -q &#39;&#34;status&#34;: &#34;built&#34;&#39;; then echo &#34;Success.&#34; return 0 fi echo &#34; Sleeping \${INTERVAL_SECONDS}seconds until next status poll ...&#34; sleep &#34;\${INTERVAL_SECONDS}&#34; (( waited_seconds += INTERVAL_SECONDS )) done echo &#34;Failure.&#34; &gt;&amp;2 return 1 } ################################################## # Purge entire Cloudflare cache. # Globals: # CLOUDFLARE_API_TOKEN # CLOUDFLARE_ZONE_ID # Arguments: # None # Outputs: # Success message to stdout or error message to stderr. # Returns: # 0 on success, 1 otherwise. ################################################## function purge_cache() { echo &#34;Purging Cloudflare cache ...&#34; if curl \\  --silent \\  --request POST \\  --header &#34;Authorization: Bearer \${CLOUDFLARE_API_TOKEN}&#34; \\  --header &#34;Content-Type: application/json&#34; \\  --data &#39;{&#34;purge_everything&#34;:true}&#39; \\  &#34;https://api.cloudflare.com/client/v4/zones/\${CLOUDFLARE_ZONE_ID}/purge_cache&#34; \\  | grep -q &#39;&#34;success&#34;: true&#39;; then echo &#34;Success.&#34; return 0 else echo &#34;Failure.&#34; &gt;&amp;2 return 1 fi } ################################################## # Main function of script. # Globals: # DELAY_STEP_SECONDS # Arguments: # None ################################################## function main() { echo &#34;Sleeping \${DELAY_STEP_SECONDS}seconds ...&#34; sleep &#34;\${DELAY_STEP_SECONDS}&#34; poll_build_status || exit 1 echo &#34;Sleeping \${DELAY_STEP_SECONDS}seconds ...&#34; sleep &#34;\${DELAY_STEP_SECONDS}&#34; purge_cache || exit 1 } # Entrypoint main &#34;$@&#34; poll_build_status and purge_cache contain the functionality. The main function serves as the entry point executing those functions.
poll_build_status implements a while loop to repeatedly poll GitHub&rsquo;s API. It succeeds if the response contains &quot;status&quot;: &quot;built&quot; or times out after two minutes and fails.
Before each step, the script heuristically sleeps 15 seconds in case of latency issues with GitHub&rsquo;s API or GitHub Pages updates.
Running the ./purge_cloudflare_cache.sh script locally should output:
Awaiting completion of latest GitHub Pages build ... Success. Purging Cloudflare cache ... Success. Note that for sites with lots of files, purging the whole cache should be avoided. Cloudflare supports purging the cache for individual files.
We could list only changed files if we wanted to with a command like git diff-tree -r --no-commit-id --name-only --diff-filter=DM gh-pages and only purge the cache for these files, but that&rsquo;s out of scope for this post.
Now push .github/workflows/hugo.yml and purge_cloudflare_cache.sh to the repository:
git add -A git commit -m &#34;Add .github/workflows/hugo.yml, purge_cloudflare_cache.sh&#34; git push Go check out the Actions tab on your GitHub repository. If everything is configured correctly, GitHub Actions should be building your site:
   After the workflow succeeded, there won&rsquo;t be any changes to the site because we didn&rsquo;t change anything about our site, yet.
To make a change, switch to the light theme by setting defaultTheme = &quot;light&quot; inside the config.toml file. Push the changes by running git commit -am &quot;Set default theme to light&quot; &amp;&amp; git push and another GitHub Action workflow should automatically be triggered.
After the build completes, your website should be displayed in the light theme. If it isn&rsquo;t, make sure to also purge your browser cache.
Wrapping Up #  It took quite a bit of effort, but you now have a hands-off system in place that helps you to publish content to your website in an automated way. All you have to do is pushing new changes to your GitHub repo and after a minute the changes will be live.
`}).add({id:19,href:"/blog/hello-world/",title:"Hello World",description:"The first post of this blog",content:`I&rsquo;m a .NET developer by trade, so let&rsquo;s say hello in C#!
using System; class Program { public static void Main(string[] args) { Console.WriteLine(&#34;Hello, world!&#34;); } } `}),$.addEventListener("input",function(){let r=5,o=this.value,n=e.search(o,r,{enrich:!0}),i=new Map;for(let s of n.flatMap(l=>l.result))i.has(s.href)||i.set(s.doc.href,s.doc);if(z.innerHTML="",z.classList.remove("search__suggestions--hidden"),i.size===0&&o){let s=document.createElement("div");s.innerHTML=`No results for "<strong>${o}</strong>"`,s.classList.add("search__no-results"),z.appendChild(s);return}for(let[s,l]of i){let h=document.createElement("a");h.href=s,h.classList.add("search__suggestion-item"),z.appendChild(h);let p=document.createElement("div");p.textContent=l.title,p.classList.add("search__suggestion-title"),h.appendChild(p);let g=document.createElement("div");if(g.textContent=l.description,g.classList.add("search__suggestion-description"),h.appendChild(g),z.childElementCount===r)break}})})();})();
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
