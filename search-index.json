[{"content":"When using GitHub Actions, we often include steps that use Git to check out the code of the repository, make some changes to the code, and then commit these changes back to the repo. Additional steps subsequently act upon these changes within the same workflow run. For example, appending auto-generated release notes to a CHANGELOG.md file and committing it to the repo, to later bundle the changelog with release artifacts. Another example is using a workflow that periodically uses a linter to clean up the code base. Simple enough, right?\nTL;DR# If you\u0026rsquo;re just interested in the best‚Ñ¢ solution, I recommend using the following snippet:\njobs: update-changelog: runs-on: ubuntu-latest outputs: commit_hash: ${{ steps.commit-and-push.outputs.commit_hash }} steps: - name: Check Out the Repo uses: actions/checkout@v3 - name: Update CHANGELOG.md run: echo \u0026#34;Added changes on $(date)\u0026#34; \u0026gt;\u0026gt; CHANGELOG.md - name: Commit and Push Changes id: commit-and-push uses: stefanzweifel/git-auto-commit-action@v4 publish: needs: update-changelog runs-on: ubuntu-latest steps: - name: Check Out the Repo Again uses: actions/checkout@v3 with: ref: ${{ needs.update-changelog.outputs.commit_hash }} - name: Display CHANGELOG.md run: | cat CHANGELOG.md # The changes are here üéâüéâüéâ If you wanna know how it works and how I got there, continue reading!\nThe Issue# When I recently tried to implement this, I came up with the following (example) solution:\n# BAD CODE: DON\u0026#39;T USE! jobs: update-changelog: runs-on: ubuntu-latest steps: - name: Check Out the Repo uses: actions/checkout@v3 - name: Update CHANGELOG.md run: echo \u0026#34;Added changes on $(date)\u0026#34; \u0026gt;\u0026gt; CHANGELOG.md - name: Commit and Push uses: stefanzweifel/git-auto-commit-action@v4 publish: needs: update-changelog runs-on: ubuntu-latest steps: - name: Check Out the Repo Again uses: actions/checkout@v3 - name: Display CHANGELOG.md run: | cat CHANGELOG.md # The changes are missing üòï As you can see, there are two jobs: update-changelog and publish. By default, jobs always run in parallel. To ensure publishing only happens after updating the changelog, we set publish.needs to update-changelog.\nThe update-changelog job checks out the code using the actions/checkout action, appends a message containing the current date to the CHANGELOG.md file, and then uses the stefanzweifel/git-auto-commit-action action to commit and push the code to the repository.\nIn the publish job that follows, I check out the code again to inspect the contents of our changelog file, only to find out that the changes are missing.\nIt\u0026rsquo;s not the first time that I have fallen into this trap, and since you are reading this, likely you have too. We are not alone! I came across a popular GitHub issue from the actions/checkout action repository where people share the same pain.\nWhy Does This Happen?# To understand what\u0026rsquo;s going on, I did a little digging in the actions/checkout code base. In the following, I will only discuss workflows triggered by push events to branches. For events with other origins, such as pull requests or pushing tags, please consult the documentation and make appropriate adjustments.\nFirst, let\u0026rsquo;s have a look at the docs of the ref parameter of the action:\nThe branch, tag or SHA to checkout. When checking out the repository that triggered a workflow, this defaults to the reference or SHA for that event.\nSo if the ref parameter is left unspecified, the action will use the github.ref and github.sha values from the github context. You can find the code for this logic in input-helper.ts.\ngithub.sha github.ref The commit SHA that triggered the workflow. [\u0026hellip;] The fully-formed ref of the branch [\u0026hellip;] that triggered the workflow run. For workflows triggered by push, this is the branch [\u0026hellip;] that was pushed. [\u0026hellip;] Taking a look at the git-source-provider.ts and the getRefSpec functions in ref-helper.ts shows that actions/checkout ensures that exactly the commit specified in the github.sha variable of the github context is fetched and checked out.\nSo now we know! Across the lifetime of a workflow run, github.sha never changes. So by default, actions/checkout always checks out the commit that triggered the workflow.\nDirty Fix #1: git pull# One suggestion from the GitHub issue I linked to earlier is to perform a git pull after checking out the repository like this:\n# DIRTY CODE: KINDA WORKS! publish: needs: update-changelog runs-on: ubuntu-latest steps: - name: Check Out the Repo Again uses: actions/checkout@v3 - name: Pull Changes run: git pull origin main # OR git pull origin ${{ github.ref_name }} - name: Display CHANGELOG.md run: | cat CHANGELOG.md # The changes are here üéâ This looks harmless enough, right? After checking out the code, we simply get the changes we made by running git pull. Before we discuss why this might be a bad idea, let\u0026rsquo;s look at the second dirty fix that \u0026ldquo;kinda works\u0026rdquo;.\nDirty Fix #2: Use ref: main# Another recommendation given in the linked issue is to explicitly specify the branch to check out via the ref parameter as follows:\n# DIRTY CODE: KINDA WORKS! publish: needs: update-changelog runs-on: ubuntu-latest steps: - name: Check Out the Repo Again uses: actions/checkout@v3 with: ref: main # OR ${{ github.ref }} - name: Display CHANGELOG.md run: | cat CHANGELOG.md # The changes are here üéâ Instead of checking out an exact commit, the action now checks out the latest available commit on the branch specified in the ref parameter. The resulting behavior is effectively identical to Dirty Fix #1.\nWhy Is This a Bad Idea?# I know this diagram is a terrible illustration of a race condition, but I wanted to try out GoAT:\ng i t h u b . s h a 1 . u p d a t e - c h a n g 2 e . l o O g O P S I E üèá 3 . p u b l i m s a h i n The diagram illustrates a possible race condition that can occur when using the dirty fixes above. In this context, an OOPSIE is another commit being pushed right between the execution of the update-changelog and publish jobs. This means, that we could end up with a discrepancy between the changelog and what is published. While this is more unlikely to happen when working on your own, it can certainly happen on busy branches.\nI know this is a terrible diagram to illustrate a race condition, but I wanted to try out GoAT ASCII diagram generation.\nThe Solution# To mitigate this issue, we have to make sure that the publish job checks out exactly the commit that was pushed by the update-changelog job.\nTo do that, we first define a commit_hash output on the update-changelog job. It contains the value of the commit_hash output of the stefanzweifel/git-auto-commit-action:\nupdate-changelog: runs-on: ubuntu-latest outputs: commit_hash: ${{ steps.commit-and-push.outputs.commit_hash }} steps: - name: Check Out the Repo uses: actions/checkout@v3 - name: Update CHANGELOG.md run: echo \u0026#34;Added changes on $(date)\u0026#34; \u0026gt;\u0026gt; CHANGELOG.md - name: Commit and Push Changes id: commit-and-push uses: stefanzweifel/git-auto-commit-action@v4 I recommend using the stefanzweifel/git-auto-commit-action, but if you want to do things manually, the Commit and Push Changes step would look similar to this:\n- name: Commit and Push Changes id: commit-and-push run: | git add CHANGELOG.md git config --local user.email \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git config --local user.name \u0026#34;github-actions[bot]\u0026#34; git commit -m \u0026#34;Update changelog\u0026#34; git push echo \u0026#34;commit_hash=$(git rev-parse HEAD)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT In the following publish step, we can reference the commit_hash output like this:\npublish: needs: update-changelog runs-on: ubuntu-latest steps: - name: Check Out the Repo Again uses: actions/checkout@v3 with: ref: ${{ needs.update-changelog.outputs.commit_hash }} - name: Display CHANGELOG.md run: | cat CHANGELOG.md # The changes are here üéâüéâüéâ This makes sure that we check out exactly what we committed and pushed in the update-changelog job.\nOne final thing to note is that this puts the repository in a detached HEAD state (not a bad thing).\nI hope you enjoyed this article and that I could clear up a bit of the confusion surrounding this topic. I would love to know what you think!\n","date":"2023-04-02","id":0,"permalink":"/blog/checking-out-code-committed-within-the-same-github-actions-workflow-run/","summary":"When using GitHub Actions, we often include steps that use Git to check out the code of the repository, make some changes to the code, and then commit these changes back to the repo. Additional steps subsequently act upon these changes within the same workflow run. For example, appending auto-generated release notes to a CHANGELOG.md file and committing it to the repo, to later bundle the changelog with release artifacts. Another example is using a workflow that periodically uses a linter to clean up the code base. Simple enough, right?\n","tags":["CI","Continuous Integration","DevOps","Git","GitHub Actions"],"title":"Checking Out Code Committed Within the Same GitHub Actions Workflow Run"},{"content":"In this post, I\u0026rsquo;ll share how I use OBS Studio and ffmpeg to create short MP4/WebM video snippets for my blog posts. Using the \u0026lt;video\u0026gt; tag with the autoplay and loop attributes makes them look like GIFs. However, modern video formats result in much smaller file sizes.\nThe Matrix effect is powered by the cmatrix PowerShell module The above 30 second video has the following properties:\nFrames per second 10 Resolution 1024x576 (16:9) Size 316 KB (MP4)\n294 KB (WebM) Bitrate dynamic Most of my video content showcases CLI output or me interacting with an application. So I much prefer small file sizes over quality. The content area of my blog is less than 1000 pixels wide, so a video width of 1024 pixels is more than enough. 10 FPS looks choppy but is good enough for my purposes. Using dynamic bitrate reduces the file size even further.\nOBS Settings# In OBS Studio, under Settings ‚Üí Video, set the resolution and FPS.\nUnder Settings ‚Üí Output ‚Üí Recording:\nSet the Recording Format to mp4 Set Encoder to x264 I chose CRF Rate Control on 23 for simplicity. Try lower values to increase quality. The CPU Usage Preset placebo will slow down encoding significantly and murder your CPU. But it will result in better compression. I usually use Tune stillimage because the picture in many of my videos doesn\u0026rsquo;t change a lot in-between frames Remove Audio# Most of my videos don\u0026rsquo;t have any sound. To save a few kilobytes, I use ffmpeg\u0026rsquo;s -an option to remove the audio stream from the container:\nffmpeg -i input.mp4 -c copy -an output.mp4 The -c copy codec option causes ffmpeg to copy all streams to the output file instead of re-encoding them.\nTranscode to WebM# To transcode input.mp4 to WebM, I use the following command:\nffmpeg -i input.mp4 -c:v libvpx-vp9 -crf 31 -b:v 0 -an output.webm It re-encodes the video using the VP9 codec, with its default CRF value of 31. The option -b:v 0 forces a dynamic bitrate. We remove the audio stream again using the -an option.\nCreate Thumbnail# We select exactly one frame using the option -frames:v 1 option. The -ss option allows us to set the time for capturing the thumbnail. The following command captures the thumbnail from 25 seconds into the video:\nffmpeg -i input.mp4 -ss 00:00:25 -frames:v 1 output.jpg Thanks for reading! Your mileage may vary, so I encourage you to read the ffmpeg docs and play around with the settings.\n","date":"2022-04-26","id":1,"permalink":"/blog/use-obs-and-ffmpeg-to-create-gif-like-screencasts/","summary":"In this post, I\u0026rsquo;ll share how I use OBS Studio and ffmpeg to create short MP4/WebM video snippets for my blog posts. Using the \u0026lt;video\u0026gt; tag with the autoplay and loop attributes makes them look like GIFs. However, modern video formats result in much smaller file sizes.\n","tags":["OBS Studio","ffmpeg","MP4","Transcoding","WebM"],"title":"Use OBS and ffmpeg to Create Modern GIF-like Screencasts"},{"content":"Over the weekend, I was on a quest to find the cheapest available Azure Virtual Machine to house my Azure Kubernetes Service cluster. I previously wrote about optimizing storage cost with Azure Kubernetes Service (AKS), where I talk about the importance of selecting VMs that support ephemeral OS disks. But what VM type and region should I choose? Searching through reservation offers in the Azure Portal turned out to be tedious. This time, I\u0026rsquo;ll demonstrate how to find the best offers using PowerShell and export the results to a CSV file.\nAs always, you can find the source code on my GitHub.\nSo Many Choices# My AKS cluster hosts a handful of services and doesn\u0026rsquo;t need a lot of resources. Two vCPUs, eight gigs of RAM, and Premium Storage are sufficient. The analytics data suggests that most of my readers are from the US and Europe, so any region between the eastern US and western Europe will do:\ncanadaeast canadacentral eastus eastus2 northeurope westeurope ukwest uksouth switzerlandnorth germanywestcentral francecentral norwayeast swedencentral We can browse offers in the Azure Portal under Reservations ‚Üí Add ‚Üí Virtual machine. However, we can only request one quote for a reservation at a time.\nClicking through the VMs for each region would take a long time. üò¥ It certainly would have been faster than writing this blog article. But hacking together a PowerShell script was way more fun! I used a bunch of heuristics that \u0026ldquo;just work\u0026rdquo;, so it\u0026rsquo;s not perfect and causes some 400 Bad Request responses. But it gets the job done.\nRequirements# We\u0026rsquo;ll need the following things to get started:\nPowerShell version 7.2.2 or later Azure Az PowerShell module Active Azure subscription Parameters and Helper Functions# Let\u0026rsquo;s create a file named Get-VmReservationQuotes.ps1 and add the following parameters to it:\nparam ( [Parameter(Mandatory, HelpMessage=\u0026#34;Azure subscription ID\u0026#34;)] [string] $SubscriptionId, [Parameter(HelpMessage=\u0026#34;Reservation term duration in years (1 or 3)\u0026#34;)] [int] $TermYears = 3, [Parameter(HelpMessage=\u0026#34;Azure regions\u0026#34;)] [string[]] $Locations = @( \u0026#34;canadaeast\u0026#34;, \u0026#34;canadacentral\u0026#34;, \u0026#34;eastus\u0026#34;, \u0026#34;eastus2\u0026#34;, \u0026#34;northeurope\u0026#34;, \u0026#34;westeurope\u0026#34;, \u0026#34;ukwest\u0026#34;, \u0026#34;uksouth\u0026#34;, \u0026#34;switzerlandnorth\u0026#34;, \u0026#34;germanywestcentral\u0026#34;, \u0026#34;francecentral\u0026#34;, \u0026#34;norwayeast\u0026#34;, \u0026#34;swedencentral\u0026#34; ), [Parameter(HelpMessage=\u0026#34;CSV output filename\u0026#34;)] [string] $OutFile = \u0026#34;VmReservationQuotes.csv\u0026#34; ) This allows us to call the script like this:\n.\\Get-VmReservationQuotes.ps1 -SubscriptionId \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; We also add some helper functions that we\u0026rsquo;ll re-use throughout the script:\nfunction Get-VmLocation ($vmSize) { return $vmSize.Locations[0].ToLower() } function Test-VmCapability ($vmSize, $capabilityName, $capabilityValue) { foreach($capability in $vmSize.Capabilities) { if ($capability.Name -eq $capabilityName -and $capability.Value -eq $capabilityValue) { return $true } } return $false } Get-VmLocation extracts the location from a VM object returned by the Get-AzComputeResourceSku cmdlet. With Test-VmCapability, we can check whether the VM supports a specific feature like ephemeral OS disks:\nTest-VmCapability $vmSize \u0026#34;EphemeralOSDiskSupported\u0026#34; \u0026#34;True\u0026#34; Query and Filter Available VM Types# Using Get-AzComputeResourceSku and our helper functions, we query Azure for all the available VM types and apply our filter heuristics:\n$vmSizes = @() foreach ($vmSize in Get-AzComputeResourceSku) { # Skip `availibilitySets`, `disks`, etc. if (-not ($vmSize.ResourceType -eq \u0026#39;virtualMachines\u0026#39;)) { continue } # Skip unavailable offers if ($vmSize.Restrictions.Count -gt 0) { continue } # Filter locations if (-not $Locations.Contains((Get-VmLocation $vmSize))) { continue } # Select D-Series VMs if (-not $vmSize.Name.StartsWith(\u0026#34;Standard_D\u0026#34;)) { continue } # Exclude confidential VMs if ($vmSize.Name.StartsWith(\u0026#34;Standard_DC\u0026#34;)) { continue } # Exclude memory-optimized VMs if ($vmSize.Name.StartsWith(\u0026#34;Standard_D11\u0026#34;) -or $vmSize.Name.StartsWith(\u0026#34;Standard_DS11\u0026#34;)) { continue } if (-not (Test-VmCapability $vmSize \u0026#34;EphemeralOSDiskSupported\u0026#34; \u0026#34;True\u0026#34;)) { continue } if (-not (Test-VmCapability $vmSize \u0026#34;PremiumIO\u0026#34; \u0026#34;True\u0026#34;)) { continue } if (-not (Test-VmCapability $vmSize \u0026#34;vCPUs\u0026#34; \u0026#34;2\u0026#34;)) { continue } $vmSizes += $vmSize } Pretty straightforward, right?\nRequest Quotes# Now we put the filtered $vmSizes through another loop and query Azure for quotes using Get-AzReservationQuote. After, we calculate the monthly price and export the results as CSV.\n$i = 0 foreach ($vmSize in $vmSizes) { $location = Get-VmLocation $vmSize $displayName = \u0026#34;$($vmSize.Name)-$location\u0026#34; # Progress bar $i++ $percent = [Math]::Floor(($i / $vmSizes.Count) * 100) Write-Progress -Activity \u0026#34;Requesting VM quotes\u0026#34; -Status \u0026#34;$percent% $displayName\u0026#34; -PercentComplete $percent $quote = Get-AzReservationQuote ` -ReservedResourceType \u0026#34;VirtualMachines\u0026#34; ` -Sku $vmSize.Name ` -Location $location ` -Term \u0026#34;P${TermYears}Y\u0026#34; ` -BillingScopeId $SubscriptionId ` -Quantity 1 ` -AppliedScopeType Shared ` -DisplayName \u0026#34;$displayName\u0026#34; # BillingCurrencyTotal is a JSON string, e.g.: # { # \u0026#34;currencyCode\u0026#34;: \u0026#34;CHF\u0026#34;, # \u0026#34;amount\u0026#34;: 1130 # } # Extract amount value `1130` from JSON $billingCurrencyTotal = $quote.BillingCurrencyTotal | ConvertFrom-Json $termPrice = $billingCurrencyTotal.amount @{ Name = $vmSize.Name; Location = $location; PricePerMonth = $termPrice / ($TermYears * 12) } | Export-Csv -Path \u0026#34;${OutFile}\u0026#34; -NoTypeInformation -Delimiter \u0026#34;;\u0026#34; -Append } Besides the progress bar, the trickiest part is extracting the price from the BillingCurrencyTotal multi-string field via regular expression.\nAnd The Winner Is ü•Å# VM Standard_D2as_v4 Location eastus Monthly Cost CHF 25.94\nUSD 27.10 Pretty cool, eh? Do you have any thoughts or questions? Let me know in the comments or on Twitter. Feedback is always welcome!\n","date":"2022-04-26","id":2,"permalink":"/blog/use-powershell-to-request-quotes-for-azure-reservations/","summary":"Over the weekend, I was on a quest to find the cheapest available Azure Virtual Machine to house my Azure Kubernetes Service cluster. I previously wrote about optimizing storage cost with Azure Kubernetes Service (AKS), where I talk about the importance of selecting VMs that support ephemeral OS disks. But what VM type and region should I choose? Searching through reservation offers in the Azure Portal turned out to be tedious. This time, I\u0026rsquo;ll demonstrate how to find the best offers using PowerShell and export the results to a CSV file.\n","tags":["Azure","Azure PowerShell","Azure Reservations","Azure Virtual Machines","PowerShell"],"title":"Use PowerShell to Request Quotes for Azure Reservations"},{"content":"One aspect of managing Azure Virtual Desktop (AVD) is keeping it up-to-date. One strategy is periodically building a \u0026ldquo;golden\u0026rdquo; image and re-deploying AVD session host VMs using the updated image. In this post, we\u0026rsquo;ll use Packer and GitHub Actions to build a Windows 11 image and push it to Azure.\nFirst, we\u0026rsquo;ll use Terraform to prepare some resources for Packer: a resource group for build artifacts and a service principal (SP) for authentication. We\u0026rsquo;ll also export the SP credentials as GitHub Actions secrets, making them available to our CI workflow.\nThen we\u0026rsquo;ll build a customized Windows 11 image with Packer suitable for software development workstations. We\u0026rsquo;ll use Chocolatey to install some apps like Visual Studio 2022 for .NET development, 7zip, the Kubernetes CLI and more. We\u0026rsquo;ll also use a custom PowerShell script to install Azure PowerShell.\nFinally, we\u0026rsquo;ll schedule a GitHub Actions workflow that runs the Packer build. We\u0026rsquo;ll query Azure daily to check for new Windows releases and run Packer as soon as a new version is made available by Microsoft.\nAs usual, all the code is available on GitHub.\nGet the Latest Windows 11 Version Available on Azure# Microsoft releases monthly quality updates for Windows, on Patch Tuesday, the second Tuesday of each month. Microsoft can provide a release outside of the monthly schedule in exceptional circumstances, e.g., to fix a critical security vulnerability.\nWe can use the Azure CLI to query the available Windows versions on Azure like this:\naz vm image list \\ --publisher MicrosoftWindowsDesktop \\ --offer office-365 \\ --sku win11-23h2-avd-m365 \\ --all If you prefer using a Windows 11 base image without Office 365, use --offer windows-11 and --sku win11-23h2-avd instead. You can discover more images using the Azure CLI commands az vm image list-publishers, az vm image list-offers, and az vm image list-skus.\nSo, to specify an image, a value for publisher, offer, SKU, and version is required. To get the latest version number available, we use the follwing snippet (thanks pc-dok for commenting and providing a more elegant query):\naz vm image show \\ --urn \u0026#34;${IMAGE_PUBLISHER}:${IMAGE_OFFER}:${IMAGE_SKU}:latest\u0026#34; \\ --query name \\ --out tsv The result of the command above looks like this:\n22631.4037.240813 Prepare Packer Resources with Terraform# Before being able to use Packer, we have to create some resources. I like using Terraform, but you could also use the Azure CLI or the Azure Portal.\nResource Groups# First we create two resource groups:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;packer_artifacts\u0026#34; { name = \u0026#34;packer-artifacts-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;packer_build\u0026#34; { name = \u0026#34;packer-build-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } Packer puts resources required during build time into the packer-build-rg resource group, which means it should only contain resources during build time. Packer publishes the resulting managed images to the packer-artifacts-rg resource group. We could also let Packer manage the creation of resource groups, but in my opinion, it\u0026rsquo;s easier to scope permissions if we pre-provision them.\nAuthentication and Authorization# We\u0026rsquo;ll use SP authentication with Packer because it integrates well with GitHub Actions:\nresource \u0026#34;azuread_application\u0026#34; \u0026#34;packer\u0026#34; { display_name = \u0026#34;packer-sp-app\u0026#34; } resource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;packer\u0026#34; { client_id = azuread_application.packer.client_id } resource \u0026#34;azuread_service_principal_password\u0026#34; \u0026#34;packer\u0026#34; { service_principal_id = azuread_service_principal.packer.id } To authorize the SP to manage resources inside the resource groups, we use role-based access control (RBAC) and assign the SP the Contributor role. To allow the SP to check for existing images via the Azure CLI az image show command, we also assign it the Reader role on the subscription level.\ndata \u0026#34;azurerm_subscription\u0026#34; \u0026#34;subscription\u0026#34; {} resource \u0026#34;azurerm_role_assignment\u0026#34; \u0026#34;subscription_reader\u0026#34; { scope = data.azurerm_subscription.subscription.id role_definition_name = \u0026#34;Reader\u0026#34; principal_id = azuread_service_principal.packer.id } resource \u0026#34;azurerm_role_assignment\u0026#34; \u0026#34;packer_build_contributor\u0026#34; { scope = azurerm_resource_group.packer_build.id role_definition_name = \u0026#34;Contributor\u0026#34; principal_id = azuread_service_principal.packer.id } resource \u0026#34;azurerm_role_assignment\u0026#34; \u0026#34;packer_artifacts_contributor\u0026#34; { scope = azurerm_resource_group.packer_artifacts.id role_definition_name = \u0026#34;Contributor\u0026#34; principal_id = azuread_service_principal.packer.id } Export GitHub Actions Secrets# To make the credentials accessible to GitHub Actions, we export them to GitHub as encrypted secrets like this:\ndata \u0026#34;github_repository\u0026#34; \u0026#34;packer_windows_avd\u0026#34; { full_name = \u0026#34;schnerring/packer-windows-avd\u0026#34; } resource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_client_id\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_CLIENT_ID\u0026#34; plaintext_value = azuread_application.packer.client_id } resource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_client_secret\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_CLIENT_SECRET\u0026#34; plaintext_value = azuread_service_principal_password.packer.value } resource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_subscription_id\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_SUBSCRIPTION_ID\u0026#34; plaintext_value = data.azurerm_subscription.subscription.subscription_id } resource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_tenant_id\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_TENANT_ID\u0026#34; plaintext_value = data.azurerm_subscription.subscription.tenant_id } We\u0026rsquo;ll later also make use of the Azure Login Action to dynamically query the latest Windows version available on Azure with the Azure CLI. It expects the credentials in the following JSON format:\n{ \u0026#34;clientId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, \u0026#34;subscriptionId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;\u0026lt;GUID\u0026gt;\u0026#34; } Let\u0026rsquo;s export another secret named AZURE_CREDENTIALS containing the credentials in the JSON format above using Terraform\u0026rsquo;s jsonencode function:\nresource \u0026#34;github_actions_secret\u0026#34; \u0026#34;github_actions_azure_credentials\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;AZURE_CREDENTIALS\u0026#34; plaintext_value = jsonencode( { clientId = azuread_application.packer.client_id clientSecret = azuread_service_principal_password.packer.value subscriptionId = data.azurerm_subscription.subscription.subscription_id tenantId = data.azurerm_subscription.subscription.tenant_id } ) } Finally, we export the names of the resource groups we created like this:\nresource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_artifacts_resource_group\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_ARTIFACTS_RESOURCE_GROUP\u0026#34; plaintext_value = azurerm_resource_group.packer_artifacts.name } resource \u0026#34;github_actions_secret\u0026#34; \u0026#34;packer_build_resource_group\u0026#34; { repository = data.github_repository.packer_windows_avd.name secret_name = \u0026#34;PACKER_BUILD_RESOURCE_GROUP\u0026#34; plaintext_value = azurerm_resource_group.packer_build.name } Add Terraform Outputs# To run Packer locally, we also need to access the credentials locally. We can use Terraform outputs for this:\noutput \u0026#34;packer_artifacts_resource_group\u0026#34; { value = azurerm_resource_group.packer_artifacts.name } output \u0026#34;packer_build_resource_group\u0026#34; { value = azurerm_resource_group.packer_build.name } output \u0026#34;packer_client_id\u0026#34; { value = azuread_application.packer.client_id sensitive = true } output \u0026#34;packer_client_secret\u0026#34; { value = azuread_service_principal_password.packer.value sensitive = true } output \u0026#34;packer_subscription_id\u0026#34; { value = data.azurerm_subscription.subscription.subscription_id sensitive = true } output \u0026#34;packer_tenant_id\u0026#34; { value = data.azurerm_subscription.subscription.tenant_id sensitive = true } After running terraform apply to deploy everything, we can access output values like this: terraform output packer_client_secret.\nCreate the Packer Template# Let\u0026rsquo;s add a Packer template file named windows.pkr.hcl.\nInput Variables# Input variables allow us to parameterize the Packer build. We can later set their values from default values, environment, file, or CLI arguments.\nWe need to add variables allowing us to pass the SP credentials and resource group names, as well as the image publisher, offer, SKU, and version:\nvariable \u0026#34;client_id\u0026#34; { type = string description = \u0026#34;Azure Service Principal App ID.\u0026#34; sensitive = true } variable \u0026#34;client_secret\u0026#34; { type = string description = \u0026#34;Azure Service Principal Secret.\u0026#34; sensitive = true } variable \u0026#34;subscription_id\u0026#34; { type = string description = \u0026#34;Azure Subscription ID.\u0026#34; sensitive = true } variable \u0026#34;tenant_id\u0026#34; { type = string description = \u0026#34;Azure Tenant ID.\u0026#34; sensitive = true } variable \u0026#34;artifacts_resource_group\u0026#34; { type = string description = \u0026#34;Packer Artifacts Resource Group.\u0026#34; } variable \u0026#34;build_resource_group\u0026#34; { type = string description = \u0026#34;Packer Build Resource Group.\u0026#34; } variable \u0026#34;source_image_publisher\u0026#34; { type = string description = \u0026#34;Windows Image Publisher.\u0026#34; } variable \u0026#34;source_image_offer\u0026#34; { type = string description = \u0026#34;Windows Image Offer.\u0026#34; } variable \u0026#34;source_image_sku\u0026#34; { type = string description = \u0026#34;Windows Image SKU.\u0026#34; } variable \u0026#34;source_image_version\u0026#34; { type = string description = \u0026#34;Windows Image Version.\u0026#34; } Configure Azure ARM Builder# Next, we configure Packer\u0026rsquo;s Azure Resource Manager (ARM) Builder. We start with the source configuration block:\nsource \u0026#34;azure-arm\u0026#34; \u0026#34;avd\u0026#34; { # WinRM Communicator communicator = \u0026#34;winrm\u0026#34; winrm_use_ssl = true winrm_insecure = true winrm_timeout = \u0026#34;5m\u0026#34; winrm_username = \u0026#34;packer\u0026#34; # Service Principal Authentication client_id = var.client_id client_secret = var.client_secret subscription_id = var.subscription_id tenant_id = var.tenant_id # Source Image os_type = \u0026#34;Windows\u0026#34; image_publisher = var.source_image_publisher image_offer = var.source_image_offer image_sku = var.source_image_sku image_version = var.source_image_version # Destination Image managed_image_resource_group_name = var.artifacts_resource_group managed_image_name = \u0026#34;${var.source_image_sku}-${var.source_image_version}\u0026#34; # Packer Computing Resources build_resource_group_name = var.build_resource_group vm_size = \u0026#34;Standard_D4ds_v4\u0026#34; } The WinRM communicator is Packer\u0026rsquo;s way of talking to Azure Windows VMs. We give the resulting image a unique managed_image_name by concatenating the values of the SKU and version. The remaining options are self-explanatory.\nNext, we define a builder block that contains our provisioning steps:\nbuild { source \u0026#34;azure-arm.avd\u0026#34; {} # Install Chocolatey: https://chocolatey.org/install#individual provisioner \u0026#34;powershell\u0026#34; { inline = [\u0026#34;Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://community.chocolatey.org/install.ps1\u0026#39;))\u0026#34;] } # Install Chocolatey packages provisioner \u0026#34;file\u0026#34; { source = \u0026#34;./packages.config\u0026#34; destination = \u0026#34;D:/packages.config\u0026#34; } provisioner \u0026#34;powershell\u0026#34; { inline = [\u0026#34;choco install --confirm D:/packages.config\u0026#34;] # See https://docs.chocolatey.org/en-us/choco/commands/install#exit-codes valid_exit_codes = [0, 3010] } provisioner \u0026#34;windows-restart\u0026#34; {} # Azure PowerShell Modules provisioner \u0026#34;powershell\u0026#34; { script = \u0026#34;./install-azure-powershell.ps1\u0026#34; } # Generalize image using Sysprep # See https://www.packer.io/docs/builders/azure/arm#windows # See https://docs.microsoft.com/en-us/azure/virtual-machines/windows/build-image-with-packer#define-packer-template provisioner \u0026#34;powershell\u0026#34; { inline = [ \u0026#34;while ((Get-Service RdAgent).Status -ne \u0026#39;Running\u0026#39;) { Start-Sleep -s 5 }\u0026#34;, \u0026#34;while ((Get-Service WindowsAzureGuestAgent).Status -ne \u0026#39;Running\u0026#39;) { Start-Sleep -s 5 }\u0026#34;, \u0026#34;\u0026amp; $env:SystemRoot\\\\System32\\\\Sysprep\\\\Sysprep.exe /oobe /generalize /quiet /quit /mode:vm\u0026#34;, \u0026#34;while ($true) { $imageState = Get-ItemProperty HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Setup\\\\State | Select ImageState; if($imageState.ImageState -ne \u0026#39;IMAGE_STATE_GENERALIZE_RESEAL_TO_OOBE\u0026#39;) { Write-Output $imageState.ImageState; Start-Sleep -s 10 } else { break } }\u0026#34; ] } } Let\u0026rsquo;s look at what\u0026rsquo;s happening here:\nFirst, we reference the source block that we defined before. We install Chocolatey using the one-liner PowerShell command from the official docs. We copy the packages.config file, an XML manifest containing a list of apps for Chocolatey to install, to the temporary disk D: of the VM. Then we pass the manifest to the choco install command. When the command exits with the code 3010, a reboot is required. We make Packer aware of that by passing 3010 to the list of valid_exit_codes. For good measure, we reboot the VM. We run a custom PowerShell script to install the Azure PowerShell modules. Finally, we generalize the image using Sysprep The packages.config manifest looks like this:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages\u0026gt; \u0026lt;!-- FSLogix --\u0026gt; \u0026lt;package id=\u0026#34;fslogix\u0026#34; /\u0026gt; \u0026lt;!-- PowerShell --\u0026gt; \u0026lt;!-- See https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4#install-the-msi-package-from-the-command-line --\u0026gt; \u0026lt;package id=\u0026#34;powershell-core\u0026#34; installArguments=\u0026#34;ADD_FILE_CONTEXT_MENU_RUNPOWERSHELL=1 ADD_EXPLORER_CONTEXT_MENU_OPENPOWERSHELL=1\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;openssh\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;curl\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;wget\u0026#34; /\u0026gt; \u0026lt;!-- Visual Studio 2022 Community --\u0026gt; \u0026lt;!-- See https://docs.microsoft.com/en-us/visualstudio/install/use-command-line-parameters-to-install-visual-studio?view=vs-2022#layout-command-and-command-line-parameters --\u0026gt; \u0026lt;!-- See https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-community?view=vs-2022 --\u0026gt; \u0026lt;package id=\u0026#34;visualstudio2022community\u0026#34; packageParameters=\u0026#34;--add Microsoft.VisualStudio.Workload.Azure;includeRecommended --add Microsoft.VisualStudio.Workload.ManagedDesktop;includeRecommended --add Microsoft.VisualStudio.Workload.NetCrossPlat;includeRecommended --add Microsoft.VisualStudio.Workload.NetWeb;includeRecommended --add Microsoft.VisualStudio.Workload.Office;includeRecommended\u0026#34; /\u0026gt; \u0026lt;!-- System Administration --\u0026gt; \u0026lt;package id=\u0026#34;sysinternals\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;rsat\u0026#34; /\u0026gt; \u0026lt;!-- Developer Tools --\u0026gt; \u0026lt;package id=\u0026#34;azure-cli\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;filezilla\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;git\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;sql-server-management-studio\u0026#34; /\u0026gt; \u0026lt;!-- Kubernetes --\u0026gt; \u0026lt;package id=\u0026#34;kubernetes-cli\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;kubernetes-helm\u0026#34; /\u0026gt; \u0026lt;!-- Hashicorp --\u0026gt; \u0026lt;package id=\u0026#34;packer\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;terraform\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;graphviz\u0026#34; /\u0026gt; \u0026lt;!-- Common Apps --\u0026gt; \u0026lt;package id=\u0026#34;7zip\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;sumatrapdf\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;keepassxc\u0026#34; /\u0026gt; \u0026lt;/packages\u0026gt; Note that installing Chocolatey packages like that is a pretty naive approach that I wouldn\u0026rsquo;t recommend for production-level scenarios. Using the Chocolatey community repository has limits in terms of reliability, control, and trust.. Also, any failing package installation breaks the entire build, so pinning versions is a good idea.\nThe install-azure-powershell.ps1 provisioning script to install the Azure PowerShell modules looks like this:\n# See https://learn.microsoft.com/en-us/powershell/azure/install-azps-windows?view=azps-12.2.0\u0026amp;tabs=powershell\u0026amp;pivots=windows-msi $ErrorActionPreference = \u0026#34;Stop\u0026#34; $downloadUrl = \u0026#34;https://github.com/Azure/azure-powershell/releases/download/v12.2.0-August2024/Az-Cmdlets-12.2.0.38863-x64.msi\u0026#34; $outFile = \u0026#34;D:\\az_pwsh.msi\u0026#34; # temporary disk Write-Host \u0026#34;Downloading $downloadUrl ...\u0026#34; [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 Invoke-WebRequest -Uri $downloadUrl -OutFile $outFile Write-Host \u0026#34;Installing ...\u0026#34; Start-Process \u0026#34;msiexec.exe\u0026#34; -Wait -ArgumentList \u0026#34;/package $outFile\u0026#34; Write-Host \u0026#34;Done.\u0026#34; We only covered two methods for installing software into the golden image. Often it doesn\u0026rsquo;t even make sense to bake apps into your golden image, e.g., when app deployments frequently change. For these scenarios, other solutions like Microsoft Endpoint Manager or MSIX App Attach exist.\nRun Packer Locally# To run Packer locally, we use the following command (note the . at the very end):\npacker build \\ -var \u0026#34;artifacts_resource_group=$(terraform output -raw packer_artifacts_resource_group)\u0026#34; \\ -var \u0026#34;build_resource_group=$(terraform output -raw packer_build_resource_group)\u0026#34; \\ -var \u0026#34;client_id=$(terraform output -raw packer_client_id)\u0026#34; \\ -var \u0026#34;client_secret=$(terraform output -raw packer_client_secret)\u0026#34; \\ -var \u0026#34;subscription_id=$(terraform output -raw packer_subscription_id)\u0026#34; \\ -var \u0026#34;tenant_id=$(terraform output -raw packer_tenant_id)\u0026#34; \\ -var \u0026#34;source_image_publisher=MicrosoftWindowsDesktop\u0026#34; \\ -var \u0026#34;source_image_offer=office-365\u0026#34; \\ -var \u0026#34;source_image_sku=win11-23h2-avd-m365\u0026#34; \\ -var \u0026#34;source_image_version=22631.4037.240813\u0026#34; \\ . With the -var option, we can set Packer variables. We pass the Terraform outputs that we configured earlier to Packer using command substitution.\nGitHub Actions# Next, we tie everything together using GitHub Actions. Add a workflow by adding a file named .github/workflows/packer.yml. We name the workflow and specify the events that trigger it:\nname: Packer Windows 11 on: push: branches: - main schedule: - cron: 0 0 * * * It\u0026rsquo;s triggered whenever we push code to the main branch. We schedule to run the workflow daily at 0:00 UTC using the POSIX cron syntax.\nUsing workflow-level environment variables, we specify the desired source image publisher, offer, and SKU :\nenv: AZ_CLI_VERSION: 2.40.1 IMAGE_PUBLISHER: MicrosoftWindowsDesktop # With Office 365 IMAGE_OFFER: office-365 IMAGE_SKU: win11-23h2-avd-m365 # Without Office 365 #IMAGE_OFFER: windows-11 #IMAGE_SKU: win11-23h2-avd Here is a high-level overview of the workflow jobs where all the magic happens:\njobs: latest_windows_version: # Get latest Windows version from Azure check_image_exists: # Check if latest version has already been built packer: # Run Packer Let\u0026rsquo;s look into each job in detail next.\nJob: latest_windows_version# We use the AZURE_CREDENTIALS secret we defined earlier with Terraform to authenticate to Azure with the Azure Login Action.\nAfter, we use the Azure CLI Action to run the snippet to calculate the latest available image version.\nTo allow using the result from the other jobs, we define the version job output. It\u0026rsquo;s set using the echo \u0026quot;version=${latest_version}\u0026quot; \u0026gt;\u0026gt; $GITHUB_OUTPUT command.\nlatest_windows_version: name: Get latest Windows version from Azure runs-on: ubuntu-latest outputs: version: ${{ steps.get_latest_version.outputs.version }} steps: - name: Azure Login uses: azure/login@v2 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Get Latest Version id: get_latest_version uses: azure/cli@v2 with: azcliversion: ${{ env.AZ_CLI_VERSION }} inlineScript: | latest_version=$( az vm image show \\ --urn \u0026#34;${IMAGE_PUBLISHER}:${IMAGE_OFFER}:${IMAGE_SKU}:latest\u0026#34; \\ --query name \\ --out tsv ) echo \u0026#34;Publisher: ${IMAGE_PUBLISHER}\u0026#34; echo \u0026#34;Offer: ${IMAGE_OFFER}\u0026#34; echo \u0026#34;SKU: ${IMAGE_SKU}\u0026#34; echo \u0026#34;Version: ${latest_version}\u0026#34; echo \u0026#34;version=${latest_version}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT Job: check_image_exists# This job needs the latest_windows_version job to finish first because it depends on its version output.\nRemember that we name our Packer images (managed_image_name) like ${sku}-${version}. Using the IMAGE_SKU workflow environment variable and the version output from the latest_windows_version job, we can use the az image show command to check for existing images inside the Packer artifacts resource group.\nDepending on whether or not an image exists, we set the job output exists to true or false.\ncheck_image_exists: name: Check if latest version has already been built runs-on: ubuntu-latest needs: latest_windows_version outputs: exists: ${{ steps.get_image.outputs.exists }} steps: - name: Azure Login uses: azure/login@v2 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Check If Image Exists id: get_image uses: azure/cli@v2 with: azcliversion: ${{ env.AZ_CLI_VERSION }} inlineScript: | if az image show \\ --resource-group \u0026#34;${{ secrets.PACKER_ARTIFACTS_RESOURCE_GROUP }}\u0026#34; \\ --name \u0026#34;${IMAGE_SKU}-${{ needs.latest_windows_version.outputs.version }}\u0026#34;; then image_exists=true else image_exists=false fi echo \u0026#34;Image Exists: ${image_exists}\u0026#34; echo \u0026#34;exists=${image_exists}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT Job: packer# This job needs the outputs from both previous jobs. It only runs if the exists output of the check_image_exists job equals false. Otherwise, it\u0026rsquo;s skipped.\nWe use the Checkout Action to make the code available to the workflow and validate the syntax of the Packer template by running packer validate.\nFinally, we run packer build. This time we use environment variables in the form of PKR_VAR_* to set the Packer inputs opposed to the -var CLI option we used earlier when running Packer locally.\npacker: name: Run Packer runs-on: ubuntu-latest needs: [latest_windows_version, check_image_exists] if: needs.check_image_exists.outputs.exists == \u0026#39;false\u0026#39; steps: - name: Checkout Repository uses: actions/checkout@v2 - name: Validate Packer Template uses: hashicorp/packer-github-actions@master with: command: validate arguments: -syntax-only - name: Build Packer Image uses: hashicorp/packer-github-actions@master with: command: build arguments: -color=false -on-error=abort env: PKR_VAR_client_id: ${{ secrets.PACKER_CLIENT_ID }} PKR_VAR_client_secret: ${{ secrets.PACKER_CLIENT_SECRET }} PKR_VAR_subscription_id: ${{ secrets.PACKER_SUBSCRIPTION_ID }} PKR_VAR_tenant_id: ${{ secrets.PACKER_TENANT_ID }} PKR_VAR_artifacts_resource_group: ${{ secrets.PACKER_ARTIFACTS_RESOURCE_GROUP }} PKR_VAR_build_resource_group: ${{ secrets.PACKER_BUILD_RESOURCE_GROUP }} PKR_VAR_source_image_publisher: ${{ env.IMAGE_PUBLISHER }} PKR_VAR_source_image_offer: ${{ env.IMAGE_OFFER }} PKR_VAR_source_image_sku: ${{ env.IMAGE_SKU }} PKR_VAR_source_image_version: ${{ needs.latest_windows_version.outputs.version }} Job: cleanup# If the pipeline crashes somewhere along the way, resources inside the packer-build-rg resource group may linger and incur costs. We want to define a job that purges everything from the resource group.\nOne way to achieve this is by starting a deployment to the resource group using an empty ARM template. First, we add an empty Bicep named cleanup-resource-group.bicep. We can then deploy the template using az deployment group create:\ncleanup: name: Cleanup Packer Resources runs-on: ubuntu-latest needs: packer steps: - name: Checkout Repository uses: actions/checkout@v2 - name: Azure Login uses: azure/login@v2 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Cleanup Resource Group uses: azure/cli@v2 with: azcliversion: ${{ env.AZ_CLI_VERSION }} inlineScript: | az deployment group create \\ --mode Complete \\ --resource-group \u0026#34;${{ secrets.PACKER_BUILD_RESOURCE_GROUP }}\u0026#34; \\ --template-file cleanup-resource-group.bicep What Do You Think?# Phew! That was quite a bit of work. Here is a summary one of the workflow runs that took around one hour to complete:\nI especially like how the GitHub Actions part turned out. Leave a comment or at me on Twitter to let me know what you think about it!\n","date":"2022-03-27","id":3,"permalink":"/blog/automate-building-custom-windows-images-for-azure-virtual-desktop-with-packer-and-github-actions/","summary":"One aspect of managing Azure Virtual Desktop (AVD) is keeping it up-to-date. One strategy is periodically building a \u0026ldquo;golden\u0026rdquo; image and re-deploying AVD session host VMs using the updated image. In this post, we\u0026rsquo;ll use Packer and GitHub Actions to build a Windows 11 image and push it to Azure.\n","tags":["AVD","Azure","Azure CLI","Azure Virtual Desktop","CI","Continuous Integration","DevOps","GitHub Actions","IaC","Infrastructure as Code","Packer","Terraform"],"title":"Automate Building Custom Windows Images For Azure Virtual Desktop With Packer And GitHub Actions"},{"content":"With Azure Virtual Desktop (AVD), you can deliver secure Windows 11 desktops and environments anywhere. It\u0026rsquo;s pretty easy to deploy and scale. You can provide a coherent user experience from any end-user device and reduce costs by leveraging Windows 11 multi-session licensing. This tutorial will guide you through setting up AVD with AADDS using Terraform.\nAs always, all the code is available on my GitHub.\nPrerequisites# Besides an active Azure subscription and Terraform configured on your workstation, Azure Active Directory Domain Services (AADDS) are required. Check out my previous post on setting up AADDS with Terraform if you haven\u0026rsquo;t already! Some Terraform resources in this guide, e.g., the network peerings and AADDS domain-join (AADDS-join) VM extension, depend on the AADDS resources from that post.\nDo You Know What\u0026rsquo;s Exciting?# It\u0026rsquo;s possible to just Azure AD-join (AAD-join) AVD session hosts, eliminating the requirement to use AADDS or on-premise AD DS and reduce the costs and complexity of AVD deployments even more. Unfortunately, it\u0026rsquo;s not yet fully production-ready because FSLogix profile support for AAD-joined AVD VMs is only in public preview. Currently, using AAD authentication with Azure Files still requires hybrid identities. But it\u0026rsquo;s nice that AVD is one step closer to being a cloud-only solution. I can\u0026rsquo;t wait to terraformify all of it! Stay tuned because I\u0026rsquo;ll post about it as soon as things are generally available.\nOverview# We\u0026rsquo;ll deploy AADDS and AVD resources to separate virtual networks and resource groups. It is called a hub-spoke network topology, a typical approach to organize large-scale networks. The hub (aadds-vnet), the central connectivity point, typically contains other services besides AADDS. E.g., a VPN gateway connecting your on-premises network to the Azure cloud. Azure Bastion or Azure Firewall are also services that might reside in the hub network. Spoke networks (avd-vnet) contain isolated workloads using network peerings to connect to the hub.\nNetwork Resources# Create the avd-rg resource group and add the avd-vnet spoke network to it. The network uses the AADDS domain controllers (DCs) as dns_servers:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;avd-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } # Network Resources resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;avd-vnet\u0026#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name address_space = [\u0026#34;10.10.0.0/16\u0026#34;] dns_servers = azurerm_active_directory_domain_service.aadds.initial_replica_set.0.domain_controller_ip_addresses } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;avd-snet\u0026#34; resource_group_name = azurerm_resource_group.avd.name virtual_network_name = azurerm_virtual_network.avd.name address_prefixes = [\u0026#34;10.10.0.0/24\u0026#34;] } To give AVD VMs line of sight of AADDS, we need to add the following network peerings:\nresource \u0026#34;azurerm_virtual_network_peering\u0026#34; \u0026#34;aadds_to_avd\u0026#34; { name = \u0026#34;hub-to-avd-peer\u0026#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name remote_virtual_network_id = azurerm_virtual_network.avd.id } resource \u0026#34;azurerm_virtual_network_peering\u0026#34; \u0026#34;avd_to_aadds\u0026#34; { name = \u0026#34;avd-to-aadds-peer\u0026#34; resource_group_name = azurerm_resource_group.avd.name virtual_network_name = azurerm_virtual_network.avd.name remote_virtual_network_id = azurerm_virtual_network.aadds.id } Host Pool# A host pool is a collection of Azure virtual machines that register to Azure Virtual Desktop as session hosts when you run the Azure Virtual Desktop agent. All session host virtual machines in a host pool should be sourced from the same image for a consistent user experience.\nWe add the AVD host pool and the registration info. We\u0026rsquo;ll later register the session hosts via VM extension to the host pool using the token from the registration info:\nlocals { # Switzerland North is not supported avd_location = \u0026#34;West Europe\u0026#34; } resource \u0026#34;azurerm_virtual_desktop_host_pool\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;avd-vdpool\u0026#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name type = \u0026#34;Pooled\u0026#34; load_balancer_type = \u0026#34;BreadthFirst\u0026#34; friendly_name = \u0026#34;AVD Host Pool using AADDS\u0026#34; } resource \u0026#34;time_rotating\u0026#34; \u0026#34;avd_registration_expiration\u0026#34; { # Must be between 1 hour and 30 days rotation_days = 29 } resource \u0026#34;azurerm_virtual_desktop_host_pool_registration_info\u0026#34; \u0026#34;avd\u0026#34; { hostpool_id = azurerm_virtual_desktop_host_pool.avd.id expiration_date = time_rotating.avd_registration_expiration.rotation_rfc3339 } I deploy my AADDS and AVD resources to the Switzerland North region. However, I have to deploy AVD service resources to West Europe because the AVD service isn\u0026rsquo;t available in all regions.\nTo get the latest supported regions, re-register the AVD resource provider:\nSelect your subscription under Subscriptions in the Azure Portal. Select the Resource Provider menu. Re-register Microsoft.DesktopVirtualization. Workspace and App Group# Next, we create a workspace and add an app group. Two types of app groups exist:\nDesktop: full desktop RemoteApp: individual apps Adding the following gives AVD users the full desktop experience:\nresource \u0026#34;azurerm_virtual_desktop_workspace\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;avd-vdws\u0026#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name } resource \u0026#34;azurerm_virtual_desktop_application_group\u0026#34; \u0026#34;avd\u0026#34; { name = \u0026#34;desktop-vdag\u0026#34; location = local.avd_location resource_group_name = azurerm_resource_group.avd.name type = \u0026#34;Desktop\u0026#34; host_pool_id = azurerm_virtual_desktop_host_pool.avd.id } resource \u0026#34;azurerm_virtual_desktop_workspace_application_group_association\u0026#34; \u0026#34;avd\u0026#34; { workspace_id = azurerm_virtual_desktop_workspace.avd.id application_group_id = azurerm_virtual_desktop_application_group.avd.id } Session Hosts# Let\u0026rsquo;s add two session hosts to the AVD host pool. To be able to adjust the amount of VMs inside the host pool later, we define a variable:\nvariable \u0026#34;avd_host_pool_size\u0026#34; { type = number description = \u0026#34;Number of session hosts to add to the AVD host pool.\u0026#34; } Next, we add the VM NICs for the session hosts:\nresource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;avd\u0026#34; { count = var.avd_host_pool_size name = \u0026#34;avd-nic-${count.index}\u0026#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name ip_configuration { name = \u0026#34;avd-ipconf\u0026#34; subnet_id = azurerm_subnet.avd.id private_ip_address_allocation = \u0026#34;Dynamic\u0026#34; } } After, we add the session hosts:\nresource \u0026#34;random_password\u0026#34; \u0026#34;avd_local_admin\u0026#34; { length = 64 } resource \u0026#34;random_id\u0026#34; \u0026#34;avd\u0026#34; { count = var.avd_host_pool_size byte_length = 4 } resource \u0026#34;azurerm_windows_virtual_machine\u0026#34; \u0026#34;avd\u0026#34; { count = var.avd_host_pool_size name = \u0026#34;avd-vm-${count.index}-${random_id.avd[count.index].hex}\u0026#34; location = azurerm_resource_group.avd.location resource_group_name = azurerm_resource_group.avd.name size = \u0026#34;Standard_D4s_v4\u0026#34; license_type = \u0026#34;Windows_Client\u0026#34; admin_username = \u0026#34;avd-local-admin\u0026#34; admin_password = random_password.avd_local_admin.result network_interface_ids = [azurerm_network_interface.avd[count.index].id] os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = \u0026#34;Premium_LRS\u0026#34; } source_image_reference { publisher = \u0026#34;MicrosoftWindowsDesktop\u0026#34; offer = \u0026#34;windows-11\u0026#34; sku = \u0026#34;win11-21h2-avd\u0026#34; version = \u0026#34;latest\u0026#34; } } To ensure the session hosts utilize the licensing benefits available with AVD, we select Windows_Client as license_type value.\nThe reason we append a random number to the VM name is to prevent name conflicts with dangling host pool registrations.\nUnderstanding VM Extensions# To figure out the required VM extensions, I used the AVD wizard of the Azure Portal. During the review + create step, I downloaded the ARM template and reverse-engineered it.\nSometimes, creating complex deployments via Azure Portal feels like magic. Backtracking the generated ARM templates is something I like to do to get a deeper understanding of what\u0026rsquo;s happening under the hood. It\u0026rsquo;s usually my initial step when trying to terraformify something for the first time that I can\u0026rsquo;t find good examples of elsewhere.\nYou can find the AVD ARM templates on official Azure GitHub github.com/Azure/RDS-Templates/ARM-wvd-templates. The VM templates reside in the nestedtemplates directory containing the VM extension resources that we want to replicate with Terraform:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;rdshPrefix\u0026#39;), add(copyindex(), parameters(\u0026#39;vmInitialNumber\u0026#39;)), \u0026#39;/\u0026#39;, \u0026#39;Microsoft.PowerShell.DSC\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;rdsh-vm-loop\u0026#34; ], \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;rdsh-dsc-loop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[parameters(\u0026#39;rdshNumberOfInstances\u0026#39;)]\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Powershell\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DSC\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;2.73\u0026#34;, \u0026#34;autoUpgradeMinorVersion\u0026#34;: true, \u0026#34;settings\u0026#34;: { \u0026#34;modulesUrl\u0026#34;: \u0026#34;[parameters(\u0026#39;artifactsLocation\u0026#39;)]\u0026#34;, \u0026#34;configurationFunction\u0026#34;: \u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;hostPoolName\u0026#34;: \u0026#34;[parameters(\u0026#39;hostpoolName\u0026#39;)]\u0026#34;, \u0026#34;registrationInfoToken\u0026#34;: \u0026#34;[parameters(\u0026#39;hostpoolToken\u0026#39;)]\u0026#34;, \u0026#34;aadJoin\u0026#34;: \u0026#34;[parameters(\u0026#39;aadJoin\u0026#39;)]\u0026#34;, \u0026#34;sessionHostConfigurationLastUpdateTime\u0026#34;: \u0026#34;[parameters(\u0026#39;SessionHostConfigurationVersion\u0026#39;)]\u0026#34; } } } }, { \u0026#34;condition\u0026#34;: \u0026#34;[not(parameters(\u0026#39;aadJoin\u0026#39;))]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;rdshPrefix\u0026#39;), add(copyindex(), parameters(\u0026#39;vmInitialNumber\u0026#39;)), \u0026#39;/\u0026#39;, \u0026#39;joindomain\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;rdsh-dsc-loop\u0026#34; ], \u0026#34;copy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;rdsh-domain-join-loop\u0026#34;, \u0026#34;count\u0026#34;: \u0026#34;[parameters(\u0026#39;rdshNumberOfInstances\u0026#39;)]\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Compute\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;JsonADDomainExtension\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;1.3\u0026#34;, \u0026#34;autoUpgradeMinorVersion\u0026#34;: true, \u0026#34;settings\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;domain\u0026#39;)]\u0026#34;, \u0026#34;ouPath\u0026#34;: \u0026#34;[parameters(\u0026#39;ouPath\u0026#39;)]\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;[parameters(\u0026#39;administratorAccountUsername\u0026#39;)]\u0026#34;, \u0026#34;restart\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;options\u0026#34;: \u0026#34;3\u0026#34; }, \u0026#34;protectedSettings\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;[parameters(\u0026#39;administratorAccountPassword\u0026#39;)]\u0026#34; } } }, However, the default parameters of the ARM template downloaded from the Azure Portal differ from the values found on GitHub, e.g., modulesParameter:\nGitHub: https://raw.githubusercontent.com/Azure/RDS-Templates/master/ARM-wvd-templates/DSC/Configuration.zip Azure: https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-20-2022.zip It seems that Microsoft periodically releases the Configuration.zip to the galleryartifacts container of the wvdportalstorageblob storage account. To peek inside the container, we can use the List Blobs operation of the Blob Service REST API.\nThe URLs that the Azure Portal uses sometimes change. At the time of writing, it uses the Configuration_01-20-2022.zip file despite Configuration_02-23-2022.zip being available.\nAADDS-join the VMs# When AADDS-joining a computer, it will be added to the built-in AADDS Computers Organizational Unit (OU) of the domain by default. To add the VM to a different OU, we can optionally specify the OU path during domain-join. Create an optional variable:\nvariable \u0026#34;avd_ou_path\u0026#34; { type = string description = \u0026#34;OU path used to AADDS domain-join AVD session hosts.\u0026#34; default = \u0026#34;\u0026#34; } We then AADDS-join the session hosts with the JsonADDomainExtension VM extension:\nresource \u0026#34;azurerm_virtual_machine_extension\u0026#34; \u0026#34;avd_aadds_join\u0026#34; { count = var.avd_host_pool_size name = \u0026#34;aadds-join-vmext\u0026#34; virtual_machine_id = azurerm_windows_virtual_machine.avd[count.index].id publisher = \u0026#34;Microsoft.Compute\u0026#34; type = \u0026#34;JsonADDomainExtension\u0026#34; type_handler_version = \u0026#34;1.3\u0026#34; auto_upgrade_minor_version = true settings = \u0026lt;\u0026lt;-SETTINGS { \u0026#34;Name\u0026#34;: \u0026#34;${azurerm_active_directory_domain_service.aadds.domain_name}\u0026#34;, \u0026#34;OUPath\u0026#34;: \u0026#34;${var.avd_ou_path}\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;${azuread_user.dc_admin.user_principal_name}\u0026#34;, \u0026#34;Restart\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;Options\u0026#34;: \u0026#34;3\u0026#34; } SETTINGS protected_settings = \u0026lt;\u0026lt;-PROTECTED_SETTINGS { \u0026#34;Password\u0026#34;: \u0026#34;${random_password.dc_admin.result}\u0026#34; } PROTECTED_SETTINGS lifecycle { ignore_changes = [settings, protected_settings] } depends_on = [ azurerm_virtual_network_peering.aadds_to_avd, azurerm_virtual_network_peering.avd_to_aadds ] } We have to ensure that the session hosts have line of sight to the AADDS DCs. To do that, we add the network peering resources to the depends_on list of the VM extension.\nAfter a VM has been AADDS-joined, it doesn\u0026rsquo;t make sense to join it again when the settings or protected_settings of the VM extension change, so we ignore_changes of these properties.\nRegister VMs to the Host Pool# First, let\u0026rsquo;s add a variable containing the URL to the zip file containing the DSC configuration, making it easier to update it in the future:\nvariable \u0026#34;avd_register_session_host_modules_url\u0026#34; { type = string description = \u0026#34;URL to .zip file containing DSC configuration to register AVD session hosts to AVD host pool.\u0026#34; default = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_02-23-2022.zip\u0026#34; } Then, we register the session hosts to the host pool with the DSC VM extension:\nresource \u0026#34;azurerm_virtual_machine_extension\u0026#34; \u0026#34;avd_register_session_host\u0026#34; { count = var.avd_host_pool_size name = \u0026#34;register-session-host-vmext\u0026#34; virtual_machine_id = azurerm_windows_virtual_machine.avd[count.index].id publisher = \u0026#34;Microsoft.Powershell\u0026#34; type = \u0026#34;DSC\u0026#34; type_handler_version = \u0026#34;2.73\u0026#34; settings = \u0026lt;\u0026lt;-SETTINGS { \u0026#34;modulesUrl\u0026#34;: \u0026#34;${var.avd_register_session_host_modules_url}\u0026#34;, \u0026#34;configurationFunction\u0026#34;: \u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;hostPoolName\u0026#34;: \u0026#34;${azurerm_virtual_desktop_host_pool.avd.name}\u0026#34;, \u0026#34;aadJoin\u0026#34;: false } } SETTINGS protected_settings = \u0026lt;\u0026lt;-PROTECTED_SETTINGS { \u0026#34;properties\u0026#34;: { \u0026#34;registrationInfoToken\u0026#34;: \u0026#34;${azurerm_virtual_desktop_host_pool_registration_info.avd.token}\u0026#34; } } PROTECTED_SETTINGS lifecycle { ignore_changes = [settings, protected_settings] } depends_on = [azurerm_virtual_machine_extension.avd_aadds_join] } Again, we ignore_changes to the settings and protected_settings properties.\nRole-based Access Control (RBAC)# Let\u0026rsquo;s create a group in AAD that authorizes its members to access the AVD application group we created earlier. To do so, we create a group and assign the AAD built-in Desktop Virtualization User role to it:\ndata \u0026#34;azurerm_role_definition\u0026#34; \u0026#34;desktop_virtualization_user\u0026#34; { name = \u0026#34;Desktop Virtualization User\u0026#34; } resource \u0026#34;azuread_group\u0026#34; \u0026#34;avd_users\u0026#34; { display_name = \u0026#34;AVD Users\u0026#34; security_enabled = true } resource \u0026#34;azurerm_role_assignment\u0026#34; \u0026#34;avd_users_desktop_virtualization_user\u0026#34; { scope = azurerm_virtual_desktop_application_group.avd.id role_definition_id = data.azurerm_role_definition.desktop_virtualization_user.id principal_id = azuread_group.avd_users.id } Assuming that we want to authorize users that already exist within our AAD, we create a variable containing the UPNs of these users:\nvariable \u0026#34;avd_user_upns\u0026#34; { type = list(string) description = \u0026#34;List of user UPNs authorized to access AVD.\u0026#34; default = [] } We are able then query those users with Terraform and add them to the group like this:\ndata \u0026#34;azuread_user\u0026#34; \u0026#34;avd_users\u0026#34; { for_each = toset(var.avd_user_upns) user_principal_name = each.key } resource \u0026#34;azuread_group_member\u0026#34; \u0026#34;avd_users\u0026#34; { for_each = data.azuread_user.avd_users group_object_id = azuread_group.avd_users.id member_object_id = each.value.id } What\u0026rsquo;s Next?# Great! We successfully created an AVD environment with Terraform. Test it by logging into one of the available AVD clients.\nI\u0026rsquo;ll write about creating custom AVD images with Packer next and follow it up by showing you how to configure FSLogix user profiles on your AADDS-joined AVD session hosts. Stay tuned!\n","date":"2022-02-27","id":4,"permalink":"/blog/deploy-azure-virtual-desktop-avd-using-terraform-and-azure-active-directory-domain-services-aadds/","summary":"With Azure Virtual Desktop (AVD), you can deliver secure Windows 11 desktops and environments anywhere. It\u0026rsquo;s pretty easy to deploy and scale. You can provide a coherent user experience from any end-user device and reduce costs by leveraging Windows 11 multi-session licensing. This tutorial will guide you through setting up AVD with AADDS using Terraform.\n","tags":["AADDS","AVD","Azure","Azure Active Directory Domain Services","Azure Virtual Desktop","IaC","Infrastructure as Code","Terraform"],"title":"Deploy Azure Virtual Desktop (AVD) Using Terraform and Azure Active Directory Domain Services (AADDS)"},{"content":"I wanted to revisit this topic for a while because the previous guide I wrote about setting up Azure Active Directory Domain Services (AADDS) with Terraform is outdated. However, the article still attracts around 100 visitors per month. People also keep downloading the deprecated Terraform module I created. Time to set things right!\nWith v2.69.0 of the official Terraform azurerm provider released, the active_directory_domain_service resource is now available. In this post, I\u0026rsquo;ll briefly walk you through the required steps of setting up AADDS. See also the official Microsoft documentation for more details.\nI also published the code to a sample GitHub repo.\nWhat Are Azure Active Directory Domain Services?# Bringing traditional Active Directory Domain Services (AD DS) to the cloud, typically required to set up, secure, and maintain domain controllers (DCs). Azure Active Directory Domain Services (AADDS or Azure AD DS) is a Microsoft-managed solution, providing a subset of traditional AD DS features without the need to self-manage DCs. One such service that requires AD DS features is Azure Virtual Desktop (AVD).\nPrerequisites# Before getting started, you need the following things:\nActive Azure subscription Azure Active Directory (Azure AD / AAD) tenant Service Principal# First, create the service principal for the Domain Controller Services published application. In public Azure, the ID is 2565bd9d-da50-47d4-8b85-4c97f669dc36. For other clouds the value is 6ba9a5d4-8456-4118-b521-9c5ca10cdf84.\nresource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;aadds\u0026#34; { application_id = \u0026#34;2565bd9d-da50-47d4-8b85-4c97f669dc36\u0026#34; } If the service principal already exists, the following error occurs:\nError: A resource with the ID \u0026#34;11111111-1111-1111-1111-111111111111\u0026#34; already exists with azuread_service_principal.aadds, on aadds.tf line 2, in resource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;aadds\u0026#34;: 2: resource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;aadds\u0026#34; { To be managed via Terraform, this resource needs to be imported into the State. Please see the resource documentation for \u0026#34;azuread_service_principal\u0026#34; for more information. Import the service principal with the following command:\nterraform import azuread_service_principal.aadds 11111111-1111-1111-1111-111111111111 Note that 11111111-1111-1111-1111-111111111111 is the Object ID and not the Application ID.\nMicrosoft.AAD Resource Provider Registration# To use AADDS, register the Microsoft.AAD resource provider:\nresource \u0026#34;azurerm_resource_provider_registration\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;Microsoft.AAD\u0026#34; } If the provider is already registered, you can import it into Terraform with the following command:\nterraform import azurerm_resource_provider_registration.aadds /subscriptions/00000000-0000-0000-0000-000000000000/providers/Microsoft.AAD DC Admin Group and User# Next, create an Azure AD group for users administering the AADDS domain and add an admin.\nresource \u0026#34;azuread_group\u0026#34; \u0026#34;dc_admins\u0026#34; { display_name = \u0026#34;AAD DC Administrators\u0026#34; description = \u0026#34;AADDS Administrators\u0026#34; members = [azuread_user.dc_admin.object_id] security_enabled = true } resource \u0026#34;random_password\u0026#34; \u0026#34;dc_admin\u0026#34; { length = 64 } resource \u0026#34;azuread_user\u0026#34; \u0026#34;dc_admin\u0026#34; { user_principal_name = \u0026#34;dc-admin@example.com\u0026#34; display_name = \u0026#34;AADDS DC Administrator\u0026#34; password = random_password.dc_admin.result } Resource Group# Add the resource group for AADDS resources:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } Network Resources# Add the virtual network and subnet next.\nresource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-vnet\u0026#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name address_space = [\u0026#34;10.0.0.0/16\u0026#34;] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-snet\u0026#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [\u0026#34;10.0.0.0/24\u0026#34;] } To lock down access to the managed domain, add the following network security group. The AllowRD and AllowPSRemoting rules allow the Azure platform to monitor, manage, and update the managed domain:\nresource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-nsg\u0026#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name security_rule { name = \u0026#34;AllowRD\u0026#34; priority = 201 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;3389\u0026#34; source_address_prefix = \u0026#34;CorpNetSaw\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } security_rule { name = \u0026#34;AllowPSRemoting\u0026#34; priority = 301 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;5986\u0026#34; source_address_prefix = \u0026#34;AzureActiveDirectoryDomainServices\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } resource azurerm_subnet_network_security_group_association \u0026#34;aadds\u0026#34; { subnet_id = azurerm_subnet.aadds.id network_security_group_id = azurerm_network_security_group.aadds.id } When you enable secure LDAP, it\u0026rsquo;s recommended to create an additional security rule to restrict inbound LDAPS access to specific IP addresses to protect the managed domain from brute force attacks:\nsecurity_rule { name = \u0026#34;AllowLDAPS\u0026#34; priority = 401 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;636\u0026#34; source_address_prefix = \u0026#34;\u0026lt;Authorized LDAPS IPs\u0026gt;\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } AADDS Managed Domain# Finally, deploy the AADDS managed domain:\nresource \u0026#34;azurerm_active_directory_domain_service\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds\u0026#34; location = azurerm_resource_group.aadds.location resource_group_name = azurerm_resource_group.aadds.name domain_name = \u0026#34;aadds.example.com\u0026#34; sku = \u0026#34;Standard\u0026#34; initial_replica_set { subnet_id = azurerm_subnet.aadds.id } notifications { additional_recipients = [\u0026#34;alice@example.com\u0026#34;, \u0026#34;bob@example.com\u0026#34;] notify_dc_admins = true notify_global_admins = true } security { sync_kerberos_passwords = true sync_ntlm_passwords = true sync_on_prem_passwords = true } depends_on = [ azuread_service_principal.aadds, azurerm_resource_provider_registration.aadds, azurerm_subnet_network_security_group_association.aadds, ] } Run terraform apply to deploy everything. It takes an hour to complete. Please share your thoughts with me in the comments or on Twitter!\n","date":"2022-02-20","id":5,"permalink":"/blog/set-up-azure-active-directory-domain-services-aadds-with-terraform-updated/","summary":"I wanted to revisit this topic for a while because the previous guide I wrote about setting up Azure Active Directory Domain Services (AADDS) with Terraform is outdated. However, the article still attracts around 100 visitors per month. People also keep downloading the deprecated Terraform module I created. Time to set things right!\n","tags":["AADDS","ARM","Azure","Azure Active Directory Domain Services","Cloud","IaC","Infrastructure As Code","Terraform"],"title":"Set Up Azure Active Directory Domain Services (AADDS) With Terraform"},{"content":"I recently tried to percent-encode (or URL-encode) strings in Hugo. Percent-encoding is used to encode data in query strings of URLs because they must only contain ASCII characters. There is a Hugo built-in querify function to transform key-value pairs to query strings. But I required to URL-encode a single string value and not build a query string from key-value pairs.\nMy goal is to allow users of the Hugo theme to add social media sharing buttons through the site configuration like this:\n[params] [[params.socialShare]] formatString = \u0026#34;https://www.facebook.com/sharer.php?u={url}\u0026#34; [[params.socialShare]] formatString = \u0026#34;https://reddit.com/submit?url={url}\u0026amp;title={title}\u0026#34; The formatString contains {url} and {title} placeholders. The following is a simplified sharing button implementation using the format strings where placeholders are substituted with page-level variable values during Hugo build-time:\n{{ $href := .formatString }} {{ $href := replace $href \u0026#34;{url}\u0026#34; .Permalink }} {{ $href := replace $href \u0026#34;{title}\u0026#34; .Title }} \u0026lt;a href=\u0026#34;{{ $href | safeURL }}\u0026#34;\u0026gt;Share Me!\u0026lt;/a\u0026gt; The issue here is that .Permalink and .Title must be percent-encoded. As I\u0026rsquo;ve searched the web on how to do this with Hugo, I only found unsolved forum posts:\nAbout htmlEscape: transform https into https%3A%2F%2F URL encoding (percent encoding) with Hugo? Is there a url encoder function in Hugo? I couldn\u0026rsquo;t believe this was an unsolved mystery and assumed I was just too dumb to read the docs. Hugo was written in Go, so I looked into how to do percent-encoding in Go: with the url.QueryEscape() function from the net/url package.\nSearching the Hugo code base for url.QueryEscape leads to exactly one result. It turns out I discovered an undocumented, built-in Hugo function called urlquery. The following:\n{{ urlquery \u0026#34;https://schnerring.net\u0026#34; }} \u0026hellip;returns the result: https%3A%2F%2Fschnerring.net. Here is the fixed version of the sharing button:\n{{ $href := .formatString }} {{ $href := replace $href \u0026#34;{url}\u0026#34; (urlquery .Permalink) }} {{ $href := replace $href \u0026#34;{title}\u0026#34; (urlquery .Title) }} \u0026lt;a href=\u0026#34;{{ $href | safeURL }}\u0026#34;\u0026gt;Share Me!\u0026lt;/a\u0026gt; The full social sharing button implementation is available on my GitHub.\n","date":"2022-01-16","id":6,"permalink":"/blog/percent-encoding-with-the-hugo-urlquery-function/","summary":"I recently tried to percent-encode (or URL-encode) strings in Hugo. Percent-encoding is used to encode data in query strings of URLs because they must only contain ASCII characters. There is a Hugo built-in querify function to transform key-value pairs to query strings. But I required to URL-encode a single string value and not build a query string from key-value pairs.\n","tags":["hugo"],"title":"Percent-encoding with the Hugo `urlquery` Function"},{"content":"I use the Azure Kubernetes Service (AKS) to host a few services, like Synapse, Remark42, and Plausible Analytics. As of yet, none of these services require much computing power, so I chose a small VM size for the AKS node: Standard_B2ms. I recently analyzed my Azure costs and found out that I could have saved more than a hundred bucks in the past months if I had been more diligent.\nWhat Storage Does AKS Use?# Besides the underlying storage of Persistent Volumes Kubernetes deployments use, there\u0026rsquo;s also the node itself requiring an OS disk. For OS disks, two types exist: managed and ephemeral. The Microsoft docs note the following on ephemeral OS disks:\nBy default, Azure automatically replicates the operating system disk for an virtual machine to Azure storage to avoid data loss should the VM need to be relocated to another host. However, since containers aren\u0026rsquo;t designed to have local state persisted, this behavior offers limited value while providing some drawbacks, including slower node provisioning and higher read/write latency.\nBy contrast, ephemeral OS disks are stored only on the host machine, just like a temporary disk. This provides lower read/write latency, along with faster node scaling and cluster upgrades.\nLike the temporary disk, an ephemeral OS disk is included in the price of the virtual machine, so you incur no additional storage costs.\nI created my cluster in 2021, and ephemeral OS disks have been generally available since November 2020 and B-series VMs support ephemeral OS disks. However, the cost analysis in the Azure Portal shows that I\u0026rsquo;m incurring storage costs for my cluster\u0026rsquo;s managed OS disks. The following screenshot shows that I paid CHF 24.43 in October 2021.\nI pay CHF 33.50 per month for the Standard_B2ms VM instance, which the screenshot doesn\u0026rsquo;t correctly display. To save around 70% compared to pay-as-you-go prices, I use Azure Reserved Virtual Machine Instances, which Microsoft bills separately. So why am I paying for a managed OS disk that almost doubles the operating cost of my AKS cluster? Well, because I made a mistake.\nUsing Terraform to Deploy AKS# I use Terraform to manage my AKS cluster. The azurerm_kubernetes_cluster Terraform resource features the os_disk_type option:\n(Optional) The type of disk which should be used for the Operating System. Possible values are Ephemeral and Managed. Defaults to Managed. Changing this forces a new resource to be created.\nHere it is! I didn\u0026rsquo;t explicitly set it to Ephemeral when I initially configured my cluster over six months ago. An expensive mistake!\nUpdate on Ephemeral OS Disk Support for B-series VMs# As someone correctly pointed out in the comments, B-series VMs don\u0026rsquo;t support ephemeral OS disks. My tests with the Azure CLI and Terraform confirm his findings. The information from the official Microsoft documentation on this is inconsistent. Someone on GitHub mentioned a query from the Microsoft docs to determine VM types supporting ephemeral OS disks. Interestingly, the query result suggests B-series VMs support ephemeral OS storage.\n$vmSizes = Get-AzComputeResourceSku | where { $_.ResourceType -eq \u0026#39;virtualMachines\u0026#39; -and $_.Locations.Contains(\u0026#39;CentralUSEUAP\u0026#39;) } foreach ($vmSize in $vmSizes) { foreach ($capability in $vmSize.capabilities) { if ($capability.Name -eq \u0026#39;EphemeralOSDiskSupported\u0026#39; -and $capability.Value -eq \u0026#39;true\u0026#39;) { $vmSize } } } Due to these limitations with B-series VMs, right now, it\u0026rsquo;s more efficient to run general-purpose D-series VMs instead.\n","date":"2021-11-29","id":7,"permalink":"/blog/reduce-storage-costs-when-deploying-azure-kubernetes-service-clusters-with-terraform/","summary":"I use the Azure Kubernetes Service (AKS) to host a few services, like Synapse, Remark42, and Plausible Analytics. As of yet, none of these services require much computing power, so I chose a small VM size for the AKS node: Standard_B2ms. I recently analyzed my Azure costs and found out that I could have saved more than a hundred bucks in the past months if I had been more diligent.\n","tags":["AKS","Azure","Azure Kubernetes Service","Cloud","Storage","Terraform"],"title":"Reduce Storage Costs when Deploying Azure Kubernetes Service Clusters with Terraform"},{"content":"Update 2023-07-28# In May 2023, Mullvad announced the discontinuation of port forwarding support. Starting from 2023-07-01, all existing ports will be removed, making this guide obsolete. üò≠\nIn this quick guide, I\u0026rsquo;ll show you how to use Mullvad port forwarding and OPNsense to create a WireGuard VPN \u0026ldquo;tunnel-inside-a-tunnel\u0026rdquo; configuration, to be able to connect to your home network from the outside. It\u0026rsquo;s pretty nifty because you won\u0026rsquo;t have to expose your public IP address. This time, I\u0026rsquo;ll give you more of a high-level overview and reference the relevant documentation instead of a detailed step-by-step guide.\n\u0026ldquo;Outer\u0026rdquo; WireGuard Tunnel# I won\u0026rsquo;t cover the configuration steps of the \u0026ldquo;outer\u0026rdquo; tunnel leading from your OPNsense router to a Mullvad VPN server in this post. I already cover this topic in-depth in my OPNsense baseline guide.\n\u0026ldquo;Inner\u0026rdquo; WireGuard Tunnel# Configuring the \u0026ldquo;inner\u0026rdquo; tunnel is also covered by the WireGuard Road Warrior Setup guide from the official OPNsense documentation. Make sure to set it up and verify it\u0026rsquo;s working before you continue.\nWhy Mullvad Port Forwarding?# While you set up the inner tunnel, you might have noticed that the external road warrior clients need to know the public IP of your OPNsense host. If you have a residential internet subscription it\u0026rsquo;s likely your ISP provides you with a dynamic IP address. So every time your public IP address changes, you need to update the VPN client configurations. A typical solution for this issue is to use Dynamic DNS (DDNS). You can configure a DDNS client to monitor your dynamic IP address. And as soon it changes, it associates the IP address with a public DNS record, e.g., vpn.example.com. You can then use the DNS hostname in your WireGuard client configurations instead of an explicit IP. But what if you don\u0026rsquo;t want to expose your \u0026ldquo;real\u0026rdquo; IP address to the public? Cloudflare proxying comes to mind, but it operates on layer 7, so it doesn\u0026rsquo;t work with WireGuard.\nMullvad port forwarding to the rescue! It allows us to forward any traffic through the \u0026ldquo;outer\u0026rdquo; tunnel. Read the official port forwarding with Mullvad VPN guide to find out how to configure your ports.\nConfigure OPNsense# Only a few steps are needed to configure OPNsense. In the following, I\u0026rsquo;ll assume the following.\nThe interface of the outer WireGuard tunnel is named WAN_VPN1 The randomly generated Mullvad port number is 61234 The WireGuard local peer for external clients listens to port 51888 First, we allow inbound traffic for the Mullvad port on the WireGuard interface of the outer tunnel. Navigate to Firewall ‚Üí Rules ‚Üí WAN_VPN1 and add the following rule.\nAction Pass Interface WAN_VPN1 Protocol UDP Source any Destination WAN_VPN1 address Destination port range from 61234 to 61234 Description Allow Mullvad port forward Secondly, we redirect the traffic to the WireGuard local peer for external clients. Navigate to Firewall ‚Üí NAT ‚Üí Port Forward and add the following rule.\nInterface WAN_VPN1 Protocol UDP Source any Destination WAN_VPN1 address Destination port range from 61234 to 61234 Redirect target IP 127.0.0.1 Redirect target port 51888 Description Redirect Mullvad port forward to WireGuard Optionally, you can configure a public DNS record pointing to the exit IP of your Mullvad VPN tunnel.\nAnd that\u0026rsquo;s it for this little guide. Let me know what you think on Twitter or in the comments below!\n","date":"2021-11-28","id":8,"permalink":"/blog/use-mullvad-port-forwarding-to-connect-to-your-opnsense-home-network-with-wireguard/","summary":"Update 2023-07-28# In May 2023, Mullvad announced the discontinuation of port forwarding support. Starting from 2023-07-01, all existing ports will be removed, making this guide obsolete. üò≠\n","tags":["Firewall","Mullvad","Network","OPNsense","VPN","WireGuard"],"title":"Use Mullvad Port Forwarding to Connect to Your OPNsense Home Network with WireGuard"},{"content":"For storage in my homelab, I use TrueNAS. Additionally, I run a couple of apps on top of it as jails. For over a year, I\u0026rsquo;ve been using an uninterruptible power supply (UPS) to protect my TrueNAS from possible data loss in case of a power failure. What I\u0026rsquo;ve been missing throughout that time are the monitoring and management tools to shut down everything gracefully when the battery of the UPS runs low. In the event of a power outage lasting longer than 30 minutes, the battery would run out of juice. Everything attached to the UPS would be powered off immediately, and data loss might occur. Luckily I live in an area where power outages rarely happen. I also have backups I could restore if my TrueNAS data gets corrupted. Still, doing this right and configuring Network UPS Tools (NUT) to orchestrate shutdowns has been on my to-do list for way too long. It\u0026rsquo;s time to tackle the issue!\nAs always, I\u0026rsquo;ll only mention settings deviating from the defaults.\nRequirements# My server rack contains the following hardware.\nOPNsense firewall and router TrueNAS storage and jail apps Mikrotik CRS328-24P-4S+RM core switch running SwOS CyberPower PR750ERT2U UPS ISP modem Of the above, only OPNsense and TrueNAS are susceptible to data corruption in case of power loss. If a shutdown due to low battery power is required, I want to shut down TrueNAS first. After, OPNsense shuts down. To achieve this with NUT, I configure OPNsense as NUT master and TrueNAS as NUT slave. Only one master must exist. However, multiple slaves may exist.\nCyberPower PR750ERT2U# The CyberPower PR750ERT2U has about everything I\u0026rsquo;d ever want from a UPS.\nHID-compliant USB port to connect OPNsense Fanless operation in utility mode. It isn\u0026rsquo;t apparent from the specifications, but the CyberPower support kindly provided me with this information 750VA capacity allowing me to add another server Pure sine wave output Line-interactive topology Energy-saving capabilities Initially, I looked at similar used APC units on eBay. At the time, I couldn\u0026rsquo;t find any good deals on eBay meeting all of the requirements above. I then found the CyberPower unit selling new for under $400 and grabbed one. I\u0026rsquo;ve been a happy CyberPower customer ever since. To be clear, I don\u0026rsquo;t affiliate with CyberPower.\nNUT on OPNsense# NUT Service Configuration# To configure the NUT service on OPNsense, we do the following.\nInstall the os-nut plugin under System ‚Üí Firmware ‚Üí Plugins Refresh the browser and enable the USB HID driver under Services ‚Üí Nut ‚Üí Configuration ‚Üí UPS Type ‚Üí USBHID-Driver Reboot OPNsense Connect the UPS to OPNsense via USB Set the Monitor Password and Admin Password under Services ‚Üí Nut ‚Üí Configuration ‚Üí Nut Account Settings Set a Name (e.g., cyberpower) and Enable Nut under Services ‚Üí Nut ‚Üí Configuration ‚Üí General Settings ‚Üí General Nut Settings We should see the data of the UPS under Services ‚Üí Nut ‚Üí Diagnostics.\nbattery.charge: 100 battery.charge.low: 0 battery.charge.warning: 35 battery.mfr.date: CPS battery.runtime: 5020 battery.runtime.low: 300 battery.type: PbAcid battery.voltage: 0.6 battery.voltage.nominal: 22 device.mfr: CPS device.model: PR750ERT2U device.serial: XXXXXXXXXXXX device.type: ups driver.name: usbhid-ups driver.parameter.pollfreq: 30 driver.parameter.pollinterval: 2 driver.parameter.port: auto driver.parameter.synchronous: no driver.version: 2.7.4 driver.version.data: CyberPower HID 0.4 driver.version.internal: 0.41 input.voltage: 235.0 input.voltage.nominal: 230 output.voltage: 253.0 ups.beeper.status: enabled ups.delay.shutdown: 20 ups.delay.start: 30 ups.load: 17 ups.mfr: CPS ups.model: PR750ERT2U ups.productid: 0601 ups.realpower.nominal: 750 ups.serial: XXXXXXXXXXXX ups.status: OL ups.test.result: No test initiated ups.timer.shutdown: 0 ups.timer.start: 0 ups.vendorid: 0764 Port Forward# To allow TrueNAS to communicate with the NUT server on OPNsense, we add the following port forward under Firewall ‚Üí NAT ‚Üí Port Forward. I assume that TrueNAS is part of the LAN network.\nInterface LAN Source TrueNAS IP Destination LAN address Destination port range from 3493 to 3493 Redirect target IP 127.0.0.1 Redirect target port 3493 Description Redirect NUT traffic to OPNsense To see if it works, we SSH into TrueNAS and run the following to retrieve the UPS status from OPNsense.\nupsc cyberpower@192.168.1.1 TrueNAS# Set the following options under Services ‚Üí UPS ‚Üí Configure.\nGeneral Options Identifier The Name we earlier set in OPNsense, e.g., cyberpower UPS Mode Slave Remote Host \u0026lt;IP or hostname of OPNsense\u0026gt; Remote Port 3493 Identifier auto Shutdown Shutdown Mode UPS reaches low battery Shutdown Command /sbin/poweroff Monitor Monitor User monuser Monitor Password The Monitor Password we earlier set in OPNsense Clicking Save returns us to the Services menu. We need to enable the UPS service and check Start Automatically to start the service at boot time.\nTest# We could disconnect the UPS from power and let the battery drain until the UPS reaches low battery status. At some point, it\u0026rsquo;s probably a good idea to do. But repeatedly doing that unnecessarily wears down the battery. Another way is to run the Force Shut Down command. SSH into the NUT master (OPNsense) and run the following.\nupsmon -c fsd If we configured everything correctly, TrueNAS and OPNsense shut down.\n","date":"2021-11-28","id":9,"permalink":"/blog/configure-nut-for-opnsense-and-truenas-with-the-cyberpower-pr750ert2u-ups/","summary":"For storage in my homelab, I use TrueNAS. Additionally, I run a couple of apps on top of it as jails. For over a year, I\u0026rsquo;ve been using an uninterruptible power supply (UPS) to protect my TrueNAS from possible data loss in case of a power failure. What I\u0026rsquo;ve been missing throughout that time are the monitoring and management tools to shut down everything gracefully when the battery of the UPS runs low. In the event of a power outage lasting longer than 30 minutes, the battery would run out of juice. Everything attached to the UPS would be powered off immediately, and data loss might occur. Luckily I live in an area where power outages rarely happen. I also have backups I could restore if my TrueNAS data gets corrupted. Still, doing this right and configuring Network UPS Tools (NUT) to orchestrate shutdowns has been on my to-do list for way too long. It\u0026rsquo;s time to tackle the issue!\n","tags":["CyberPower","Homelab","NUT","OPNsense","Self-host","TrueNAS","UPS"],"title":"Configure NUT for OPNsense and TrueNAS with the CyberPower PR750ERT2U UPS"},{"content":"This beginner-friendly, step-by-step guide walks you through the initial configuration of your OPNsense firewall. The title of this guide is an homage to the pfSense baseline guide with VPN, Guest, and VLAN support that some of you guys might know, and this is an OPNsense migration of it. I found that guide two years ago and immediately fell in love with the network setup. After researching for weeks, I decided to use OPNsense instead of pfSense. I bit the bullet and bought the Deciso DEC630 appliance. Albeit expensive and possibly overkill for my needs, I\u0026rsquo;m happy to support the open-source mission of Deciso, the maintainers of OPNsense. The only thing I regret about the purchase is that I now can\u0026rsquo;t afford the sexier-looking successor model, the DEC690.\nTo configure OPNsense, I followed the instructions of the pfSense guide, taking notes on the differences. Some options moved to different menus or changed. As my notes grew, I decided to publish them as a guide on my website.\nMy goal was to create a comprehensive guide that\u0026rsquo;s easy to follow. But I tried to strike a different balance regarding the brevity of the instructions compared to the pfSense guide. It\u0026rsquo;s a matter of personal taste, but I find the instructions in that guide too verbose. I intentionally omit most of the repetitive \u0026ldquo;click save and apply\u0026rdquo; instructions and only list configuration changes deviating from defaults, making exceptions for important settings. I consider the OPNsense defaults stable enough for this approach in the hope of keeping the effort required to maintain this guide to a minimum.\nI\u0026rsquo;m a homelab hobbyist, so be warned that this guide likely contains errors. Please, verify the steps yourself and do your research. I hope this guide is as helpful and inspiring to you as the pfSense guide was to me. Your feedback is always welcome and very much appreciated.\nOverview# WAN# DHCP WAN from a single Internet Service Provider (ISP) Mullvad VPN multi-WAN with gateway groups LAN# We segregate the local network into several areas with different requirements.\nManagement Network (VLAN 10)# The Management network connects native management interfaces like WiFi access points and IPMI interfaces.\nVPN Network (VLAN 20)# The primary LAN network uses the WireGuard VPN tunnels for outbound connections, maximizing privacy and security. If the VPN tunnels fail, outbound connections won\u0026rsquo;t be possible. Exceptions to selectively route traffic through the ISP WAN gateway are possible.\n\u0026ldquo;Clear\u0026rdquo; Network (VLAN 30)# General-purpose web access network that doesn\u0026rsquo;t use VPN tunnels. All outgoing connections leave through the ISP WAN gateway. It serves as a backup network in case the VPN tunnels fail.\nGuest Network (VLAN 40)# The network that visitors use. It allows unrestricted internet access. Local networks aren\u0026rsquo;t accessible.\nLAN Network# \u0026ldquo;Native\u0026rdquo; VLAN, used to debug and test new configurations.\nDNS Services# We\u0026rsquo;ll configure a DNS resolver (Unbound), as well as a DNS forwarder (Dnsmasq) in OPNsense. Management and VPN networks will use the resolver, the Clear network will use the forwarder, and the Guest network will use Cloudflare as an external DNS resolver. We\u0026rsquo;ll dig into the details later.\nHardware Selection and Installation# The original pfSense guide features a large section of hardware recommendations and installation instructions.\nAs mentioned earlier, I bought the Deciso DEC630 appliance, which is why I\u0026rsquo;m not advising on hardware choices. Have a look at the official hardware sizing \u0026amp; setup guidelines for more information. See also Initial Installation \u0026amp; Configuration.\nI verified this guide with a clean install of OPNsense version 21.7.5.\nWizard# Navigate to 192.168.1.1 in your browser and login with default credentials:\nUsername: root Password: opnsense Click Next to leave the welcome screen and get started with the initial wizard configuration.\nGeneral Information# I prefer using the DNS servers of Quad9 over the ones of my ISP. Only the Clear network will use these anyway, as secured networks use Unbound instead. The Guest network will use Cloudflare DNS servers.\nFor the domain, I prefer to use a subdomain of a domain name I own, like corp.example.com. I only use this subdomain internally. I consider the local.lan pattern a relic of the past. To prevent our local network structure from leaking to the outside world, we\u0026rsquo;ll later configure Unbound and Dnsmasq to treat the domain as private.\nDomain corp.example.com Primary DNS Server 9.9.9.9 Secondary DNS Server 149.112.112.112 Override DNS unchecked Enable DNSSEC Support checked Harden DNSSEC data checked If you prefer using your ISP\u0026rsquo;s DNS servers, leave the Override DNS option checked.\nTime Server Information# Choose the NTP servers geographically closest to your location. I live in Switzerland, which makes the servers from the ch.pool.ntp.org pool the natural choice.\nTime server hostname 0.ch.pool.ntp.org 1.ch.pool.ntp.org 2.ch.pool.ntp.org 3.ch.pool.ntp.org Timezone Europe/Zurich Configure Interfaces# By default, the WAN interface obtains an IP address from your ISP via DHCP. DHCP is also configured for the LAN interface by default and has the IP 192.168.1.1. It works for most people, so we just keep the defaults.\nSet Root Password# Choose a strong root password and complete the wizard.\nGeneral Settings# Access# Navigate to System ‚Üí Settings ‚Üí Administration.\nHTTP Redirect Disable web GUI redirect rule checked Permitting root user login and password login is a quick and dirty way of enabling SSH access, but I strongly discourage you from doing it. They are disabled for security reasons. I highly recommend using certificate- or key-based authentication. If your device has a serial console port, like the Deciso DEC630, enabling SSH is not required.\nSecure Shell Secure Shell Server checked Authentication Sudo Ask password Permit sudo usage for administrators with shell access. Navigate to System ‚Üí Access ‚Üí Users and add a new user.\nUsername \u0026lt;choose a username\u0026gt; Password \u0026lt;choose a secure password\u0026gt; Login shell /bin/csh Group Memberships admins Authorized keys \u0026lt;valid SSH public key\u0026gt; Configuring the SSH client and generating keys is out of scope for this guide, so I\u0026rsquo;ll just recommend this DigitalOcean tutorial covering SSH essentials.\nMiscellaneous# Navigate to System ‚Üí Settings ‚Üí Miscellaneous.\nPower Savings Use PowerD checked Power Mode Hiadaptive Choose Cryptography settings and Thermal Sensors settings compatible with your hardware.\nFirewall Settings# Navigate to Firewall ‚Üí Settings ‚Üí Advanced.\nAlthough IPv6 is something I want to use, it\u0026rsquo;s out of scope for this guide, so we uncheck the following.\nAllow IPv6 unchecked When a rule uses a specific gateway and goes down, a rule gets created, sending traffic to the default gateway. Checking this option skips the creation of this rule.\nGateway Monitoring Skip rules checked Depending on your hardware, you might want to tweak the following settings to improve performance.\nMiscellaneous Firewall Optimization conservative Tries to avoid dropping any legitimate idle connections at the expense of increased memory usage and CPU utilization. Firewall Maximum Table Entries 2000000 default is 1'000'000 We disable the auto-generated anti-lockout rule because we\u0026rsquo;ll define it manually later.\nDisable anti-lockout checked Checksum Offloading# For some hardware, checksum offloading doesn\u0026rsquo;t work, particularly some Realtek cards. Rarely, drivers may have problems with checksum offloading and some specific NICs. If your hardware is incompatible with checksum offloading, disable it.\nNavigate to Interfaces ‚Üí Settings.\nHardware CRC unchecked Disable hardware checksum offload VLANs# Switch Choice# A 802.1Q-capable switch with properly configured VLANs is required. Check my router on a stick VLAN configuration guide to see an example setup with a Mikrotik switch.\nVLAN Definitions# Typically, the LAN port also carries the VLAN traffic and functions as trunk port. For me, the default is the igb0 port. I chose it as the parent interface for all VLANs in the following steps.\nNavigate to Interfaces ‚Üí Other Types ‚Üí VLAN and add the VLANs.\nManagement VLAN# Parent igb0 VLAN tag 10 Description VLAN10_MANAGE VPN VLAN# Parent igb0 VLAN tag 20 Description VLAN20_VPN Clear VLAN# Parent igb0 VLAN tag 30 Description VLAN30_CLEAR Guest VLAN# Parent igb0 VLAN tag 40 Description VLAN40_GUEST VLAN Interfaces# We add an interface for each VLAN. Navigate to Interfaces ‚Üí Assignments.\nSelect vlan 10, enter the description VLAN10_MANAGE, and click + Select vlan 20, enter the description VLAN20_VPN, and click + Select vlan 30, enter the description VLAN30_CLEAR, and click + Select vlan 40, enter the description VLAN40_GUEST, and click + Click Save.\nVLAN Interface IPs# To easier remember which IP range belongs to which VLAN, I like the convention of matching the third octet of the IP with the VLAN ID. I.e., assigning the VLAN with the ID 10 the address 192.168.10.0/24.\nInterface: VLAN10_MANAGE# Select the VLAN10_MANAGE interface.\nEnable Interface checked IPv4 Configuration Type Static IPv4 IPv4 Address 192.168.10.1/24 Click Save.\nInterface: VLAN20_VPN# Enable Interface checked IPv4 Configuration Type Static IPv4 IPv4 Address 192.168.20.1/24 Interface: VLAN30_CLEAR# Enable Interface checked IPv4 Configuration Type Static IPv4 IPv4 Address 192.168.30.1/24 Interface: VLAN40_GUEST# Enable Interface checked IPv4 Configuration Type Static IPv4 IPv4 Address 192.168.40.1/24 VLAN Interface DHCP# We need to configure DHCP for each VLAN we created. I use x.x.x.100-199 for dynamic and x.x.x.10.10-99 for static IP address assignments. You might want to amend these ranges to your requirements.\nNavigate to Services ‚Üí DHCPv4.\nDHCP: VLAN10_MANAGE# Select VLAN10_MANAGE.\nEnable checked Range from 192.168.10.100 to 192.168.10.199 Click Save.\nDHCP: VLAN20_VPN# Enable checked Range from 192.168.20.100 to 192.168.20.199 DHCP: VLAN30_CLEAR# Enable checked Range from 192.168.30.100 to 192.168.30.199 DHCP: VLAN40_GUEST# Enable checked Range from 192.168.40.100 to 192.168.40.199 DNS servers 1.1.1.1 1.0.0.1 DHCP: LAN# Range from 192.168.1.100 to 192.168.1.199 WireGuard VPN with Mullvad# In recent years, Mullvad has been my VPN provider of choice. When That One Privacy Site was still a thing, Mullvad was one of the top recommendations there. After reading the review, I decided to try it out and haven\u0026rsquo;t looked back since. No personally identifiable information is required to register, and paying cash via mail works perfectly.\nI decided to go with WireGuard because I\u0026rsquo;m fine riding the bleeding edge. üòé For more detailed steps, check the official OPNsense documentation on setting up WireGuard with Mullvad and WireGuard selective routing.\nPlease note that the FreeBSD kernel does not (yet) natively support WireGuard, so you must install it as a plugin. Possibly, this doesn\u0026rsquo;t meet your stability, security, or performance requirements.\nNavigate to System ‚Üí Firmware ‚Üí Plugins and install os-wireguard. Refresh the browser and navigate to VPN ‚Üí WireGuard.\nRemote Peers# Select your preferred WireGuard servers from the Mullvad\u0026rsquo;s server list and take note of their names and public keys. It\u0026rsquo;s worth spending some time to benchmark server performance before making a choice.\nSelect the Endpoints tab and click Add. Here is the configuration for the remote ch5-wireguard Mullvad endpoint.\nName mullvad-ch5-wireguard Public Key /iivwlyqWqxQ0BVWmJRhcXIFdJeo0WbHQ/hZwuXaN3g= Allowed IPs 0.0.0.0/0 Endpoint Address 193.32.127.66 Endpoint Port 51820 Keepalive 25 To mitigate risks against DNS poisoning, resolve the server\u0026rsquo;s hostname and enter its IP as Endpoint Address. You can do this by running nslookup ch5-wireguard.mullvad.net in a shell. Make sure to not confuse this address with the SOCKS5 Proxy Address from Mullvad\u0026rsquo;s server list!\nRepeat the steps above to add another server, e.g., ch6-wireguard. Note that all endpoint configurations use the Endpoint Port 51820.\nLocal Peers# Select the Local tab, click Add, and enable the advanced mode.\nName mullvad0 Listen Port 51820 Tunnel Address \u0026lt;LEAVE EMPTY\u0026gt; Peers ch5-wireguard Disable Routes checked Gateway \u0026lt;LEAVE EMPTY\u0026gt; Click Save to generate the WireGuard key pair. Click Edit and copy the generated Public Key.\nNext, run the following shell command to get a Mullvad access token:\naccess_token=$( \\ curl -X \u0026#39;POST\u0026#39; \u0026#39;https://api.mullvad.net/auth/v1/token\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;account_number\u0026#34;: \u0026#34;YOUR MULLVAD ACCOUNT NUMBER\u0026#34; }\u0026#39; \\ | jq -r .access_token) Then run the following command to create a Mullvad Device with DNS hijacking disabled:\ncurl -X POST https://api.mullvad.net/accounts/v1/devices \\ -H \u0026#34;Authorization: Bearer $access_token\u0026#34; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;pubkey\u0026#34;:\u0026#34;YOUR PUBLIC KEY\u0026#34;,\u0026#34;hijack_dns\u0026#34;:false}\u0026#39; I cover the snippet above and Mullvad\u0026rsquo;s DNS hijacking in another post: Use Custom DNS Servers With Mullvad And Any WireGuard Client.\n{ \u0026#34;id\u0026#34;: \u0026#34;d8f07004-4559-4d19-b58b-985b257cd115\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uptown insect\u0026#34;, \u0026#34;pubkey\u0026#34;: \u0026#34;ufO5jCni55uvioHM/eLBgyrrUMocEXsADPc2OvYhF3k=\u0026#34;, \u0026#34;hijack_dns\u0026#34;: false, \u0026#34;created\u0026#34;: \u0026#34;2023-07-30T02:08:41+00:00\u0026#34;, \u0026#34;ipv4_address\u0026#34;: \u0026#34;10.138.30.139/32\u0026#34;, \u0026#34;ipv6_address\u0026#34;: \u0026#34;fc00:bbbb:bbbb:bb01:d:0:a:1e8b/128\u0026#34;, \u0026#34;ports\u0026#34;: [] } Copy the IPv4 IP address to the Tunnel Address field of the Wireguard local peer. Subtract one from the Tunnel Address and enter the result as Gateway IP. E.g., 10.105.248.50 for the example above. It\u0026rsquo;s just a convention I like, but you can use any arbitrary, unused private RFC1918 IP.\nRepeat the steps above to create a second local peer named mullvad1. Remember to use a different Listen Port (e.g., 51821).\nWhen you finish, select the General tab. Check Enable WireGuard. You should see a handshake for the wg0 and wg1 tunnels on the Handshakes tab.\nWireGuard Interfaces# Navigate to Interfaces ‚Üí Assignments.\nSelect wg0, add the description WAN_VPN0, and click + Select wg1, add the description WAN_VPN1, and click + Enable the newly created interfaces and restart the WireGuard service after. It ensures the interfaces get an IP address from WireGuard.\nVPN Gateways# Navigate to System ‚Üí Gateways ‚Üí Single and add the VPN gateways.\nWAN_VPN0# Name WAN_VPN0 Interface WAN_VPN0 Address Family IPv4 IP Address 10.105.248.50 Far Gateway checked Disable Gateway Monitoring unchecked Monitor IP 100.64.0.1 WAN_VPN1# Name WAN_VPN1 Interface WAN_VPN1 Address Family IPv4 IP Address 10.109.231.89 Far Gateway checked Disable Gateway Monitoring unchecked Monitor IP 100.64.0.2 Monitoring IPs# Each VPN gateway requires a unique monitoring IP because setting a monitoring IP installs a static route. Optimally, the monitoring IP should be the least possible amount of hops away from the gateway. For Mullvad specifically, we can \u0026ldquo;abuse\u0026rdquo; the local infrastructure that\u0026rsquo;s available through a Mullvad connection. Any of the following IPs are only one hop away from the tunnel exit.\n100.64.0.1 to 100.64.0.3 are Mullvad\u0026rsquo;s ad-blocking and tracker-blocking DNS service servers 10.64.0.1 is the local Mullvad gateway You can easily verify the above by running traceroute 100.64.0.1 from a host connected to Mullvad.\nAdd Static IPv4 Configuration to the WireGuard Interfaces# OPNsense versions newer than 21.7.3 require adding static IPv4 configuration to the WireGuard interface. Otherwise, Unbound will use the default route despite setting the Outgoing Network Interfaces option. Other solutions exist, but I\u0026rsquo;m not sure which the \u0026ldquo;best\u0026rdquo; or most logical one is. As WireGuard integration matures, this section hopefully becomes obsolete. You can find more information regarding this issue on GitHub.\nNavigate to Interfaces and edit the WireGuard interfaces.\nIP Configuration: WAN_VPN0# IPv4 Configuration Type Static IPv4 IPv4 address 10.105.248.51/32 IPv4 Upstream Gateway WAN_VPN0 - 10.105.248.50 IP Configuration: WAN_VPN1# IPv4 Configuration Type Static IPv4 IPv4 address 10.109.231.90/32 IPv4 Upstream Gateway WAN_VPN1 - 10.109.231.89 Gateway Group# Navigate to System ‚Üí Gateways ‚Üí Group and click Add.\nGroup Name WAN_VPN_GROUP WAN_VPN0 Tier 1 WAN_VPN1 Tier 2 (failover) Trigger Level Packet Loss or High Latency It\u0026rsquo;s also possible to configure load balancing by putting multiple interfaces into the same tier.\nStatic Routes (Optional)# Defining static routes for the tunnel gateways is optional. It would be necessary, for example, if we want to consider the VPN gateways as default gateway candidates. It requires static routes to the ISP WAN gateway to keep the tunnel connections alive.\nNavigate to System ‚Üí Routes ‚Üí Configuration and click Add.\nNetwork Address 193.32.127.66/32 Gateway WAN_DHCP Description Keep tunnels to mullvad-ch5-wireguard alive Network Address 193.32.127.67/32 Gateway WAN_DHCP Description Keep tunnels to mullvad-ch6-wireguard alive DNS# OPNsense includes a DNS resolver (Unbound) and a DNS forwarder (Dnsmasq / Unbound in forwarding mode). Simple setups usually use one of either, but we\u0026rsquo;ll use both. Because we\u0026rsquo;ll also use Unbound and Dnsmasq for internal DNS resolution, we don\u0026rsquo;t want to use them for the Guest network, as this would expose our internal network structure. That\u0026rsquo;s the reason why we earlier configured it to use Cloudflare DNS servers instead.\nLike the name suggests, a DNS forwarder forwards DNS requests to an external DNS resolver of an ISP, Quad9, Cloudflare, or similar service provider. We\u0026rsquo;ll configure the forwarder for the Clear network. In case the primary, secured networks lose connectivity, the Clear network can serve as a backup.\nOne of the advantages of self-hosting a DNS resolver is improved privacy. A resolver iteratively queries a chain of one or more DNS servers to resolve a request, so there isn\u0026rsquo;t a single instance knowing all your DNS requests. It comes at the cost of speed when resolving a hostname for the first time. As Unbound\u0026rsquo;s cache grows, the cost diminishes. We\u0026rsquo;ll configure our primary networks to use Unbound.\nWe\u0026rsquo;ll also keep DNS traffic from Unbound within the VPN tunnels. In the rare case of a VPN outage, we\u0026rsquo;ll want local DNS services to fail and not leak through the ISP WAN. The reason for this isn\u0026rsquo;t improved privacy as you might think. In some cases, this might even hurt your privacy. Why? Either your ISP or your VPN provider will see the iterative DNS requests Unbound sends. So it becomes a question of who you rather entrust with this data. But if there are no privacy benefits, why do it? Honestly, I don\u0026rsquo;t require such a setup. I configured it for educational purposes and fun. Other reasons that don\u0026rsquo;t affect me but other users are:\nISP selling user data ISP enforcing censorship ISP hijacking DNS traffic to redirect it to their DNS resolver; this makes self-hosting a DNS resolver impossible Let\u0026rsquo;s summarize our goals:\nUse a DNS resolver for the management and VPN networks Resolve private domain hostnames for management and VPN networks Prevent DNS leaks from Unbound through the ISP WAN gateway Use DNS forwarding for the Clear network Use external DNS resolvers for the Guest network Resolver (Unbound)# Navigate to Services ‚Üí Unbound DNS ‚Üí General.\nNetwork Interfaces LAN VLAN10_MANAGE VLAN20_VPN DNSSEC checked DHCP registration checked DHCP static mappings checked Local Zone Type static Outgoing Network Interfaces WAN_VPN0 WAN_VPN1 Navigate to Services ‚Üí Unbound DNS ‚Üí Advanced.\nHide Identity checked Hide Version checked Prefetch Support checked Prefetch DNS Key Support checked Harden DNSSEC data checked The final step is to add a custom SOA record to the local zone making Unbound the authoritative name server for corp.example.com. This way, we prevent Unbound from querying external name servers for the internal domain and exposing our network structure to the outside world. For advanced Unbound configuration like this, we use Templates.\nConnect to OPNsense via serial console or SSH and add a +TARGETS file by running sudo vi /usr/local/opnsense/service/templates/OPNsense/Unbound/+TARGETS containing:\nprivate_domains.conf:/usr/local/etc/unbound.opnsense.d/private_domains.conf Add the template file by running sudo vi /usr/local/opnsense/service/templates/OPNsense/Unbound/private_domains.conf containing:\nserver: local-data: \u0026#34;corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600\u0026#34; Here is a translation of what the SOA record means.\nName corp.example.com Record Type SOA Primary Name Server opnsense.corp.example.com Administrator Email root@example.com Serial 2021110201 (YYMMDDnn) Refresh 86400 (24 hours) Retry 7200 (2 hours) Expire 3600000 (1000 hours) TTL 3600 (1 hour) Run the following to verify the configuration.\n# generate template configctl template reload OPNsense/Unbound # show generated file cat /usr/local/etc/unbound.opnsense.d/private_domains.conf # check if configuration is valid configctl unbound check Forwarder (Dnsmasq)# Dnsmasq will forward DNS requests to the configured system DNS servers and 127.0.0.1 (Unbound). Earlier, you either explicitly configured them or decided to receive the DNS servers via DHCP from your ISP. Because Unbound already uses port 53, we\u0026rsquo;ll use port 5335 for Dnsmasq. We\u0026rsquo;ll later create rules to port forward DNS traffic to this port.\nNavigate to Services ‚Üí Dnsmasq DNS ‚Üí Settings.\nEnable checked Listen Port 5335 Do not forward private reverse lookups checked Forward reverse DNS lookups in the 192.168.0.0/16 range to Unbound by adding the following Domain Overrides. We additionally make Unbound the authoritative DNS server for corp.example.com.\nDomain IP Description 168.192.in-addr.arpa 192.168.20.1 Forward reverse lookups of private IP addresses to Unbound corp.example.com 192.168.20.1 Make Unbound the authoritative DNS server for private domain Firewall# Here is an overview of what we want to implement with firewall rules.\nAllow internet access for specific ports through WAN and VPN Allow intranet communications Redirect outbound DNS traffic to either Unbound or Dnsmasq Redirect NTP traffic to OPNsense Block intranet access for the Guest network VLAN10 VLAN20 VLAN30 VLAN40 LAN Internet WAN VPN + selective WAN WAN WAN WAN Intranet pass pass pass block pass ICMP pass pass pass pass pass Anti-lockout yes no no no yes DNS Unbound Unbound Dnsmasq external Unbound NTP local local local external external Interface Groups# We use interface groups to apply policies to multiple interfaces at once and reduce the number of required firewall rules significantly. Do not use them for WAN interfaces because they don\u0026rsquo;t use the reply-to directive!\nI\u0026rsquo;m honestly not sure if I went overboard with interface groups and over-abstracted things. Currently, I\u0026rsquo;m happy with the configuration, and I guess only time will tell how maintainable this approach is. I\u0026rsquo;d like to know what you think and would very much appreciate your feedback.\nNavigate to Firewall ‚Üí Groups and add the following interface groups.\nIG_LOCAL# Name IG_LOCAL Description All local interfaces Members LAN VLAN10_MANAGE VLAN20_VPN VLAN30_CLEAR VLAN40_GUEST IG_OUT_WAN# Name IG_OUT_WAN Description Interfaces allowing outbound WAN traffic Members LAN VLAN10_MANAGE VLAN30_CLEAR VLAN40_GUEST IG_OUT_VPN# Name IG_OUT_VPN Description Interfaces allowing outbound VPN traffic and selective outbound WAN traffic Members VLAN20_VPN IG_DNS_RESOLVE# Name IG_DNS_RESOLVE Description Interfaces forced to use Unbound Members VLAN10_MANAGE VLAN20_VPN IG_DNS_FORWARD# Name IG_DNS_FORWARD Description Interfaces forced to use Dnsmasq Members VLAN30_CLEAR IG_NTP# Name IG_NTP Description Interfaces forced to use OPNsense as NTP server Members VLAN10_MANAGE VLAN20_VPN VLAN30_CLEAR Aliases# We define a few reusable aliases that help us condense our firewall rules. Some of them might become hard to maintain as they grow, in which case you might want to consider nesting aliases.\nNavigate to Firewall ‚Üí Aliases and create the following aliases.\nSelective Routing Addresses# Services like banks might object to traffic originating from known VPN endpoints. We selectively route traffic from the VPN VLAN through the default WAN gateway.\nName SELECTIVE_ROUTING Type Host(s) Description External hosts reachable from IG_OUT_VPN networks through WAN If you\u0026rsquo;re having issues with a service not working due to VPN, add the hostname to this alias, e.g., netflix.com.\nAdmin / Anti-lockout Ports# Name PORTS_ANTI_LOCKOUT Type Port(s) Content 443 (Web GUI) 22 (SSH) Description OPNsense admin ports Ports Allowed To Communicate Between VLANs# Allowed ports for intranet traffic. Amend the list depending on your needs.\nName PORTS_OUT_LAN Type Port(s) Description Ports allowed for intranet Content:\n53 DNS 5353:5354 mDNS 123 NTP 21 FTP 22 SSH 161 SNMP 80 HTTP 8080: HTTP alt / UniFi device and application communication 443 HTTPS 8443 HTTPS alt / UniFi application GUI/API as seen in a web browser 8880 UniFi HTTP portal redirection 10001 UniFi device discovery 5001 iPerf 623 IPMI 5900 VNC 3389 RDP 49152:65535 ephemeral ports Ports Allowed to Communicate with the Internet# Allow ports for egress internet traffic. Amend the list depending on your needs.\nName PORTS_OUT_WAN Type Port(s) Description Ports allowed for internet Content:\n21 FTP 22 SSH 80 HTTP 8080 HTTP alt 443 HTTPS 8443 HTTPS alt 465 SMTPS 587: SMTPS 993: IMAPS 49152:65535 ephemeral ports A Fair Warning about Egress Filtering# As you add applications, you will be constantly amending the PORTS_OUT_WAN list. Depending on the application, the required ports may be poorly documented, so you\u0026rsquo;ll have to figure them out by inspecting the firewall logs. As other users have mentioned in the comments, blocking all egress traffic for all VLANs by default is probably not worth the hassle. Personally, I\u0026rsquo;ve given up on egress filtering altogether because of the administrative overhead that comes with it.\nIt is useful for high-security VLANs connecting devices such as cash registers in a retail store. Another example is VLANs with many untrusted IoT devices that have noisy telemetry. Putting them into a VLAN with egress filtering prevents them from \u0026ldquo;calling home\u0026rdquo;.\nIf you create the alias ALL_PORTS = 1:65535 and add it to the Content field of the PORTS_OUT_WAN alias, you can disable all egress filtering with the option of re-enabling it again later.\nNAT# Network Address Translation (NAT) is required to translate private to public IP addresses. We have the following requirements.\nTranslate IG_OUT_WAN and IG_OUT_VPN network addresses to the WAN address range. Translating IG_OUT_VPN to WAN allows selective routing. Translate IG_OUT_VPN network addresses to the WAN_VPN0 address range. Navigate to Firewall ‚Üí NAT ‚Üí Outbound.\nSelect Manual outbound NAT rule generation and add the following rules.\nIG_OUT_WAN to WAN# Interface WAN Source address IG_OUT_WAN net Description IG_OUT_WAN to WAN IG_OUT_VPN to WAN# Interface WAN Source address IG_OUT_VPN net Description IG_OUT_VPN to WAN IG_OUT_VPN to WAN_VPN0# Interface WAN_VPN0 Source address IG_OUT_VPN net Description IG_OUT_VPN to WAN_VPN0 IG_OUT_VPN to WAN_VPN1# Interface WAN_VPN1 Source address IG_OUT_VPN net Description IG_OUT_VPN to WAN_VPN1 Rules# Navigate to Firewall ‚Üí Rules.\nAnti-Lockout# Before adding any other rules, we add the anti-lockout ones on the VLAN10_MANAGE and LAN networks, so we can\u0026rsquo;t lock ourselves out. üòÖ\nSelect Floating and add the following rule.\nAction Pass Interface LAN VLAN10_MANAGE Protocol TCP/UDP Source any Destination This Firewall Destination port range PORTS_ANTI_LOCKOUT Description Anti-lockout This Firewall is a pre-defined alias representing all interface addresses of OPNsense.\nAllow Intranet Pings# We allow ICMP pings for the entire local network. Pings are maliciously abusable, so you may want to put stricter rules into place if required.\nSelect IG_LOCAL and add the following rule.\nAction Pass Interface IG_LOCAL TCP/IP Version IPv4 Protocol ICMP ICMP type Echo Request Source IG_LOCAL net Description Allow intranet pings Reject Intranet Traffic By Default# By default, we reject traffic on local interfaces instead of blocking it. Block drops packets silently. Reject returns a \u0026ldquo;friendly\u0026rdquo; response to the sender. To be able to override this rule, unchecking Quick is crucial! To use Firewall Logs to review blocked ports and amend our port list alias if necessary, we enable logging on this rule.\nSelect IG_LOCAL and add the following rule.\nAction Reject Quick unchecked Interface IG_LOCAL TCP/IP Version IPv4+IPv6 Protocol any Source IG_LOCAL net Destination IG_LOCAL net Log checked Description Reject intranet traffic by default Allow Intranet Traffic# We only allow intranet traffic on the ports defined in the PORTS_OUT_LAN alias. We\u0026rsquo;ll override this rule for the VLAN40_GUEST network later, so we must uncheck the Quick option again. For the Management network, you might want to consider stricter rules, as well.\nSelect IG_LOCAL and add the following rule.\nAction Pass Quick unchecked Interface IG_LOCAL Protocol TCP/UDP Source IG_LOCAL net Destination IG_LOCAL net Destination port range PORTS_OUT_LAN Description Allow intranet traffic Allow Internet Traffic# We allow internet traffic on PORTS_OUT_WAN for IG_OUT_WAN networks.\nSelect IG_OUT_WAN and add the following rule.\nAction Pass Quick unchecked Interface IG_OUT_WAN Protocol TCP/UDP Source IG_OUT_WAN net Destination / Invert checked Destination IG_LOCAL net Destination port range PORTS_OUT_WAN Description Allow internet traffic through WAN We later want to enable unrestricted internet access on the Guest network, so make sure to uncheck the Quick option!\nNext, we allow internet traffic on PORTS_OUT_WAN for the IG_OUT_VPN networks.\nSelect IG_OUT_VPN and add the following rules to configure selective routing.\nAction Pass Interface IG_OUT_VPN Protocol TCP/UDP Source IG_OUT_VPN net Destination SELECTIVE_ROUTING Destination port range PORTS_OUT_WAN Description Allow selected internet traffic through WAN Action Pass Protocol TCP/UDP Source IG_OUT_VPN net Destination / Invert checked Destination IG_LOCAL net Destination port range PORTS_OUT_WAN Description Allow internet traffic through WAN_VPN0 Gateway WAN_VPN_GROUP Restrict Guest Network# Select VLAN40_GUEST and add the following rules.\nTo block Web GUI and SSH access from the Guest network, we block traffic to any OPNsense interface on the PORTS_ANTI_LOCKOUT ports. We enable logging for this rule to be able to see if any guests try to access OPNsense.\nAction Block Interface VLAN40_GUEST Protocol TCP/UDP Source VLAN40_GUEST net Destination This Firewall Destination port range PORTS_ANTI_LOCKOUT Log checked Description Block admin ports We block access to other local networks and also enable logging for the rule.\nAction Block Interface VLAN40_GUEST Protocol TCP/UDP Source VLAN40_GUEST net Destination IG_LOCAL net Log checked Description Block traffic to local networks Finally, we enable unrestricted internet access on Guest networks.\nAction Pass Interface VLAN40_GUEST Protocol TCP/UDP Source VLAN40_GUEST net Destination / Invert checked Destination IG_LOCAL net Description Unrestricted internet access LAN Network For Testing And Debugging# I just keep the pre-defined \u0026ldquo;LAN to any\u0026rdquo; rules. I periodically reconfigure this network for testing and debugging and don\u0026rsquo;t use it for anything else.\nRedirect Outbound DNS Traffic# To prevent clients from explicitly querying outbound DNS and leaking information to the outside, we redirect any outbound DNS traffic to Unbound or Dnsmasq.\nNavigate to Firewall ‚Üí NAT ‚Üí Port Forward and add the following rules.\nInterface IG_DNS_FORWARD Protocol TCP/UDP Source IG_DNS_FORWARD net Destination any Destination port range DNS Redirect target IP 127.0.0.1 Redirect target port 5335 Description Redirect any DNS traffic to Dnsmasq Interface IG_DNS_RESOLVE Protocol TCP/UDP Source IG_DNS_RESOLVE net Destination / Invert checked Destination IG_DNS_RESOLVE net Destination port range DNS Redirect target IP 127.0.0.1 Redirect target port DNS Description Redirect outbound DNS traffic to Unbound Redirect Outbound NTP Traffic# To sync the time of all our devices on the network to OPNsense, we redirect all NTP traffic.\nNavigate to Firewall ‚Üí NAT ‚Üí Port Forward and add the following rule.\nInterface IG_NTP Protocol UDP Source IG_NTP net Destination / Invert checked Destination IG_NTP net Destination port range NTP Redirect target IP 127.0.0.1 Redirect target port NTP Description Redirect outbound NTP traffic to OPNsense Test# Now would be a could time to reboot OPNsense to make sure all settings are applied.\nTest DHCP# Connect to a host in each VLAN and verify it receives an IP inside the specified DHCP range. Here is the output of the ip -4 addr show eth0 command from a Ubuntu host connected to the VPN VLAN.\n8: eth0: \u0026lt;BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 group default qlen 1 inet 192.168.20.106/24 brd 192.168.20.255 scope global dynamic valid_lft 7196sec preferred_lft 7196sec Test DNS# We have to verify the following functionality of our DNS architecture:\nVLAN20_VPN Unbound resolves remote and local hostname lookups Redirect outbound DNS traffic to Unbound Reverse lookups of private IPs Don\u0026rsquo;t leak lookups for the private corp.example.com domain VL30_CLEAR Dnsmasq forwards remote hostname lookups to the system DNS servers like Quad9 and Unbound Forward local hostname lookups to Unbound Redirect outbound DNS traffic to Dnsmasq Forward local reverse lookups of private IPs to Unbound Don\u0026rsquo;t leak lookups for the private corp.example.com domain and forward them to Unbound VL40_GUEST Use external DNS resolvers Allow for clients to override DNS OPNsense lookups are blocked We\u0026rsquo;ll use the dig tool and the firewall logs under Firewall ‚Üí Log Files ‚Üí Live View for testing.\nI\u0026rsquo;ll also skip the Management network because it requires the same testing as the VPN network.\nVLAN20_VPN: Test DNS# Connect to VLAN20_VPN.\nVLAN20_VPN: Remote Hostname Lookups# Run dig california.gov:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; california.gov ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 41004 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;california.gov. IN A ;; ANSWER SECTION: california.gov. 300 IN A 63.196.102.29 ;; Query time: 36 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 22:37:34 CET 2021 ;; MSG SIZE rcvd: 59 Here are the firewall logs showing the iterative DNS requests Unbound sends.\nVLAN20_VPN: Local Hostname Lookups# Run dig opnsense.corp.example.com:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; opnsense.corp.example.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 22291 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.corp.example.com. IN A ;; ANSWER SECTION: opnsense.corp.example.com. 3600 IN A 192.168.1.1 opnsense.corp.example.com. 3600 IN A 192.168.10.1 opnsense.corp.example.com. 3600 IN A 192.168.20.1 ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:48:19 CET 2021 ;; MSG SIZE rcvd: 105 VLAN20_VPN: Redirect Outbound DNS Traffic# Run dig opnsense.org @8.8.8.8:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; opnsense.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 17970 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 184 IN A 178.162.131.118 ;; Query time: 0 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Tue Nov 16 21:51:15 CET 2021 ;; MSG SIZE rcvd: 57 dig can\u0026rsquo;t tell that OPNsense hijacked the request and thus displays an incorrect SERVER value. If you check the firewall logs, you shouldn\u0026rsquo;t see any requests to 8.8.8.8. Instead, you should see iterative root server requests.\nVLAN20_VPN: Reverse Lookups of Private IPs# Run dig -x 192.168.20.1:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; -x 192.168.20.1 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 9264 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;1.20.168.192.in-addr.arpa. IN PTR ;; ANSWER SECTION: 1.20.168.192.in-addr.arpa. 3600 IN PTR OPNsense.corp.example.com. ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:56:14 CET 2021 ;; MSG SIZE rcvd: 96 If you want, additionally reverse-lookup an IP that doesn\u0026rsquo;t exist. The firewall logs mustn\u0026rsquo;t contain requests to external DNS servers.\nVLAN20_VPN: Verify corp.example.com Is Private# To test whether OPNsense is the authoritative server for corp.example.com, we lookup a non-existent hostname in that domain. dig should return an authoritative NXDOMAIN response with the SOA record we earlier defined earlier in the AUTHORITY SECTION.\nRun dig nowhere.corp.example.com:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; nowhere.corp.example.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NXDOMAIN, id: 44590 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;nowhere.corp.example.com. IN A ;; AUTHORITY SECTION: corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600 ;; Query time: 0 msec ;; SERVER: 192.168.20.1#53(192.168.20.1) ;; WHEN: Tue Nov 16 21:59:58 CET 2021 ;; MSG SIZE rcvd: 110 VLAN20_VPN: DNS Leak Test# In your browser, navigate to dnsleaktest.com or mullvad.net/check. We expect the \u0026ldquo;leaked\u0026rdquo; DNS server to match our Mullvad public Mullvad IP. The second leak is from the Outgoing Interface we configured for Unbound:\nVLAN30_CLEAR: Test DNS# Connect to VLAN30_CLEAR.\nVLAN30_CLEAR: Remote Hostname Lookups# Run dig opnsense.org:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; opnsense.org ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 65053 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 596 IN A 178.162.131.118 ;; Query time: 5 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Tue Nov 16 16:45:08 CET 2021 ;; MSG SIZE rcvd: 57 Check the firewall logs. Enable logging for the port forward rule if you want it to show up.\nYou can see that Dnsmasq forwards to the DNS servers defined under System ‚Üí Settings ‚Üí General and Unbound.\nVLAN30_CLEAR: Local Hostname Lookups# Run dig opnsense.corp.example.com:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; opnsense.corp.example.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 61385 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;opnsense.corp.example.com. IN A ;; ANSWER SECTION: opnsense.corp.example.com. 1 IN A 192.168.1.1 ;; Query time: 0 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Wed Nov 17 00:45:49 CET 2021 ;; MSG SIZE rcvd: 73 VLAN30_CLEAR: Redirect Outbound DNS Traffic# Run dig opnsense.org @8.8.8.8:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; opnsense.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 34638 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;opnsense.org. IN A ;; ANSWER SECTION: opnsense.org. 430 IN A 178.162.131.118 ;; Query time: 3 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Tue Nov 16 00:12:15 CET 2021 ;; MSG SIZE rcvd: 57 We confirm it works by looking at the firewall logs again:\nVLAN30_CLEAR: Forward Reverse Lookups of Private IPs to Unbound# Run dig -x 192.168.20.1:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; -x 192.168.20.1 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 20607 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;1.20.168.192.in-addr.arpa. IN PTR ;; ANSWER SECTION: 1.20.168.192.in-addr.arpa. 3600 IN PTR OPNsense.home.schnerring.net. ;; Query time: 1 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Wed Nov 17 00:09:05 CET 2021 ;; MSG SIZE rcvd: 96 The firewall logs confirm it works.\nVLAN30_CLEAR: Verify corp.example.com Is Private# Run dig nowhere.corp.example.com:\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; nowhere.corp.example.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NXDOMAIN, id: 7481 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;nowhere.corp.example.com. IN A ;; AUTHORITY SECTION: corp.example.com. 3600 IN SOA opnsense.corp.example.com. root.example.com. 2021110201 86400 7200 3600000 3600 ;; Query time: 0 msec ;; SERVER: 192.168.30.1#53(192.168.30.1) ;; WHEN: Tue Nov 16 20:44:35 CET 2021 ;; MSG SIZE rcvd: 112 This time, requests will only be forwarded to Unbound, but not external DNS resolvers.\nVLAN30_CLEAR: DNS Leak Test# As we saw earlier, we expect the Quad9 and the Mullvad public IPs to leak. Here is the result of an extended test from dnsleaktest.com:\nVLAN40_GUEST: Test DNS# Connect to VLAN40_GUEST.\nVerify that dig opnsense.org @192.168.40.1 times out.\nThe Cloudflare DNS servers you configured in the DHCP settings of the Guest VLAN should show up when running the leak test:\nThanks For Reading ‚ù§Ô∏è# If you\u0026rsquo;re here, I thank you for reading all this! Any feedback is highly appreciated.\nWhen I decided to write this guide, I didn\u0026rsquo;t think it would take soooo long. I worked on it intensively for the better part of a month. I spent like a week configuring Unbound to use WireGuard tunnels, only to find out that I couldn\u0026rsquo;t get it working due to a bug. But it was all worth it and a fulfilling journey ‚Äî I have learned so much OPNsense and networking. But I\u0026rsquo;m also happy to be able to put this aside for a while. üéâ\nSo what\u0026rsquo;s next?\nLet\u0026rsquo;s Encrypt certificates and HAProxy to secure self-hosted services. I imagine configuring it should be pretty straightforward.\nMe and possibly others want to be able to access my home network from the outside via WireGuard. I have a dynamic IP, so I thought I\u0026rsquo;d have to resort to Dynamic DNS. But I think port forwarding with Mullvad is the better solution and doesn\u0026rsquo;t require me to associate my public IP address with a public DNS record.\nTraffic shaping and intrusion prevention is something I want to look into, too.\nSo yeah, OPNsense and I will be friends for a while. üë´\n","date":"2021-11-17","id":10,"permalink":"/blog/opnsense-baseline-guide-with-vpn-guest-and-vlan-support/","summary":"This beginner-friendly, step-by-step guide walks you through the initial configuration of your OPNsense firewall. The title of this guide is an homage to the pfSense baseline guide with VPN, Guest, and VLAN support that some of you guys might know, and this is an OPNsense migration of it. I found that guide two years ago and immediately fell in love with the network setup. After researching for weeks, I decided to use OPNsense instead of pfSense. I bit the bullet and bought the Deciso DEC630 appliance. Albeit expensive and possibly overkill for my needs, I\u0026rsquo;m happy to support the open-source mission of Deciso, the maintainers of OPNsense. The only thing I regret about the purchase is that I now can\u0026rsquo;t afford the sexier-looking successor model, the DEC690.\n","tags":["DNS","Dnsmasq","Firewall","Homelab","Let's Encrypt","Mullvad","Network","OPNsense","Router","Self-host","Unbound","VLAN","VPN","WireGuard"],"title":"OPNsense Baseline Guide with Mullvad VPN Multi-WAN, Guest, and VLAN Support"},{"content":"This post documents the steps required to install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex in TrueNAS jails version 12.0-U6.\nThe FN11.3 iocage jails - Plex, Tautulli, Sonarr, Radarr, Lidarr, Jackett, Transmission, Organizr guide inspired me to write this guide. I\u0026rsquo;ll also briefly cover permissions.\nDisclaimer# From Wikipedia on BitTorrent Legislation:\nAlthough the protocol itself is legal, problems stem from using the protocol to traffic copyright infringing works, since BitTorrent is often used to download otherwise paid content, such as movies and video games.\nSee also: Legal issues with BitTorrent (Wikipedia)\nI\u0026rsquo;ll only cover options that deviate from the defaults. I use DHCP reservations to manage my server IPs, so I use the dhcp=1 option to create jails.\nNote that using hardlinks with the *arrs doesn\u0026rsquo;t work with this setup. For hardlinking to work, the torrent client and *arrs must share the same dataset and jail mount points.\nGoals# The diagram above translates to the following requirements:\nEach service lives inside a separate jail Each service inside the jail runs as a different user Each jail owns a dataset for configuration data Jails share media datasets, but only one user has write permissions. E.g., Radarr can only read (ro) from the torrent dataset but write (rw) to the movies dataset. Groups# Navigate to Accounts ‚Üí Groups and add the following groups.\nGID 850 Name qbittorrent Samba Authentication unchecked GID 354 Name jackett Samba Authentication unchecked GID 356 Name lidarr Samba Authentication unchecked GID 352 Name radarr Samba Authentication unchecked GID 351 Name sonarr Samba Authentication unchecked GID 972 Name plex Samba Authentication unchecked Users# Navigate to Accounts ‚Üí Users and add the following users.\nFull Name qBittorrent Username qbittorrent User ID 850 New Primary Group unchecked Primary Group qbittorrent Primary Group qbittorrent Disable Password Yes Shell nologin Samba Authentication unchecked Full Name Jackett Username jackett User ID 354 New Primary Group unchecked Primary Group jackett Disable Password Yes Shell nologin Samba Authentication unchecked Full Name Lidarr Username lidarr User ID 356 New Primary Group unchecked Primary Group lidarr Disable Password Yes Shell nologin Samba Authentication unchecked Full Name Radarr Username radarr User ID 352 New Primary Group unchecked Primary Group radarr Disable Password Yes Shell nologin Samba Authentication unchecked Full Name Sonarr Username sonarr User ID 351 New Primary Group unchecked Primary Group sonarr Disable Password Yes Shell nologin Samba Authentication unchecked Full Name Plex Username plex User ID 972 New Primary Group unchecked Primary Group plex Disable Password Yes Shell nologin Samba Authentication unchecked Datasets# Navigate to Storage ‚Üí Pools and add the datasets and permissions. We\u0026rsquo;ll use default 755 permissions for all datasets.\nJail Config Datasets# Path /mnt/vault0/apps/qbittorrent User qbittorrent Apply User checked Group qbittorrent Apply Group checked Path /mnt/vault0/apps/jackett User jackett Apply User checked Group jackett Apply Group checked Path /mnt/vault0/apps/lidarr User lidarr Apply User checked Group lidarr Apply Group checked Path /mnt/vault0/apps/radarr User radarr Apply User checked Group radarr Apply Group checked Path /mnt/vault0/apps/sonarr User sonarr Apply User checked Group sonarr Apply Group checked Path /mnt/vault0/apps/plex User plex Apply User checked Group plex Apply Group checked Media Datasets# Path /mnt/vault0/media/torrents User qbittorrent Apply User checked Group qbittorrent Apply Group checked Path /mnt/vault0/media/music User lidarr Apply User checked Group lidarr Apply Group checked Path /mnt/vault0/media/movies User radarr Apply User checked Group radarr Apply Group checked Path /mnt/vault0/media/series User sonarr Apply User checked Group sonarr Apply Group checked Jails# Connect to TrueNAS via SSH or similar.\nqBittorrent Jail# # Create jail iocage create --name qbittorrent --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec qbittorrent mkdir /mnt/config iocage fstab --add qbittorrent /mnt/vault0/apps/qbittorrent /mnt/config nullfs rw 0 0 # Mount media dataset iocage exec qbittorrent mkdir /mnt/torrents iocage fstab --add qbittorrent /mnt/vault0/media/torrents /mnt/torrents nullfs rw 0 0 # Update packages iocage exec qbittorrent \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install qBittorrent without GUI iocage exec qbittorrent pkg install qbittorrent-nox # Enable qBittorrent service iocage exec qbittorrent sysrc qbittorrent_enable=YES # Configure config directory iocage exec qbittorrent sysrc qbittorrent_conf_dir=/mnt/config # Start the service iocage exec qbittorrent service qbittorrent start Navigate to http://\u0026lt;jail IP\u0026gt;:8080 in your browser and login with the default credentials.\nUsername Password admin adminadmin Navigate to Tools ‚Üí Options... and change the Default Save Path to /mnt/torrents:\nJackett Jail# # Create jail iocage create --name jackett --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec jackett mkdir /mnt/config iocage fstab --add jackett /mnt/vault0/apps/jackett /mnt/config nullfs rw 0 0 # Change pkg repository set from `quarterly` to `latest` # At the time of this writing, `jackett` from the quarterly repo wasn\u0026#39;t working iocage exec jackett sed -i \u0026#39;\u0026#39; \u0026#39;s/quarterly/latest/g\u0026#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec jackett \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install iocage exec jackett pkg install jackett # Enable service iocage exec jackett sysrc jackett_enable=YES # Configure config directory iocage exec jackett sysrc jackett_data_dir=/mnt/config # Start the service iocage exec jackett service jackett start Navigate to http://\u0026lt;jail IP\u0026gt;:9117 in your browser to use Jackett.\nLidarr Jail# # Create jail iocage create --name lidarr --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec lidarr mkdir /mnt/config iocage fstab --add lidarr /mnt/vault0/apps/lidarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec lidarr mkdir /mnt/music iocage fstab --add lidarr /mnt/vault0/media/music /mnt/music nullfs rw 0 0 iocage exec lidarr mkdir /mnt/torrents iocage fstab --add lidarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from `quarterly` to `latest` iocage exec lidarr sed -i \u0026#39;\u0026#39; \u0026#39;s/quarterly/latest/g\u0026#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec lidarr \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install iocage exec lidarr pkg install lidarr # Enable service iocage exec lidarr sysrc lidarr_enable=YES # Configure config directory iocage exec lidarr sysrc lidarr_data_dir=/mnt/config # Start the service iocage exec lidarr service lidarr start Navigate to http://\u0026lt;jail IP\u0026gt;:8686 in your browser to use Lidarr.\nRadarr Jail# # Create jail iocage create --name radarr --release 12.2-RELEASE dhcp=1 boot=1 allow_mlock=1 # Mount jail config dataset iocage exec radarr mkdir /mnt/config iocage fstab --add radarr /mnt/vault0/apps/radarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec radarr mkdir /mnt/movies iocage fstab --add radarr /mnt/vault0/media/movies /mnt/movies nullfs rw 0 0 iocage exec radarr mkdir /mnt/torrents iocage fstab --add radarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from `quarterly` to `latest` iocage exec radarr sed -i \u0026#39;\u0026#39; \u0026#39;s/quarterly/latest/g\u0026#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec radarr \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install iocage exec radarr pkg install radarr # Enable service iocage exec radarr sysrc radarr_enable=YES # Configure config directory iocage exec radarr sysrc radarr_data_dir=/mnt/config # Start the service iocage exec radarr service radarr start Navigate to http://\u0026lt;jail IP\u0026gt;:7878 in your browser to use Radarr.\nSonarr Jail# # Create jail iocage create --name sonarr --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec sonarr mkdir /mnt/config iocage fstab --add sonarr /mnt/vault0/apps/sonarr /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec sonarr mkdir /mnt/series iocage fstab --add sonarr /mnt/vault0/media/series /mnt/series nullfs rw 0 0 iocage exec sonarr mkdir /mnt/torrents iocage fstab --add sonarr /mnt/vault0/media/torrents /mnt/torrents nullfs ro 0 0 # Change pkg repository set from `quarterly` to `latest` iocage exec sonarr sed -i \u0026#39;\u0026#39; \u0026#39;s/quarterly/latest/g\u0026#39; /etc/pkg/FreeBSD.conf # Update packages iocage exec sonarr \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install iocage exec sonarr pkg install sonarr # Enable service iocage exec sonarr sysrc sonarr_enable=YES # Configure config directory iocage exec sonarr sysrc sonarr_data_dir=/mnt/config # Start the service iocage exec sonarr service sonarr start Navigate to http://\u0026lt;jail IP\u0026gt;:8989 in your browser to use Sonarr.\nPlex Jail# # Create jail iocage create --name plex --release 12.2-RELEASE dhcp=1 boot=1 # Mount jail config dataset iocage exec plex mkdir /mnt/config iocage fstab --add plex /mnt/vault0/apps/plex /mnt/config nullfs rw 0 0 # Mount media datasets iocage exec plex mkdir /mnt/music iocage fstab --add plex /mnt/vault0/media/music /mnt/music nullfs ro 0 0 iocage exec plex mkdir /mnt/series iocage fstab --add plex /mnt/vault0/media/series /mnt/series nullfs ro 0 0 iocage exec plex mkdir /mnt/movies iocage fstab --add plex /mnt/vault0/media/movies /mnt/movies nullfs ro 0 0 # Update packages iocage exec plex \u0026#34;pkg update \u0026amp;\u0026amp; pkg upgrade\u0026#34; # Install iocage exec plex pkg install plexmediaserver # Enable service iocage exec plex sysrc plexmediaserver_enable=YES # Configure config directory iocage exec plex sysrc plexmediaserver_support_path=/mnt/config # Start the service iocage exec plex service plexmediaserver start Navigate to http://\u0026lt;jail IP\u0026gt;:32400/web in your browser to use Plex.\n","date":"2021-11-14","id":11,"permalink":"/blog/install-qbittorrent-jackett-lidarr-radarr-sonarr-and-plex-inside-truenas-jails/","summary":"This post documents the steps required to install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex in TrueNAS jails version 12.0-U6.\n","tags":["FreeBSD","Homelab","iocage","Jackett","Jail","Lidarr","Plex","qBittorrent","Radarr","Self-host","Sonarr","Torrent","TrueNAS"],"title":"Install qBittorrent, Jackett, Lidarr, Radarr, Sonarr, and Plex inside TrueNAS Jails"},{"content":"My homelab grew quite a bit over the past years. And with that, my networking needs also changed: stricter firewall rules, segregating untrusted IoT devices into separate networks, traffic prioritization, and more. I wanted to document my switch and VLAN configuration. And maybe this is useful for someone else, too.\nMikrotik CRS328-24P-4S+RM# The Mikrotik CRS328-24P-4S+RM is a beefy Layer 3 (L3) switch. It features 24 Gigabit Power over Ethernet (PoE) ports and four 10 Gbps SFP+ ports. It has a 500W power supply, so it\u0026rsquo;ll be able to serve as a core switch for my homelab for a long time. It\u0026rsquo;s rack-mountable and was significantly cheaper than comparable Ubiquiti gear I was considering at the time of the purchase. I also replaced its stock fans with Noctua NF-A4x20s, making it completely silent. As long as I don\u0026rsquo;t use too many PoE devices and keep an eye on the temperatures, these fans will dissipate enough heat for the time being. For routing, I use OPNsense, so I only need the L2 capabilities of the Mikrotik switch, which is why I run it on SwOS instead of RouterOS. So, instructions in this guide also refer to SwOS.\nTerminology# From Wikipedia:\nIn computing, a router on a stick, also known as a one-armed router, is a router that has a single physical or logical connection to a network. It is a method of inter-VLAN (virtual local area networks) routing where one router is connected to a switch via a single cable.\nWhen configuring VLANs, we usually encounter three types of port configurations (Cisco lingo):\nAccess Port\nPort carrying untagged traffic for one VLAN Trunk Port\nPort carrying tagged traffic for multiple VLANs Hybrid Port\nPort carrying tagged and untagged traffic for multiple VLANs The traffic of the native VLAN may traverse a trunk port.\nVLAN Overview# I like the convention of matching the third octet of the IP with the VLAN ID. I.e., assigning the VLAN with the ID 10 the address 192.168.10.0/24. Here is an overview of the VLANs I use:\nDescription VLAN ID Subnet Native 1 192.168.1.0/24 Management 10 192.168.10.0/24 VPN 20 192.168.20.0/24 Clear 30 192.168.30.0/24 Guest 40 192.168.40.0/24 Port Mapping# Here is how we\u0026rsquo;re going to use the ports of the switch:\nPort Description VLANs 1 Trunk port connecting OPNsense 1 (untagged), 10, 20, 30, 40 2-3 Not used 4 Hybrid port to Ubiquiti Unifi AP 10 (untagged), 20, 30, 40 5-8 Access ports connecting Management VLAN 10 9-12 Access ports connecting VPN VLAN 20 13-16 Access ports connecting Clear VLAN 30 17-20 Access ports connecting Guest VLAN 40 21-24 Access ports connecting native VLAN 1 (untagged) SFP1-SFP4 Not used Note the following:\nThe trunk port carries all tagged VLAN traffic from the switch to OPNsense The hybrid port carries the tagged traffic of VLANs 20, 30, and 40 made available by the UniFi access point (AP) via WiFi. We also allow untagged VLAN 10 traffic because UniFi devices must communicate over an untagged network to be adopted by a UniFi controller. We could use VLAN 1 for this, but I like to use VLAN 1 for debugging and instead adopt UniFi devices over the management network. For security reasons, connecting to the management network is only allowed via physical cable connection The guest network is only available via WiFi Configure SwOS# The only thing missing now is configuring the VLANs. First, we enable Independent VLAN Lookup (IVL) on the System tab. We then add our VLANs and configure port isolation on the VLANs tab:\nOn the VLAN tab, we configure trunk, hybrid, and access ports:\nEasy as that! Check the official Mikrotik docs for more VLAN example configurations.\n","date":"2021-11-10","id":12,"permalink":"/blog/router-on-a-stick-vlan-configuration-with-swos-on-the-mikrotik-crs328-24p-4s+rm-switch/","summary":"My homelab grew quite a bit over the past years. And with that, my networking needs also changed: stricter firewall rules, segregating untrusted IoT devices into separate networks, traffic prioritization, and more. I wanted to document my switch and VLAN configuration. And maybe this is useful for someone else, too.\n","tags":["Homelab","Mikrotik","Network","Router","SwOS","VLAN"],"title":"Router on a Stick VLAN Configuration with SwOS on the Mikrotik CRS328-24P-4S+RM Switch"},{"content":"To manage Ubiquiti UniFi devices, a UniFi controller is required. Over a year ago, I initially installed the controller software inside a Ubuntu VirtualBox VM. Now that version 6 of the UniFi controller software is released, it\u0026rsquo;s time to upgrade. I decided to reinstall the controller inside a TrueNAS jail instead of a VirtualBox VM. When searching the interwebs, I only found lots of outdated instructions. It turns out that it\u0026rsquo;s very straightforward, so here are my quick notes on how to do it.\nI tested the following on TrueNAS version 12.0-U6.\nInstall UniFi# Connect to a shell via Web GUI or SSH and fetch the latest FreeBSD release available. At the time of this writing, it was 12.2-RELEASE.\niocage fetch Connect to TrueNAS via shell and create the jail.\niocage create --name unifi --release 12.2-RELEASE dhcp=1 boot=1 I use DHCP reservations to manage my server IPs. Setting dhcp=1 also sets vnet=1 and bpf=1. Network configuration is out of scope for this guide. Please consult the iocage manual (man iocage) or the TrueNAS jails documentation for more info. Setting boot=1 enables auto-start at boot time.\nConnect to the jail.\niocage console unifi Once connected, run the following commands.\npkg update \u0026\u0026 pkg upgrade -y # install updates pkg install unifi6 # install unifi sysrc unifi_enable=YES # auto-start at boot service unifi start # start unifi Connect to the UniFi controller at https://\u0026lt;jail IP\u0026gt;:8443.\nUpdating# Updating requires very few steps. Please backup the jail before updating to be able to roll back if something goes wrong.\niocage stop unifi iocage snapshot unifi # OR # iocage export unifi iocage start unifi Connect to the jail.\niocage console unifi Once connected, run the following commands.\nservice unifi stop # stop unifi pkg update \u0026\u0026 pkg upgrade -y # install updates service unifi start # start unifi And that\u0026rsquo;s it!\n","date":"2021-11-04","id":13,"permalink":"/blog/install-the-ubiquiti-unifi-controller-software-v6-inside-a-truenas-jail/","summary":"To manage Ubiquiti UniFi devices, a UniFi controller is required. Over a year ago, I initially installed the controller software inside a Ubuntu VirtualBox VM. Now that version 6 of the UniFi controller software is released, it\u0026rsquo;s time to upgrade. I decided to reinstall the controller inside a TrueNAS jail instead of a VirtualBox VM. When searching the interwebs, I only found lots of outdated instructions. It turns out that it\u0026rsquo;s very straightforward, so here are my quick notes on how to do it.\n","tags":["FreeBSD","Homelab","Jails","Self-host","TrueNAS","Ubiquiti","UniFi"],"title":"Install the Ubiquiti Unifi Controller Software v6 Inside a TrueNAS Jail"},{"content":"I\u0026rsquo;ve been using Mullvad VPN for a while now but only ever used it with the official client on my workstation. I use DNS extensively in my home network, so as soon as I activate Mullvad, I can\u0026rsquo;t resolve DNS names locally. Of course, this is by design and expected. I own an OPNsense appliance, so the natural solution is to move the tunnel there.\nTL;DR# Use the following shell command to request an IP with no DNS hijacking:\n# Authenticate to Mullvad and store the returned access token access_token=$( \\ curl -X \u0026#39;POST\u0026#39; \u0026#39;https://api.mullvad.net/auth/v1/token\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;account_number\u0026#34;: \u0026#34;YOUR MULLVAD ACCOUNT NUMBER\u0026#34; }\u0026#39; \\ | jq -r .access_token) # Create a new Mullvad Device with DNS hijacking disabled curl -X POST https://api.mullvad.net/accounts/v1/devices \\ -H \u0026#34;Authorization: Bearer $access_token\u0026#34; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;pubkey\u0026#34;:\u0026#34;YOUR WIREGUARD PUBLIC KEY\u0026#34;,\u0026#34;hijack_dns\u0026#34;:false}\u0026#39; Mullvad Hijacks DNS Queries Over WireGuard Tunnels# Instead of using the OpenVPN protocol, I decided to go with the latest and greatest: WireGuard. OPNsense is a fork of FreeBSD but lacks a kernel implementation of WireGuard, requiring a plugin. It\u0026rsquo;s good enough for me to try, and I hope WireGuard will be natively supported soon.\nDuring my research on how to best configure this, there seemed to be the caveat of Mullvad hijacking DNS traffic going through WireGuard tunnels, redirecting it to their DNS servers. It decreases the likelihood of DNS leaks, but power users like you and I might not want that. What if I want to query DNS root servers through the VPN tunnel because I use my own DNS resolver? Mullvad hijacking DNS queries would make my DNS resolver trip up.\nWhat a bummer, right? Looking at the Mullvad FAQ, it seemed the only solution was to resort to OpenVPN:\nPorts 1400 UDP and 1401 TCP do not have DNS hijacking enabled, which might work better for pfSense users\nBut Mullvad launched support for custom DNS servers on the Mullvad VPN app back in April 2021. It also works for WireGuard, so what\u0026rsquo;s the secret?\nReverse-engineering the Mullvad App# Searching through the docs, I found the WireGuard on a router article explaining how to get an IP to use with Mullvad via API (note that this snippet is outdated):\ncurl https://api.mullvad.net/wg/ -d account=YOURMULLVADACCOUNTNUMBER --data-urlencode pubkey=YOURPUBLICKEY Next, let\u0026rsquo;s look at how the app requests IPs. Fortunately it\u0026rsquo;s open-source and available on GitHub, so the only reverse-engineering we\u0026rsquo;re going to be doing is reading some code. It turns out that the app uses the https://api.mullvad.net/app/v1/devices endpoint to create a Mullvad Device with DNS hijacking disabled.\nTesting Both APIs# For testing, we\u0026rsquo;ll be using the official WireGuard client. Let\u0026rsquo;s open the client, click Add empty tunnel..., and give it a name:\nThe tunnel will initially look like this:\nCopy the public key and execute the following to request our Mullvad IPs (because the snippet is outdated, this might not work in the future):\ncurl https://api.mullvad.net/wg/ -d account=YOURMULLVADACCOUNTNUMBER --data-urlencode pubkey=YOURPUBLICKEY The response will return an IPv4 and IPv6 address. Add the following to the configuration file:\n[Interface] PrivateKey = \u0026lt;PRIVATE KEY\u0026gt; Address = \u0026lt;IPv4 ADRESS\u0026gt; DNS = 9.9.9.9 [Peer] PublicKey = bmy9vGzMqc0yS3IiMMyOONyXRwPCMiyhR/bnNQ2LsCE= AllowedIPs = 0.0.0.0/0 Endpoint = 31.7.59.250:51820 We use the ch2-wireguard Mullvad server as peer and Quad9 as DNS server.\nLet\u0026rsquo;s activate the tunnel and browse to Mullvad\u0026rsquo;s connection check:\nAs expected, the Quad9 DNS server is not leaking through because Mullvad hijacks our DNS requests and redirects them to their DNS servers.\nNext, we use the API the app uses to request the Mullvad IPs. Before we can do this, we create a new public key with the WireGuard client because Mullvad doesn\u0026rsquo;t allow using the same public key more than once.\nThen we authenticate to Mullvad and store the returned access token:\naccess_token=$( \\ curl -X \u0026#39;POST\u0026#39; \u0026#39;https://api.mullvad.net/auth/v1/token\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;account_number\u0026#34;: \u0026#34;YOUR MULLVAD ACCOUNT NUMBER\u0026#34; }\u0026#39; \\ | jq -r .access_token) Next, we create a new Mullvad Device with DNS hijacking disabled:\ncurl -X POST https://api.mullvad.net/accounts/v1/devices \\ -H \u0026#34;Authorization: Bearer $access_token\u0026#34; -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;pubkey\u0026#34;:\u0026#34;YOUR NEW WIREGUARD PUBLIC KEY\u0026#34;,\u0026#34;hijack_dns\u0026#34;:false}\u0026#39; Next, we replace the IP in Address field of the WireGuard config with the new IP we received. Then we activate the tunnel and visit Mullvad\u0026rsquo;s connection check:\nHooray, the Quad9 DNS servers leak through, so Mullvad is not hijacking our DNS traffic for this tunnel!\n","date":"2021-10-31","id":14,"permalink":"/blog/use-custom-dns-servers-with-mullvad-and-any-wireguard-client/","summary":"I\u0026rsquo;ve been using Mullvad VPN for a while now but only ever used it with the official client on my workstation. I use DNS extensively in my home network, so as soon as I activate Mullvad, I can\u0026rsquo;t resolve DNS names locally. Of course, this is by design and expected. I own an OPNsense appliance, so the natural solution is to move the tunnel there.\nTL;DR# Use the following shell command to request an IP with no DNS hijacking:\n","tags":["DNS","Mullvad","Network","VPN","WireGuard"],"title":"Use Custom DNS Servers With Mullvad And Any WireGuard Client"},{"content":"In the past, I used an old laptop running Qubes OS for any cryptocurrency-related stuff, and it worked great. It\u0026rsquo;s where I first learned about Whonix, a desktop operating system designed to protect your privacy online. Unfortunately, Qubes OS is a bit picky about the hardware it runs on. My old laptop only has four gigs of RAM, and I could barely run two instances of MyEtherWallet in two separate qubes without the system running out of memory.\nThe Final Straw# When initially configuring my Qubes OS environment for Monero, I decided to go with the setup described in a user guide from the official Monero website:\nWith Qubes + Whonix you can have a Monero wallet that is without networking and running on a virtually isolated system from the Monero daemon which has all of its traffic forced over Tor.\nOut of the blue, after more than a year of using this setup, I couldn\u0026rsquo;t start the daemon and the wallet GUI simultaneously anymore. I only had enough RAM to launch one, but not both, which made the setup unusable. So I had to buy new hardware.\nReducing The Complexity# I would have loved to get a NitroPad X230, a Qubes OS-certified laptop to tinker around with Qubes OS some more. Also, the guys at Nitrokey are doing a great job in the open-source space and deserve any support they get. But I ultimately decided to reduce the complexity of my crypto setup and went with a hardware wallet instead ‚Äî the Trezor Model T.\nI\u0026rsquo;ll guide you through my new \u0026ldquo;Whonix on VirtualBox\u0026rdquo; setup and the steps required to configure it.\nImport the VirtualBox Whonix Appliance# First, we download the VirtualBox appliance provided by Whonix and make sure to verify the binary. This requires different steps depending on the OS you are using.\nNext, we import the appliance into VirtualBox by clicking File ‚Üí Import Appliance\u0026hellip;, creating two VMs ‚Äî the gateway and the workstation. The gateway connects us to the Tor network. The workstation will only connect through that gateway, so both VMs must run if we want to connect to the outside world from within the workstation VM.\nMake sure to read the Post-installation Security Advice from the official Whonix docs. We change the default user password and install the latest updates for the gateway at the very least. After, we repeat the same for the workstation VM.\nAt this point, we can optionally clone the workstation VM to have a clean Whonix image if we ever want to start over from scratch.\nInstall the Trezor Dependencies# We can now connect the Trezor device to our computer. We then right-click on the workstation VM and select Settings\u0026hellip;. Under USB, we check the Enable USB Controller and add the Trezor device. This way, it will be automatically attached to the workstation VM.\nLet\u0026rsquo;s start the VM, open a terminal, and import the udev rule for Trezor to enable communication with the Trezor device via Linux kernel:\nsudo curl https://data.trezor.io/udev/51-trezor.rules -o /etc/udev/rules.d/51-trezor.rules Next, we download the SatoshiLabs 2021 Signing Key:\nwget https://trezor.io/security/satoshilabs-2021-signing-key.asc We verify the authenticity of the key by running:\ngpg --with-fingerprint ./satoshilabs-2021-signing-key.asc The value of the fingerprint is EB48 3B26 B078 A4AA 1B6F 425E E21B 6950 A2EC B65C. If it\u0026rsquo;s not, something is very wrong ‚Äî do NOT continue! If everything looks good, we import the key:\ngpg --import ./satoshilabs-2021-signing-key.asc Then we download the Trezor Suite and corresponding signature by running the following in the terminal (replace XX.XX.X with the latest version):\nwget https://suite.trezor.io/web/static/desktop/Trezor-Suite-XX.XX.X-linux-x86_64.AppImage wget https://suite.trezor.io/web/static/desktop/Trezor-Suite-XX.XX.X-linux-x86_64.AppImage.asc We verify the binaries by running:\ngpg --verify ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage.asc The output should contain:\nGood signature from \u0026#34;SatoshiLabs 2021 Signing Key\u0026#34; Next, we make the binary executable:\nchmod +x ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage To launch the Trezor suite and connect to our hardware wallet, we run:\n./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage At this point, we can shut down the VM and take a snapshot so we can roll back later if we need to. I like to clone this VM again for each cryptocurrency requiring an external wallet. It increases the security and maintainability of the setup.\nUsing the Monero GUI# Besides giving the workstation VM a little more RAM, Monero doesn\u0026rsquo;t require additional setup since Whonix includes the Monero GUI. We launch the app and wait for the Monero daemon to sync, which might take a long time using Tor. After that, we\u0026rsquo;re ready to go!\nSetting Up Adalite# Adalite is an open-source, in-browser wallet, and only a few steps are required to get it working.\nFirst, we open the Tor browser and allow pop-up windows from https://adalite.io by adding an exception for it under Settings ‚Üí Preferences ‚Üí Privacy \u0026amp; Security.\nNext, we launch the Trezor Suite by running ./Trezor-Suite-XX.XX.X-linux-x86_64.AppImage. It starts the included Trezor Bridge, which Adalite requires to communicate with the Trezor device. We can verify the Trezor Bridge is running by navigating to http://127.0.0.1:21325/status/ in the browser.\nThe final step is to change two advanced settings of the Tor browser by navigating to about:config and clicking Accept the Risk and Continue. We set the network.proxy.no_proxies_on to 127.0.0.1:21325, so traffic to the Trezor Bridge is not proxied through the Tor network. We also need to disable First-Party Isolation by setting privacy.firstparty.isolate to false.\nWe can now use Adalite by navigating to https://adalite.io. It might be a good idea to bookmark that address, so you don\u0026rsquo;t fall victim to a phishing attack in case of a typo.\nClosing Thoughts# For my purposes, this setup provides more than enough anonymity. Depending on your needs, it might be overkill or might not be anonymous or secure enough. It always depends on your threat model.\nI don\u0026rsquo;t like that SatoshLabs publishes a new signing key every year. Just one more thing to keep in mind come next year.\n","date":"2021-10-22","id":15,"permalink":"/blog/use-the-trezor-hardware-wallet-anonymously-inside-a-virtualbox-whonix-vm-with-external-wallets-like-adalite-and-monero-gui/","summary":"In the past, I used an old laptop running Qubes OS for any cryptocurrency-related stuff, and it worked great. It\u0026rsquo;s where I first learned about Whonix, a desktop operating system designed to protect your privacy online. Unfortunately, Qubes OS is a bit picky about the hardware it runs on. My old laptop only has four gigs of RAM, and I could barely run two instances of MyEtherWallet in two separate qubes without the system running out of memory.\n","tags":["Adalite","Anonymity","Cardano","Cryptocurrency","Monero","Privacy","Qubes OS","Trezor","VirtualBox","Whonix"],"title":"Use the Trezor Hardware Wallet Anonymously Inside a VirtualBox Whonix VM With External Wallets Like Adalite and Monero GUI"},{"content":"For the last couple of months, I have been working on a custom Hugo theme in my free time. Most recently, I implemented a CSS-only burger fold-out menu to increase its responsiveness. I based the implementation on Erik Terwan\u0026rsquo;s nifty pure CSS Hamburger fold-out menu which is pretty popular on CodePen. I modernized it by utilizing SVG and newer CSS selectors to make the code more declarative and scalable. It comes with the price of not supporting as many browsers, but honestly, who cares about Internet Explorer users?\nWhat the Result Looks Like# Have a look at the result (it\u0026rsquo;s interactive):\nItem 1 Item 2 Item 3 All of the illustrating examples use the following basic styling:\n.example { background: #3c3836; color: #ebdbb2; height: 200px; overflow: hidden; position: relative; width: 300px; } Burger Anatomy# As a starting point, I chose the menu-2 icon of the excellent, MIT-licensed Tabler Icon suite:\n\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon icon-tabler icon-tabler-menu-2\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke=\u0026#34;currentColor\u0026#34; fill=\u0026#34;none\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt; \u0026lt;path stroke=\u0026#34;none\u0026#34; d=\u0026#34;M0 0h24v24H0z\u0026#34; fill=\u0026#34;none\u0026#34;/\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;6\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;6\u0026#34; /\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;12\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;12\u0026#34; /\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;18\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;18\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; The icon looks like this:\nHere is a breakdown of the arrangement of the burger\u0026rsquo;s three \u0026lt;line /\u0026gt; elements inside the SVG:\nWhen we later animate the lines, this will be important to know.\nMenu Skeleton# \u0026lt;div class=\u0026#34;menu-burger\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; /\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon icon-tabler icon-tabler-menu-2\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke=\u0026#34;currentColor\u0026#34; fill=\u0026#34;none\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34; \u0026gt; \u0026lt;path stroke=\u0026#34;none\u0026#34; d=\u0026#34;M0 0h24v24H0z\u0026#34; fill=\u0026#34;none\u0026#34; /\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;6\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;6\u0026#34; /\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;12\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;12\u0026#34; /\u0026gt; \u0026lt;line x1=\u0026#34;4\u0026#34; y1=\u0026#34;18\u0026#34; x2=\u0026#34;20\u0026#34; y2=\u0026#34;18\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;ul class=\u0026#34;menu-burger__item-list\u0026#34;\u0026gt; \u0026lt;li\u0026gt;Item 1\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Item 2\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Item 3\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; The menu contains the burger and a list of menu items inside .menu-burger__item-list, which initially is not displayed. The \u0026lt;input type=\u0026quot;checkbox\u0026quot; /\u0026gt; is used as an invisible, \u0026ldquo;CSS-only click handler\u0026rdquo; indicating the menu state. Let\u0026rsquo;s add some initial CSS next:\n.menu-burger \u0026gt; * { position: absolute; } .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } .menu-burger svg { height: 32px; width: 32px; z-index: 2; } ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } input:checked ~ .menu-burger__item-list { transform: none; } .menu-burger input:checked ~ svg line { stroke: #3c3836; } The most important things going on here are:\nAny direct descendant of .menu-burger is positioned absolutely menu-burger__item-list, svg, and checkbox input are stacked on top of each other, ordered by z-index The dimensions of the input checkbox and svg exactly match To initially hide menu-burger__item-list, it\u0026rsquo;s translated out of view A transition animation with a custom cubic-bezier easing-function is added to the transform property, so the fold-out of the menu looks nice. You can play with it on cubic-bezier.com. The \u0026ldquo;click handler\u0026rdquo; CSS magic is happening in input:checked ~ .menu-burger__item-list. ~ is the subsequent-sibling combinator. In this case it matches .menu-burger__item-list siblings of a :checked input checkbox and undoes the initial hiding transition. The same \u0026ldquo;click handler\u0026rdquo; logic is used to change the colors of the line elements from dark to light The rest is self-explanatory, generic CSS styling. Here is what we have so far:\nItem 1 Item 2 Item 3 Now that we finished building the foundation, we can continue with the fun part!\nAnimate the Burger# First, we style each \u0026lt;line /\u0026gt; by using declarative nth-of-type CSS selectors:\n.collapsible__menu svg line:nth-of-type(1) { stroke: red; } .collapsible__menu svg line:nth-of-type(2) { stroke: green; } .collapsible__menu svg line:nth-of-type(3) { stroke: blue; } This styles the first, second and third line of svg like so:\nLet\u0026rsquo;s take a step back and think about what the animation should do:\nFade out the middle line by scaling it down and changing its opacity to 0 Rotate the top and bottom lines to form an X. One way to do this is to vertically center both lines and rotate them by 45 degrees in opposing directions around their center. Before we get started, we have to understand how transformation origins work in CSS:\nThe transform origin is the point around which a transformation is applied. For example, the transform origin of the rotate() function is the center of rotation.\nIn our case, the default transform-origin is (0,0). So before we apply transformations, we have to move the transform-origins of every line to their respective center:\n.menu-burger svg line:nth-of-type(1) { transform-origin: center 6px; } .menu-burger svg line:nth-of-type(2) { transform-origin: center 12px; } .menu-burger svg line:nth-of-type(3) { transform-origin: center 18px; } Next, use the cubic-bezier function again and apply it to every line property to be animated:\n.menu-burger svg line { transition-duration: 0.5s; transition-property: stroke, opacity, transform; transition-timing-function: cubic-bezier(0.9, 0, 0.1, 1); } When clicked, we fade out the middle line of the burger:\n.menu-burger input:checked ~ svg line:nth-of-type(2) { opacity: 0; transform: scale(0.2); } Finally, we vertically center and then rotate the top and bottom lines:\n.menu-burger input:checked ~ svg line:nth-of-type(1) { transform: translate(0, 6px) rotate(45deg); } .menu-burger input:checked ~ svg line:nth-of-type(3) { transform: translate(0, -6px) rotate(-45deg); } Wrapping Up# What do you think? Let me know in the comments or at me on Twitter.\nI created a JSFiddle for you to play around with the code. Here is the entire CSS for reference:\n.example { background: #3c3836; color: #ebdbb2; height: 200px; overflow: hidden; position: relative; width: 300px; } .menu-burger \u0026gt; * { position: absolute; } .menu-burger input { height: 32px; margin: 0; opacity: 0; width: 32px; z-index: 3; } .menu-burger svg { height: 32px; width: 32px; z-index: 2; } ul.menu-burger__item-list { background: #ebdbb2; bottom: 0; color: #3c3836; list-style: none; margin: 0; padding: 32px; top: 0; transform: translate(-100%, 0); transition: transform 0.5s cubic-bezier(0.9, 0, 0.1, 1); width: 200px; z-index: 1; } input:checked ~ .menu-burger__item-list { transform: none; } .menu-burger input:checked ~ svg line { stroke: #3c3836; } .menu-burger svg line:nth-of-type(1) { transform-origin: center 6px; } .menu-burger svg line:nth-of-type(2) { transform-origin: center 12px; } .menu-burger svg line:nth-of-type(3) { transform-origin: center 18px; } .menu-burger svg line { transition-duration: 0.5s; transition-property: stroke, opacity, transform; transition-timing-function: cubic-bezier(0.9, 0, 0.1, 1); } .menu-burger input:checked ~ svg line:nth-of-type(2) { opacity: 0; transform: scale(0.2); } .menu-burger input:checked ~ svg line:nth-of-type(1) { transform: translate(0, 6px) rotate(45deg); } .menu-burger input:checked ~ svg line:nth-of-type(3) { transform: translate(0, -6px) rotate(-45deg); } ","date":"2021-08-03","id":16,"permalink":"/blog/create-a-modern-css-only-fold-out-burger-menu/","summary":"For the last couple of months, I have been working on a custom Hugo theme in my free time. Most recently, I implemented a CSS-only burger fold-out menu to increase its responsiveness. I based the implementation on Erik Terwan\u0026rsquo;s nifty pure CSS Hamburger fold-out menu which is pretty popular on CodePen. I modernized it by utilizing SVG and newer CSS selectors to make the code more declarative and scalable. It comes with the price of not supporting as many browsers, but honestly, who cares about Internet Explorer users?\n","tags":["CSS"],"title":"Create a Modern CSS-only Fold-Out Burger Menu"},{"content":"Sieve is a programming language used for email filtering. Today, I show you how I automatically sort my ProtonMail inbox into folders and subfolders using custom sieve filters. My setup uses the catch-all feature requiring at least a ProtonMail Professional subscription and a properly configured custom domain.\nThe ProtonMail Bridge has been supporting subfolders for a while now. With the release of the re-designed ProtonMail for web, subfolders are now officially supported. So now is a great time to share my setup with you guys!\nThe Goal# I only want emails from unknown senders to land in my inbox, e.g., non-automated messages from people directly contacting me or emails from unknown websites. Sieve filters sort emails from well-known senders into folders and subfolders, like webshops or social media for which I have signed up.\nI use root-level folders to categorize my email. Subfolders are for specific websites belonging to a category:\nInbox/ Webshops/ ‚îú‚îÄ Amazon/ ‚îú‚îÄ Ex Libris/ Entertainment/ ‚îú‚îÄ Netflix/ ‚îú‚îÄ Spotify/ Here, Webshops and Entertainment are the categories, and Amazon and Spotify are specific websites. For each website, I use a separate email address that maps to a pre-created folder. The generic structure of the email address I use to sign up for websites is the following:\ncategory+website@schnerring.net For the example folder structure above, the email-to-folder mapping looks like this:\nwebshops+amazon@schnerring.net ‚Üî Webshops/Amazon/ webshops+exlibris@schnerring.net ‚Üî Webshops/Ex Libris/ entertainment+netflix@schnerring.net ‚Üî Entertainment/Netflix/ entertainment+spotify@schnerring.net ‚Üî Entertainment/Spotify/ *@schnerring.net ‚Üî Inbox/ (catch-all) The Solution# You can add custom sieve filters in the ProtonMail web client by navigating to Settings ‚Üí Filters ‚Üí Add sieve filter. The following snippet meets all of my requirements:\nrequire [\u0026#34;include\u0026#34;, \u0026#34;variables\u0026#34;, \u0026#34;fileinto\u0026#34;, \u0026#34;envelope\u0026#34;]; if envelope :localpart :matches \u0026#34;To\u0026#34; \u0026#34;*+*\u0026#34; { set :lower \u0026#34;category\u0026#34; \u0026#34;${1}\u0026#34;; set :lower \u0026#34;website\u0026#34; \u0026#34;${2}\u0026#34;; } else { return; } if string :is \u0026#34;${website}\u0026#34; \u0026#34;exlibris\u0026#34; { fileinto \u0026#34;${category}/Ex Libris\u0026#34;; } else { fileinto \u0026#34;${category}\u0026#34;; fileinto \u0026#34;${category}/${website}\u0026#34;; } The require command in the first line is used to load extensions that provide functionality, like fileinto to file messages into folders or variables to declare variables.\nNext, the if-conditional checks the :localpart, the part before the @ symbol, of the To address. If it matches the pattern *+* (category+website):\nthe value of the category variable is set to the match left of + symbol (${1}) the value of the website variable is set to the match right of the + symbol (${2}). If :localpart does not match the pattern (else), the sieve filter exits (return).\nNext, the actual sorting happens. Let\u0026rsquo;s look into the else case first:\n} else { fileinto \u0026#34;${category}\u0026#34;; fileinto \u0026#34;${category}/${website}\u0026#34;; } Most of the magic happens here. The email is first moved to the category folder and then moved to the more specific website folder. This way, if the website subfolder does not (yet) exist, the email is at least moved into the root-level category folder. I find this useful when only ordering at a webshop once via guest checkout instead of signing up. This way, I don\u0026rsquo;t need to bother creating an extra subfolder that will only ever contain one or two emails.\nLastly, let\u0026rsquo;s cover the exception for the exlibris webshop in the if-part:\nif string :is \u0026#34;${website}\u0026#34; \u0026#34;exlibris\u0026#34; { fileinto \u0026#34;${category}/Ex Libris\u0026#34;; } It is just a cosmetic exception. When I register at a webshop with webshops+exlibris@schnerring.net but want to map it to a folder called Ex Libris, an explicit mapping is required. Note that fileinto is not case-sensitive, so it is valid if the email address is webshops+amazon@schnerring.net, but the folder name is Webshops/Amazon/. It also is easily extensible by adding more elseif conditions.\nFor more details on advanced custom sieve filtering, check the official ProtonMail documentation.\n","date":"2021-06-16","id":17,"permalink":"/blog/use-sieve-filters-to-auto-sort-your-protonmail-inbox-into-subfolders/","summary":"Sieve is a programming language used for email filtering. Today, I show you how I automatically sort my ProtonMail inbox into folders and subfolders using custom sieve filters. My setup uses the catch-all feature requiring at least a ProtonMail Professional subscription and a properly configured custom domain.\n","tags":["Privacy","ProtonMail","Sieve"],"title":"Use Sieve Filters to Auto-Sort Your ProtonMail Inbox into Subfolders"},{"content":"Update 2022-02-20# This guide is outdated! Please click here to go to the new guide I wrote.\nUpdate 2021-08-03# With v2.69.0 of the official Terraform azurerm provider released two weeks ago, the active_directory_domain_service resource is now available. If you are freshly adding AADDS, there is no point in reading any further ‚Äî use the official resources.\nBringing traditional Active Directory Domain Services (AD DS) to the cloud, typically required to set up, secure, and maintain domain controllers (DCs). Azure Active Directory Domain Services (AADDS or Azure AD DS) is a Microsoft-managed solution, providing a subset of traditional AD DS features without the need to self-manage DCs.\nOne such service that requires AD DS features is Azure Virtual Desktop (WVD). I have successfully deployed WVD with Terraform, but until recently, I struggled to do the same with AADDS. Today, I show you how to deploy AADDS with Terraform.\nIf you are lazy, you can skip to the end and use the custom Terraform module I published to the Terraform Registry.\nPrerequisites# Before getting started, we need the following things:\nActive Azure subscription Azure Active Directory (Azure AD or AAD) tenant Install Terraform Building Blocks# So how do we figure out what the required resources are to deploy AADDS? By reverse-engineering the AADDS configuration wizard in the Azure Portal! We launch it by adding a new managed domain after navigating to Azure AD Domain Services.\nWhat we find is that the following resources are required:\nResource group Virtual network and subnet AAD DC Administrators user group Using default values for the remaining configuration options, we can download an Azure Resource Manager (ARM) template in the final review step of the wizard.\nAnalyzing the ARM template reveals that besides the resource group, virtual network, and subnet, a Network Security Group (NSG) with the security rules required for AADDS is added.\nThe ARM template also contains the Microsoft.AAD/domainServices resource, with its parameters set to the configuration options from the wizard. The following is version 2021-03-01 of the ARM template format from the official Azure Template documentation:\n{ \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.AAD/domainServices\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-03-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;tags\u0026#34;: {}, \u0026#34;properties\u0026#34;: { \u0026#34;domainName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;replicaSets\u0026#34;: [ { \u0026#34;location\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;subnetId\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;ldapsSettings\u0026#34;: { \u0026#34;ldaps\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pfxCertificate\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pfxCertificatePassword\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;externalAccess\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;resourceForestSettings\u0026#34;: { \u0026#34;settings\u0026#34;: [ { \u0026#34;trustedDomainFqdn\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;trustDirection\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;friendlyName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;remoteDnsIps\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;trustPassword\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;resourceForest\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;domainSecuritySettings\u0026#34;: { \u0026#34;ntlmV1\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;tlsV1\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;syncNtlmPasswords\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;syncKerberosPasswords\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;syncOnPremPasswords\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;kerberosRc4Encryption\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;kerberosArmoring\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;domainConfigurationType\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;filteredSync\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;notificationSettings\u0026#34;: { \u0026#34;notifyGlobalAdmins\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;notifyDcAdmins\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;additionalRecipients\u0026#34;: [\u0026#34;string\u0026#34;] } } } The docs also reference the Azure Resource Manager QuickStart Template on GitHub. Its README confirms our previous findings but shows that the configuration wizard also must perform the following steps under the hood:\nRegister the Azure Active Directory Application Service Principal 2565bd9d-da50-47d4-8b85-4c97f669dc36 Register the Microsoft.AAD Resource Provider With everything figured out, we can continue with the fun part: Terraform!\nPutting Everything Together# Let\u0026rsquo;s register the service principal and resource provider first:\nresource \u0026#34;azuread_service_principal\u0026#34; \u0026#34;aadds\u0026#34; { application_id = \u0026#34;2565bd9d-da50-47d4-8b85-4c97f669dc36\u0026#34; } resource \u0026#34;azurerm_resource_provider_registration\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;Microsoft.AAD\u0026#34; } Next, we add the AAD DC Administrators user group:\nresource \u0026#34;azuread_group\u0026#34; \u0026#34;aadds\u0026#34; { display_name = \u0026#34;AAD DC Administrators\u0026#34; description = \u0026#34;Delegated group to administer Azure AD Domain Services\u0026#34; } Adding the resource group, virtual network, subnet, and NSG is pretty straightforward:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-vnet\u0026#34; resource_group_name = azurerm_resource_group.aadds.name location = \u0026#34;Switzerland North\u0026#34; address_space = [\u0026#34;10.0.0.0/16\u0026#34;] # AADDS DCs dns_servers = [\u0026#34;10.0.0.4\u0026#34;, \u0026#34;10.0.0.5\u0026#34;] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-snet\u0026#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [\u0026#34;10.0.0.0/24\u0026#34;] } resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-nsg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; resource_group_name = azurerm_resource_group.aadds.name security_rule { name = \u0026#34;AllowRD\u0026#34; access = \u0026#34;Allow\u0026#34; priority = 201 direction = \u0026#34;Inbound\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_address_prefix = \u0026#34;CorpNetSaw\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;3389\u0026#34; } security_rule { name = \u0026#34;AllowPSRemoting\u0026#34; access = \u0026#34;Allow\u0026#34; priority = 301 direction = \u0026#34;Inbound\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_address_prefix = \u0026#34;AzureActiveDirectoryDomainServices\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;5986\u0026#34; } } resource \u0026#34;azurerm_subnet_network_security_group_association\u0026#34; \u0026#34;aadds\u0026#34; { subnet_id = azurerm_subnet.aadds.id network_security_group_id = azurerm_network_security_group.aadds.id } Make sure to set dns_servers to the IP addresses of the DCs. You can find them on the Overview page of the managed domain after the deployment succeeded.\nThe final step is to add the AADDS deployment. Define the ARM template as Terraform templatefile named aadds-arm-template.tpl.json:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;${domainName}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.AAD/domainServices\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-03-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;${location}\u0026#34;, \u0026#34;tags\u0026#34;: ${tags}, \u0026#34;properties\u0026#34;: { \u0026#34;domainName\u0026#34;: \u0026#34;${domainName}\u0026#34;, \u0026#34;replicaSets\u0026#34;: [ { \u0026#34;location\u0026#34;: \u0026#34;${location}\u0026#34;, \u0026#34;subnetId\u0026#34;: \u0026#34;${subnetId}\u0026#34; } ], \u0026#34;domainSecuritySettings\u0026#34;: { \u0026#34;ntlmV1\u0026#34;: \u0026#34;${ntlmV1}\u0026#34;, \u0026#34;tlsV1\u0026#34;: \u0026#34;${tlsV1}\u0026#34;, \u0026#34;syncNtlmPasswords\u0026#34;: \u0026#34;${syncNtlmPasswords}\u0026#34;, \u0026#34;syncKerberosPasswords\u0026#34;: \u0026#34;${syncKerberosPasswords}\u0026#34;, \u0026#34;syncOnPremPasswords\u0026#34;: \u0026#34;${syncOnPremPasswords}\u0026#34;, \u0026#34;kerberosRc4Encryption\u0026#34;: \u0026#34;${kerberosRc4Encryption}\u0026#34;, \u0026#34;kerberosArmoring\u0026#34;: \u0026#34;${kerberosArmoring}\u0026#34; }, \u0026#34;domainConfigurationType\u0026#34;: \u0026#34;${domainConfigurationType}\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;${sku}\u0026#34;, \u0026#34;filteredSync\u0026#34;: \u0026#34;${filteredSync}\u0026#34;, \u0026#34;notificationSettings\u0026#34;: { \u0026#34;notifyGlobalAdmins\u0026#34;: \u0026#34;${notifyGlobalAdmins}\u0026#34;, \u0026#34;notifyDcAdmins\u0026#34;: \u0026#34;${notifyDcAdmins}\u0026#34;, \u0026#34;additionalRecipients\u0026#34;: ${additionalRecipients} } } } ] } We then populate its values dynamically like so:\nresource \u0026#34;azurerm_resource_group_template_deployment\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-deploy\u0026#34; resource_group_name = azurerm_resource_group.aadds.name deployment_mode = \u0026#34;Incremental\u0026#34; template_content = templatefile( \u0026#34;${path.module}/aadds-arm-template.tpl.json\u0026#34;, { # Basics \u0026#34;domainName\u0026#34; = \u0026#34;aadds.schnerring.net\u0026#34; \u0026#34;location\u0026#34; = \u0026#34;Switzerland North\u0026#34; \u0026#34;sku\u0026#34; = \u0026#34;Standard\u0026#34; \u0026#34;domainConfigurationType\u0026#34; = \u0026#34;FullySynced\u0026#34; # Networking \u0026#34;subnetId\u0026#34; = azurerm_subnet.aadds.id # Administration \u0026#34;notifyGlobalAdmins\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;notifyDcAdmins\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;additionalRecipients\u0026#34; = jsonencode([]) # Synchronization \u0026#34;filteredSync\u0026#34; = \u0026#34;Enabled\u0026#34; # Security \u0026#34;tlsV1\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;ntlmV1\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;syncNtlmPasswords\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;syncOnPremPasswords\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;kerberosRc4Encryption\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;syncKerberosPasswords\u0026#34; = \u0026#34;Enabled\u0026#34; \u0026#34;kerberosArmoring\u0026#34; = \u0026#34;Disabled\u0026#34; # Tags \u0026#34;tags\u0026#34; = jsonencode({}) } ) depends_on = [azurerm_resource_provider_registration.aadds] } Run terraform apply to deploy everything. It takes around 45 minutes to complete.\nWrapping Up# I created a custom module wrapping the above functionality and published it to the Terraform Registry. You can also find the code on my GitHub along with some examples. I decided that the creation of network resources is out of the module\u0026rsquo;s scope. Depending on what network topology you prefer, pre-provisioning the virtual network and subnet gives you more flexibility.\nThe module provides the same options as the Azure Portal configuration wizard. More advanced configuration options like LDAP and forests are not yet supported. Feel free to comment below, or open an issue or pull request on GitHub if you find something to improve.\nA minimal deployment with the custom module looks like this:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-rg\u0026#34; location = \u0026#34;Switzerland North\u0026#34; } resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-vnet\u0026#34; resource_group_name = azurerm_resource_group.aadds.name location = \u0026#34;Switzerland North\u0026#34; address_space = [\u0026#34;10.0.0.0/16\u0026#34;] # AADDS DCs dns_servers = [\u0026#34;10.0.0.4\u0026#34;, \u0026#34;10.0.0.5\u0026#34;] } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;aadds\u0026#34; { name = \u0026#34;aadds-snet\u0026#34; resource_group_name = azurerm_resource_group.aadds.name virtual_network_name = azurerm_virtual_network.aadds.name address_prefixes = [\u0026#34;10.0.0.0/24\u0026#34;] } module \u0026#34;aadds\u0026#34; { source = \u0026#34;schnerring/aadds/azurerm\u0026#34; version = \u0026#34;0.1.1\u0026#34; resource_group_name = azurerm_resource_group.aadds.name location = \u0026#34;Switzerland North\u0026#34; domain_name = \u0026#34;aadds.schnerring.net\u0026#34; subnet_id = azurerm_subnet.aadds.id } ","date":"2021-05-22","id":18,"permalink":"/blog/set-up-azure-active-directory-domain-services-aadds-with-terraform/","summary":"Update 2022-02-20# This guide is outdated! Please click here to go to the new guide I wrote.\n","tags":["AADDS","ARM","Azure","Azure Active Directory Domain Services","Azure Resource Manager Template","Cloud","IaC","Infrastructure as Code","Terraform"],"title":"Set Up Azure Active Directory Domain Services (AADDS) with Terraform"},{"content":"Did you ever think about running a Matrix homeserver? In this post, we will set one up on the Azure Kubernetes Service (AKS). We will use the reference homeserver implementation, which is Synapse from the folks at matrix.org. This post focuses on the Kubernetes stuff, keeping Synapse configuration to a minimum. Things like federation, delegation and PostgreSQL are out of scope, because plenty of excellent guides and the official documentation exist covering that. The icing on the cake will be the Synapse Admin UI deployment with secure access to the administration endpoints to make management of our homeserver easier.\nPreface# Besides some basic knowledge about AKS, Kubernetes, and Terraform, we have to set up a couple of other things before getting started.\nWe need an AKS cluster with a configured Ingress Controller to be able to expose our homeserver to the world. I use Traefik 2 in combination with its Kubernetes Ingress implementation.\nSynapse requires valid TLS certificates to work. It ships used to ship with functionality to automatically provision Let\u0026rsquo;s Encrypt certificates. However, I use cert-manager as a certificate management solution for all my services. So we skip over that part of the configuration, too.\nThe last thing we have to set up is Terraform with a properly configured kubernetes provider. If you do not want to use Terraform, transforming the code to regular YAML manifests is trivial.\nDepending on your needs, reverse proxy (ingress) functionality and certificate management is the part where your setup differs the most. If you are starting out from scratch, check out my previous post, covering much of the steps required to set up the AKS cluster.\nIngress# We work from the outside to the inside. The Ingress is what exposes our Kubernetes Service to the public internet. We also add a Namespace for all our Matrix resources to a new Terraform file matrix.tf:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix\u0026#34; } } resource \u0026#34;kubernetes_ingress_v1\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix-ing\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name annotations = { \u0026#34;cert-manager.io/cluster-issuer\u0026#34; = \u0026#34;letsencrypt-staging\u0026#34; \u0026#34;traefik.ingress.kubernetes.io/router.tls\u0026#34; = \u0026#34;true\u0026#34; } } spec { rule { host = var.synapse_server_name http { path { path = \u0026#34;/_matrix\u0026#34; backend { service { name = \u0026#34;matrix-svc\u0026#34; port { number = 8008 } } } } path { path = \u0026#34;/_synapse/client\u0026#34; backend { service { name = \u0026#34;matrix-svc\u0026#34; port { number = 8008 } } } } path { path = \u0026#34;/\u0026#34; backend { service { name = \u0026#34;matrix-admin-svc\u0026#34; port { number = 8080 } } } } } } tls { secret_name = \u0026#34;matrix-tls-secret\u0026#34; hosts = [var.synapse_server_name] } } } To route external traffic to the Synapse Service (matrix-svc), we set up the /_matrix and /_synapse/client endpoints according to the official reverse proxy documentation. The / endpoint routes traffic to the Synapse Admin UI (matrix-admin-svc). We do NOT expose the /_synapse/admin endpoint. We will look at how to access this endpoint at the end of this post.\nAfter we finish testing, we must change the certificate issuer from letsencrypt-staging to letsencrypt-production. Later on, we will also define the var.synapse_server_name Terraform input variable.\nServices# Services expose our Synapse and Synapse Admin UI deployments as network services inside our Kubernetes cluster:\nresource \u0026#34;kubernetes_service\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix-svc\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { selector = { \u0026#34;app\u0026#34; = \u0026#34;matrix\u0026#34; } port { port = 8008 target_port = 8008 } } } resource \u0026#34;kubernetes_service\u0026#34; \u0026#34;matrix_admin\u0026#34; { metadata { name = \u0026#34;matrix-admin-svc\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { selector = { \u0026#34;app\u0026#34; = \u0026#34;matrix-admin\u0026#34; } port { port = 8080 target_port = 80 } } } Pretty self-explanatory. We map the service ports to container ports of the deployments that we add in a later step.\nConfiguration Files# Before adding the deployment to our cluster, we generate the initial configuration files for Synapse:\nhomeserver.yaml: Synapse configuration matrix.schnerring.net.log.config: logging configuration matrix.schnerring.net.signature.key: the key, Synapse signs messages with We can do that with the generate command which is part of Synapse:\nkubectl run matrix-generate-pod \\ --stdin \\ --rm \\ --restart=Never \\ --command=true \\ --image matrixdotorg/synapse:latest \\ --env=\u0026#34;SYNAPSE_REPORT_STATS=yes\u0026#34; \\ --env=\u0026#34;SYNAPSE_SERVER_NAME=matrix.schnerring.net\u0026#34; \\ -- bash -c \u0026#34;/start.py generate \u0026amp;\u0026amp; sleep 300\u0026#34; This command creates a pod, runs generate, sleeps for 300 seconds, and then cleans itself up. Five minutes should be enough time to copy the generated files from the container before it terminates. The following command copies the entire /data directory from the container to a local synapse-config directory:\nkubectl cp matrix-generate-pod:/data synapse-config We then store the three configuration files as Secret on the cluster. But before, make sure to change the value of handlers.file.filename from /homeserver.log to /data/homeserver.log inside the *.log.config file. Otherwise, you will run into a permission issue caused by the default logger configuration that I discovered.\nlocals { synapse_log_config = \u0026#34;/data/${var.synapse_server_name}.log.config\u0026#34; synapse_signing_key_path = \u0026#34;/data/${var.synapse_server_name}.signing.key\u0026#34; } resource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix-secret\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name } # See also: https://github.com/matrix-org/synapse/blob/master/docker/README.md#generating-a-configuration-file data = { \u0026#34;homeserver.yaml\u0026#34; = templatefile( \u0026#34;${path.module}/synapse-config/homeserver.tpl.yaml\u0026#34;, { \u0026#34;server_name\u0026#34; = var.synapse_server_name \u0026#34;report_stats\u0026#34; = var.synapse_report_stats \u0026#34;log_config\u0026#34; = local.synapse_log_config \u0026#34;signing_key_path\u0026#34; = local.synapse_signing_key_path \u0026#34;registration_shared_secret\u0026#34; = var.synapse_registration_shared_secret \u0026#34;macaroon_secret_key\u0026#34; = var.synapse_macaroon_secret_key \u0026#34;form_secret\u0026#34; = var.synapse_form_secret } ) \u0026#34;log.config\u0026#34; = templatefile( \u0026#34;${path.module}/synapse-config/log.tpl.config\u0026#34;, { \u0026#34;log_filename\u0026#34; = \u0026#34;/data/homeserver.log\u0026#34; \u0026#34;log_level\u0026#34; = \u0026#34;INFO\u0026#34; } ) \u0026#34;signing.key\u0026#34; = var.synapse_signing_key } } To protect the signing key and the secrets contained inside the homeserver.yaml file, we use the Terraform templatefile() function, which allows us to put variable placeholders into the configuration files that are interpolated during terraform apply. This way, we can commit the configuration files to source control securely. To denote the files as template files, we change the file names to homeserver.tpl.yaml and log.tpl.config. Here is an example of how to define interpolation sequences inside homeserver.yaml:\nserver_name: \u0026#34;${server_name}\u0026#34; # ... # a secret which is used to sign access tokens. If none is specified, # the registration_shared_secret is used, if one is given; otherwise, # a secret key is derived from the signing key. # macaroon_secret_key: \u0026#34;${macaroon_secret_key}\u0026#34; # a secret which is used to calculate HMACs for form values, to stop # falsification of values. Must be specified for the User Consent # forms to work. # form_secret: \u0026#34;${form_secret}\u0026#34; Of course, you can also check out the config file templates on my GitHub.\nTo find the values inside the huge homeserver.yaml file, we use the following regular expression. It matches any line that is not a comment or whitespace:\n^(?!^\\s*#)^(?!\\n).* All we have to do now is define the Terraform input variables and set them to the values we originally generated:\nvariable \u0026#34;synapse_image_version\u0026#34; { type = string description = \u0026#34;Synapse image version.\u0026#34; default = \u0026#34;v1.33.2\u0026#34; } variable \u0026#34;synapse_server_name\u0026#34; { type = string description = \u0026#34;Public Synapse hostname.\u0026#34; } variable \u0026#34;synapse_report_stats\u0026#34; { type = bool description = \u0026#34;Enable anonymous statistics reporting.\u0026#34; } variable \u0026#34;synapse_signing_key\u0026#34; { type = string description = \u0026#34;Signing key Synapse signs messages with.\u0026#34; sensitive = true } variable \u0026#34;synapse_registration_shared_secret\u0026#34; { type = string description = \u0026#34;Allows registration of standard or admin accounts by anyone who has the shared secret.\u0026#34; sensitive = true } variable \u0026#34;synapse_macaroon_secret_key\u0026#34; { type = string description = \u0026#34;Secret which is used to sign access tokens.\u0026#34; sensitive = true } variable \u0026#34;synapse_form_secret\u0026#34; { type = string description = \u0026#34;Secret which is used to calculate HMACs for form values.\u0026#34; sensitive = true } Persistent Volume Claim# To be able to persist media, we create a PersistentVolumeClaim (PVC):\nresource \u0026#34;kubernetes_persistent_volume_claim\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix-pvc\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name } spec { access_modes = [\u0026#34;ReadWriteOnce\u0026#34;] resources { requests = { \u0026#34;storage\u0026#34; = \u0026#34;4Gi\u0026#34; } } } } Note that the default AKS StorageClass has the ReclaimPolicy set to Delete . I recommend defining a custom storage class for production. Setting its reclaim policy to Retain makes accidentally purging the media PVC much harder.\nSynapse# Let us finally deploy Synapse. We just need to mount the configuration files and the PVC we just defined:\nresource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;matrix\u0026#34; { metadata { name = \u0026#34;matrix-deploy\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix\u0026#34; } } spec { replicas = 1 selector { match_labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix\u0026#34; } } strategy { type = \u0026#34;Recreate\u0026#34; } template { metadata { labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix\u0026#34; } } spec { hostname = \u0026#34;matrix\u0026#34; restart_policy = \u0026#34;Always\u0026#34; security_context { run_as_user = \u0026#34;991\u0026#34; run_as_group = \u0026#34;991\u0026#34; fs_group = \u0026#34;991\u0026#34; run_as_non_root = true } container { name = \u0026#34;synapse\u0026#34; image = \u0026#34;matrixdotorg/synapse:${var.synapse_image_version}\u0026#34; security_context { read_only_root_filesystem = true } port { container_port = 8008 } volume_mount { mount_path = \u0026#34;/data\u0026#34; name = \u0026#34;data-vol\u0026#34; } volume_mount { name = \u0026#34;secret-vol\u0026#34; mount_path = \u0026#34;/data/homeserver.yaml\u0026#34; sub_path = \u0026#34;homeserver.yaml\u0026#34; read_only = true } volume_mount { name = \u0026#34;secret-vol\u0026#34; mount_path = local.synapse_log_config sub_path = \u0026#34;log.config\u0026#34; read_only = true } volume_mount { name = \u0026#34;secret-vol\u0026#34; mount_path = local.synapse_signing_key_path sub_path = \u0026#34;signing.key\u0026#34; read_only = true } } volume { name = \u0026#34;data-vol\u0026#34; persistent_volume_claim { claim_name = \u0026#34;matrix-pvc\u0026#34; } } volume { name = \u0026#34;secret-vol\u0026#34; secret { secret_name = \u0026#34;matrix-secret\u0026#34; } } } } } } It is good practice to lock down the container by making the root filesystem read-only, if possible. We also run the container as the 991 user and group, respectively, the default user used by Synapse.\nTo deploy everything, we run the following commands:\nterraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan After the deployment succeeds, do not forget to change the cluster issuer to letsencrypt-production and terraform apply again.\nNext, we register a new user. Synapse includes the register_new_matrix_user command. First, we query the name of the pod:\nkubectl get pods --namespace matrix We then run the following to connect to the pod:\nkubectl exec --stdin --tty --namespace matrix matrix-deploy-xxxxxxxxxx-xxxxx -- bash Now we register the user:\nregister_new_matrix_user \\ --admin \\ --user michael \\ --config /data/homeserver.yaml \\ http://localhost:8008 We use Element or any other Matrix client and sign in. Great!\nSynapse Admin UI# The Synapse API includes administration endpoints but lacks a UI for administration tasks. There is an open GitHub issue which seems to be inactive. However, awesometechnologies/synapse-admin is being actively developed and works well.\nWe already configured the ingress to route traffic to the matrix-admin-svc, so the only thing missing is the deployment:\nresource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;matrix_admin\u0026#34; { metadata { name = \u0026#34;matrix-admin-deploy\u0026#34; namespace = kubernetes_namespace.matrix.metadata.0.name labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix-admin\u0026#34; } } spec { replicas = 1 selector { match_labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix-admin\u0026#34; } } strategy { type = \u0026#34;Recreate\u0026#34; } template { metadata { labels = { \u0026#34;app\u0026#34; = \u0026#34;matrix-admin\u0026#34; } } spec { hostname = \u0026#34;matrix-admin\u0026#34; restart_policy = \u0026#34;Always\u0026#34; container { name = \u0026#34;synapse-admin\u0026#34; image = \u0026#34;awesometechnologies/synapse-admin:latest\u0026#34; port { container_port = 80 } } } } } } No configuration is required because the Admin UI is a client-side application. After running terraform apply, we can browse to matrix.schnerring.net to access the app:\nSynapse Admin UI requires access to the _synapse/admin endpoint. But we do not want to expose that endpoint to the public internet, so we have to connect to it by other means. kubectl port-forward allows us to securely forward a local port to a port on a Kubernetes service:\nkubectl port-forward service/matrix-svc --namespace matrix 8008:8008 We can now enter http://localhost:8008 as homeserver URL and login to the admin UI with the user we created earlier:\nWhat\u0026rsquo;s Next?# Regardless of what you plan on doing with your homeserver, replacing SQLite with PostgreSQL should be at the top of the priority list. Other than that, there is tons of other stuff to configure depending on your needs.\nI am super happy with what we have built and curious about what you think. I would appreciate it if you shared your opinion in the comments below.\n","date":"2021-05-14","id":19,"permalink":"/blog/deploy-a-matrix-homeserver-to-azure-kubernetes-service-aks-with-terraform/","summary":"Did you ever think about running a Matrix homeserver? In this post, we will set one up on the Azure Kubernetes Service (AKS). We will use the reference homeserver implementation, which is Synapse from the folks at matrix.org. This post focuses on the Kubernetes stuff, keeping Synapse configuration to a minimum. Things like federation, delegation and PostgreSQL are out of scope, because plenty of excellent guides and the official documentation exist covering that. The icing on the cake will be the Synapse Admin UI deployment with secure access to the administration endpoints to make management of our homeserver easier.\n","tags":["AKS","Azure","Azure Kubernetes Service","Cloud","k8s","Kubernetes","Matrix","Self-host","Synapse","Terraform"],"title":"Deploy a Matrix Homeserver to Azure Kubernetes Service (AKS) with Terraform"},{"content":"Building upon our previous work, we will deploy Remark42 on Kubernetes with Terraform and integrate it with your existing Hugo website. Make sure to check out my previous posts about creating a Hugo Website and deploying an Azure Kubernetes Service cluster if you haven\u0026rsquo;t already.\nAbout Remark42# Remark42 is a self-hosted, lightweight, and simple (yet functional) commenting system, which doesn\u0026rsquo;t spy on users.\nI like simplicity, I am a privacy enthusiast, and I build this blog with this in mind. More popular, hands-off solutions like Disqus offer easier integration and more sophisticated features, like automated spam moderation and advertising. But for my intents, it\u0026rsquo;s too bloated and invasive. For low-traffic websites like mine, Remark42 is just the better fit.\nPreparation# Besides a Hugo website and a Kubernetes cluster, you will have to install the following software on your workstation:\nTerraform kompose (optional) Converting the original docker-compose.yml# The official Remark42 repository includes a docker-compose.yml that we can download:\ncurl -O https://raw.githubusercontent.com/umputun/remark42/master/docker-compose.yml We then run kompose convert to generate regular Kubernetes YAML manifests from the docker-compose.yml file:\nKubernetes file \u0026#34;remark-deployment.yaml\u0026#34; created Kubernetes file \u0026#34;remark-claim0-persistentvolumeclaim.yaml\u0026#34; created To figure out what resources we need to create, we use the generated manifests as a starting point.\nCreate a Namespace# First, we create the Namespace where our Remark42 resources will reside. We add the following Terraform code to a new file named remark42.yml:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42\u0026#34; } } Deploy the changes by running:\nterraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan Create the Persistent Volume Claim# To enable persistence for Remark42, we create a PersistentVolumeClaim (PVC) resource. Using the generate remark-claim0-persistentvolumeclaim.yaml file as a blueprint, we can easily derive the Terraform equivalent from it and add it to the remark42.tf file:\nresource \u0026#34;kubernetes_persistent_volume_claim\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-pvc\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name labels = { \u0026#34;app\u0026#34; = \u0026#34;remark42-pvc\u0026#34; } } spec { access_modes = [\u0026#34;ReadWriteOnce\u0026#34;] storage_class_name = \u0026#34;azurefile\u0026#34; resources { requests = { \u0026#34;storage\u0026#34; = \u0026#34;1Gi\u0026#34; } } } } AKS comes pre-configured with multiple StorageClasses. Here, I use the azurefile storage class to dynamically provision a persistent volume with Azure Files. At the time of this writing, I use a B2ms-sized VM as a node which is limited to four data disks. Using Azure Files whenever possible helps me circumventing this limitation.\nCreate ConfigMap and Secret# To store configuration parameters for our Remark42 deployment, we make use of Kubernetes ConfigMap and Secret resources.\nTo store sensitive values, we use Secret:\nresource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-secret\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name } data = { \u0026#34;SECRET\u0026#34; = random_password.remark42_secret.result } } For non-sensitive stuff, we use ConfigMap:\nresource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-cm\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name } data = { \u0026#34;REMARK_URL\u0026#34; = \u0026#34;https://${cloudflare_record.remark42.hostname}\u0026#34; \u0026#34;SITE\u0026#34; = \u0026#34;schnerring.net\u0026#34; } } For this post, I have kept the configuration to a minimum:\nREMARK_URL: URL to Remark42 server SITE: site name(s) SECRET: secret key Check the official documentation or my code on GitHub for more configuration options.\nYou might have noticed that I reference a cloudflare_record in the REMARK_URL part. That\u0026rsquo;s because I also manage my DNS records with Terraform. The DNS record for remark42.schnerring.net pointing to the DNS record of my cluster looks like this:\nresource \u0026#34;cloudflare_record\u0026#34; \u0026#34;remark42\u0026#34; { zone_id = cloudflare_zone.schnerring_net.id name = \u0026#34;remark42\u0026#34; type = \u0026#34;CNAME\u0026#34; value = cloudflare_record.traefik.hostname proxied = true } Create the Deployment# Next, we add the Deployment to the remark42.tf file, using the remark-deployment.yaml file as a model. We map the previously defined configuration parameters to environment variables and mount the PVC to /srv/var.\nresource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-deploy\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name labels = { \u0026#34;app\u0026#34; = \u0026#34;remark42\u0026#34; } } spec { replicas = 1 selector { match_labels = { \u0026#34;app\u0026#34; = \u0026#34;remark42\u0026#34; } } strategy { type = \u0026#34;Recreate\u0026#34; } template { metadata { labels = { \u0026#34;app\u0026#34; = \u0026#34;remark42\u0026#34; } } spec { hostname = \u0026#34;remark42\u0026#34; restart_policy = \u0026#34;Always\u0026#34; container { name = \u0026#34;remark42\u0026#34; image = \u0026#34;umputun/remark42:${var.remark42_image_version}\u0026#34; port { container_port = 8080 } env { name = \u0026#34;REMARK_URL\u0026#34; value_from { config_map_key_ref { key = \u0026#34;REMARK_URL\u0026#34; name = \u0026#34;remark42-cm\u0026#34; } } } env { name = \u0026#34;SECRET\u0026#34; value_from { secret_key_ref { key = \u0026#34;SECRET\u0026#34; name = \u0026#34;remark42-secret\u0026#34; } } } env { name = \u0026#34;SITE\u0026#34; value_from { config_map_key_ref { key = \u0026#34;SITE\u0026#34; name = \u0026#34;remark42-cm\u0026#34; } } } volume_mount { mount_path = \u0026#34;/srv/var\u0026#34; name = \u0026#34;remark42-vol\u0026#34; } } volume { name = \u0026#34;remark42-vol\u0026#34; persistent_volume_claim { claim_name = \u0026#34;remark42-pvc\u0026#34; } } } } } } Create the Service# To expose our deployment as a network service, we create a Service resource by adding the following to the remark42.tf file:\nresource \u0026#34;kubernetes_service\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-svc\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name } spec { selector = { \u0026#34;app\u0026#34; = \u0026#34;remark42\u0026#34; } port { name = \u0026#34;http\u0026#34; port = 80 target_port = 8080 } } } Create the Ingress# I use Traefik 2 as Ingress Controller in combination with the Traefik Kubernetes Ingress provider. To manage Let\u0026rsquo;s Encrypt certificates, I use cert-manager. So depending on your cluster configuration, the following steps might differ.\nLet us add an Ingress to the remark42.tf file, to expose our service to the world:\nresource \u0026#34;kubernetes_ingress_v1\u0026#34; \u0026#34;remark42\u0026#34; { metadata { name = \u0026#34;remark42-ing\u0026#34; namespace = kubernetes_namespace.remark42.metadata.0.name annotations = { \u0026#34;cert-manager.io/cluster-issuer\u0026#34; = \u0026#34;letsencrypt-production\u0026#34; \u0026#34;traefik.ingress.kubernetes.io/router.tls\u0026#34; = \u0026#34;true\u0026#34; } } spec { rule { host = cloudflare_record.remark42.hostname http { path { path = \u0026#34;/\u0026#34; backend { service { name = \u0026#34;remark42-svc\u0026#34; port { number = 80 } } } } } } tls { hosts = [cloudflare_record.remark42.hostname] secret_name = \u0026#34;remark42-tls-secret\u0026#34; } } } Now run terraform apply to deploy everything. Note that we are using letsencrypt-staging as cluster issuer. We will have to change this to letsencrypt-production once we finished testing.\nBrowse to the demo site at https://remark42.schnerring.net/web and check whether Remark42 works. If you want to post a comment on the demo site, make sure to add the remark site ID to the SITE environment variable, separated by a , (i.e., schnerring.net,remark).\nIntegrate Remark42 with Hugo# To add the Remark42 comment widget to our Hugo site, we have to integrate it with our theme. At the time of this writing, I use the Hello Friend theme, which includes a partial template for the comment section. We add the Remark42 widget to the layouts/partials/comments.html file:\n\u0026lt;div id=\u0026#34;remark42\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; var remark_config = { host: \u0026#34;https://remark42.schnerring.net\u0026#34;, site_id: \u0026#34;schnerring.net\u0026#34;, theme: getHelloFriendTheme(), show_email_subscription: false, }; \u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; !(function (e, n) { for (var o = 0; o \u0026lt; e.length; o++) { var r = n.createElement(\u0026#34;script\u0026#34;), c = \u0026#34;.js\u0026#34;, d = n.head || n.body; \u0026#34;noModule\u0026#34; in r ? ((r.type = \u0026#34;module\u0026#34;), (c = \u0026#34;.mjs\u0026#34;)) : (r.async = !0), (r.defer = !0), (r.src = remark_config.host + \u0026#34;/web/\u0026#34; + e[o] + c), d.appendChild(r); } })(remark_config.components || [\u0026#34;embed\u0026#34;], document); \u0026lt;/script\u0026gt; You can find more configuration options for the widget in the Remark42 GitHub README.\nNext, we implement the getHelloFriendTheme() function, so Remark42 loads the correct theme. The Hello Friend theme stores the current theme in the local storage of the browser. Knowing that, implementing the function is pretty straight forward:\n\u0026lt;script\u0026gt; const defaultTheme = \u0026#34;light\u0026#34;; // same as defaultTheme in config.toml function getHelloFriendTheme() { const theme = localStorage \u0026amp;\u0026amp; localStorage.getItem(\u0026#34;theme\u0026#34;); if (!theme) { return defaultTheme; } else { return theme; } } \u0026lt;/script\u0026gt; The last thing we have to take care of is to also toggle the Remark42 theme, when clicking the theme toggle button of the Hello Friend theme. To do so, we register an additional click event handler to the .theme-toggle button and call the window.REMARK42.changeTheme() function:\n\u0026lt;script\u0026gt; const themeToggle = document.querySelector(\u0026#34;.theme-toggle\u0026#34;); themeToggle.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { setTimeout(() =\u0026gt; window.REMARK42.changeTheme(getHelloFriendTheme()), 10); }); \u0026lt;/script\u0026gt; We wait 10 ms before reading from local storage to avoid race conditions. All we have to do now is to enable comments by setting comments: true via Hugo Front Matter.\nWhat Do You Think?# You can find all the code on my GitHub. I also tagged the commits to make it easier to find the code for future reference:\nschnerring/infrastructure-core (Terraform, tag v0.5.0) schnerring/schnerring.github.io (Hugo, tag v1.1.0) Any feedback in the comments below is appreciated.\n","date":"2021-05-09","id":20,"permalink":"/blog/use-terraform-to-deploy-the-remark42-commenting-system-to-kubernetes-and-integrate-it-with-a-hugo-website/","summary":"Building upon our previous work, we will deploy Remark42 on Kubernetes with Terraform and integrate it with your existing Hugo website. Make sure to check out my previous posts about creating a Hugo Website and deploying an Azure Kubernetes Service cluster if you haven\u0026rsquo;t already.\n","tags":["AKS","Azure","Azure Kubernetes Service","Cloud","Hugo","k8s","Kubernetes","Remark42","Self-host","Terraform"],"title":"Use Terraform to Deploy the Remark42 Commenting System to Kubernetes and Integrate it with a Hugo Website"},{"content":"In this post, we will deploy a simple Azure Kubernetes Service (AKS) cluster from scratch. To expose our web services securely, we will install Traefik 2 and configure cert-manager to manage Let\u0026rsquo;s Encrypt certificates. The best part about it: we will do everything with Terraform.\nPrerequisites# Keep in mind that I have tested the steps that follow on Windows. So if you are using another OS, you might have to modify them slightly. I also omit some non-essential steps in between, so it helps if you are already familiar with Azure, Kubernetes, and Terraform.\nTo follow along, you will need the following things:\nAn active Azure subscription Install Terraform Install kubectl A registered domain A DNS provider: that supports cert-manager DNS01 challenge validation with API access, so we can manage DNS records with Terraform ‚Äî I use Cloudflare Overview# Here is an outline of the steps required to build our solution:\nSetup Terraform Create the AKS cluster Deploy cert-manager Configure Let\u0026rsquo;s Encrypt certificates Deploy Traefik Deploy a demo application Step 1: Setup Terraform# We will make use of Terraform providers to put everything together:\nazurerm to manage our AKS cluster helm to deploy cert-manager and Traefik kubernetes to manage namespaces and deploy our demo app cloudflare to manage DNS records We add a provider.tf file with the following content:\nterraform { required_version = \u0026#34;\u0026gt;= 1.4.0\u0026#34; required_providers { azurerm = { source = \u0026#34;azurerm\u0026#34; version = \u0026#34;\u0026gt;= 3.47.0\u0026#34; } cloudflare = { source = \u0026#34;cloudflare/cloudflare\u0026#34; version = \u0026#34;\u0026gt;= 4.1.0\u0026#34; } helm = { source = \u0026#34;helm\u0026#34; version = \u0026#34;\u0026gt;= 2.9.0\u0026#34; } kubernetes = { source = \u0026#34;kubernetes\u0026#34; version = \u0026#34;\u0026gt;= 2.18.1\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } provider \u0026#34;cloudflare\u0026#34; {} For now, we only configured the azurerm and cloudflare providers. After setting up the AKS cluster, we will configure the helm and kubernetes providers.\nI have opted to configure the azurerm provider with environment variables. You might want to choose a different approach depending on your needs.\nTo authenticate the cloudflare provider, I use a Cloudflare API Token with Edit Zone permissions.\nAfter, we make sure to run terraform init to get started.\nStep 2: Create the AKS cluster# Creating a production-ready AKS cluster is out of scope for this post, which means that we will not delve too deep into AKS configuration. There are many things we are skipping over, like backups and monitoring. For a little more elaborate example, check out the official Terraform on Azure documentation.\nWe create a new file k8s.tf with the following content:\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;k8s\u0026#34; { name = \u0026#34;k8s-rg\u0026#34; location = var.location } resource \u0026#34;random_id\u0026#34; \u0026#34;random\u0026#34; { byte_length = 2 } resource \u0026#34;azurerm_kubernetes_cluster\u0026#34; \u0026#34;k8s\u0026#34; { name = \u0026#34;k8s-aks\u0026#34; resource_group_name = azurerm_resource_group.k8s.name location = var.location tags = var.tags dns_prefix = \u0026#34;k8saks${random_id.random.dec}\u0026#34; default_node_pool { name = \u0026#34;default\u0026#34; node_count = 1 vm_size = \u0026#34;Standard_B2ms\u0026#34; } identity { type = \u0026#34;SystemAssigned\u0026#34; } } Note that I have defined the var.location and var.tags variables in a separate variables.tf file.\nTo be able to access the AKS cluster locally with kubectl, we define a Terraform output in the outputs.tf file:\noutput \u0026#34;kube_config\u0026#34; { value = azurerm_kubernetes_cluster.k8s.kube_config_raw description = \u0026#34;kubeconfig for kubectl access.\u0026#34; sensitive = true } We have to set sensitive = true so our credentials will not get leaked, which could happen if we later decide to run Terraform with GitHub Actions. We apply our configuration by running Terraform:\nterraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan After the deployment completes, we set up kubectl to be able to access our cluster:\nterraform output -raw kube_config \u0026gt; ~/.kube/config We check the deployment by running kubectl get all:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 3m54s Step 3: Deploy cert-manager# To issue free Let\u0026rsquo;s Encrypt certificates for the web services we provide, the first thing we have to deploy is cert-manager. We need to configure the helm provider first. While we are on it, we also configure the kubernetes provider:\nprovider \u0026#34;helm\u0026#34; { kubernetes { host = azurerm_kubernetes_cluster.k8s.kube_config.0.host client_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.cluster_ca_certificate) } } provider \u0026#34;kubernetes\u0026#34; { host = azurerm_kubernetes_cluster.k8s.kube_config.0.host client_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.k8s.kube_config.0.cluster_ca_certificate) } Stacking the providers above with our managed Kubernetes cluster resources can lead to errors and should be avoided. I will mention this once more at the end of the post.\nNext, we deploy cert-manager with Helm by adding the following Terraform code to the k8s.tf file:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;cert_manager\u0026#34; { metadata { name = \u0026#34;cert-manager\u0026#34; } } resource \u0026#34;helm_release\u0026#34; \u0026#34;cert_manager\u0026#34; { name = \u0026#34;cert-manager\u0026#34; repository = \u0026#34;https://charts.jetstack.io\u0026#34; chart = \u0026#34;cert-manager\u0026#34; version = \u0026#34;v1.14.1\u0026#34; namespace = kubernetes_namespace.cert_manager.metadata.0.name set { name = \u0026#34;installCRDs\u0026#34; value = \u0026#34;true\u0026#34; } } We then run terraform apply to deploy cert-manager. We then check our work by running kubectl get all --namespace cert-manager, which should display something like this:\nNAME READY STATUS RESTARTS AGE pod/cert-manager-7998c69865-vrvrd 1/1 Running 0 75s pod/cert-manager-cainjector-7b744d56fb-qb54k 1/1 Running 0 75s pod/cert-manager-webhook-7d6d4c78bc-svzq5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.0.141.59 \u0026lt;none\u0026gt; 9402/TCP 75s service/cert-manager-webhook ClusterIP 10.0.18.192 \u0026lt;none\u0026gt; 443/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 75s deployment.apps/cert-manager-cainjector 1/1 1 1 75s deployment.apps/cert-manager-webhook 1/1 1 1 75s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-7998c69865 1 1 1 75s replicaset.apps/cert-manager-cainjector-7b744d56fb 1 1 1 75s replicaset.apps/cert-manager-webhook-7d6d4c78bc 1 1 1 75s Step 4: Configure Let\u0026rsquo;s Encrypt Certificates# In Kubernetes, Issuers are Kubernetes resources representing certificate authorities able to generate certificates. We have to create a single ClusterIssuer, a cluster-wide Issuer, using DNS01 challenge validation with Let\u0026rsquo;s Encrypt servers. As mentioned earlier, we will use Cloudflare, but many other DNS providers are supported.\nFirst, we need to create a Cloudflare API Token on the Cloudflare website, at User Profile ‚Üí API Tokens. The following permissions are required:\nZone - DNS - Edit Zone - Zone - Read To securely pass the token to Terraform, we create a sensitive variable. We also add a variable containing the email address where Let\u0026rsquo;s Encrypt can notify us about expiring certificates:\nvariable \u0026#34;letsencrypt_email\u0026#34; { type = string description = \u0026#34;Email address that Let\u0026#39;s Encrypt will use to send notifications about expiring certificates and account-related issues to.\u0026#34; sensitive = true } variable \u0026#34;letsencrypt_cloudflare_api_token\u0026#34; { type = string description = \u0026#34;Cloudflare API token with Zone-DNS-Edit and Zone-Zone-Read permissions, which is required for DNS01 challenge validation.\u0026#34; sensitive = true } With Terraform, we then add the secret containing the API token to our cluster. Since ClusterIssuer is a cluster-scoped resource, we need to make sure the secret is globally available by putting it in the cert-manager namespace:\nresource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;letsencrypt_cloudflare_api_token_secret\u0026#34; { metadata { name = \u0026#34;letsencrypt-cloudflare-api-token-secret\u0026#34; namespace = kubernetes_namespace.cert_manager.metadata.0.name } data = { \u0026#34;api-token\u0026#34; = var.letsencrypt_cloudflare_api_token } } Next, we add the staging and production ClusterIssuer cert-manager CRD resources that use Let\u0026rsquo;s Encrypt servers. We will have to use regular Kubernetes YAML manifests since we cannot deploy CRDs with the kubernetes provider. Here, the kubernetes_manifest resource comes in. Together with the Terraform yamldecode() and templatefile() functions, we get a pretty nice solution.\nLet\u0026rsquo;s start by defining the letsencrypt-issuer.tpl.yaml template file:\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ${name} spec: acme: email: ${email} server: ${server} privateKeySecretRef: name: issuer-account-key-${name} solvers: - dns01: cloudflare: apiTokenSecretRef: name: ${api_token_secret_name} key: ${api_token_secret_data_key} We then create the staging and production ClusterIssuers like so:\nresource \u0026#34;kubernetes_manifest\u0026#34; \u0026#34;letsencrypt_issuer_staging\u0026#34; { manifest = yamldecode(templatefile( \u0026#34;${path.module}/letsencrypt-issuer.tpl.yaml\u0026#34;, { \u0026#34;name\u0026#34; = \u0026#34;letsencrypt-staging\u0026#34; \u0026#34;email\u0026#34; = var.letsencrypt_email \u0026#34;server\u0026#34; = \u0026#34;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#34; \u0026#34;api_token_secret_name\u0026#34; = kubernetes_secret.letsencrypt_cloudflare_api_token_secret.metadata.0.name \u0026#34;api_token_secret_data_key\u0026#34; = keys(kubernetes_secret.letsencrypt_cloudflare_api_token_secret.data).0 } )) depends_on = [helm_release.cert_manager] } resource \u0026#34;kubernetes_manifest\u0026#34; \u0026#34;letsencrypt_issuer_production\u0026#34; { manifest = yamldecode(templatefile( \u0026#34;${path.module}/letsencrypt-issuer.tpl.yaml\u0026#34;, { \u0026#34;name\u0026#34; = \u0026#34;letsencrypt-production\u0026#34; \u0026#34;email\u0026#34; = var.letsencrypt_email \u0026#34;server\u0026#34; = \u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34; \u0026#34;api_token_secret_name\u0026#34; = kubernetes_secret.letsencrypt_cloudflare_api_token_secret.metadata.0.name \u0026#34;api_token_secret_data_key\u0026#34; = keys(kubernetes_secret.letsencrypt_cloudflare_api_token_secret.data).0 } )) depends_on = [helm_release.cert_manager] } Now we terraform apply the changes.\nStep 5: Deploy Traefik# To manage external access to our Kubernetes cluster, we need to configure Kubernetes Ingress resources. To satisfy an Ingress, we first need to configure an Ingress Controller. We will use Traefik for this.\nTo manage ingress, we could also use the Traefik IngressRoute CRD. At the time of this writing, cert-manager cannot directly interface with Traefik CRDs, so we would have to manage Certificate and Secret resources manually, which is cumbersome.\nWe add the following to the k8s.tf file:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;traefik\u0026#34; { metadata { name = \u0026#34;traefik\u0026#34; } } resource \u0026#34;helm_release\u0026#34; \u0026#34;traefik\u0026#34; { name = \u0026#34;traefik\u0026#34; repository = \u0026#34;https://helm.traefik.io/traefik\u0026#34; chart = \u0026#34;traefik\u0026#34; version = \u0026#34;26.0.0\u0026#34; namespace = kubernetes_namespace.traefik.metadata.0.name set { name = \u0026#34;ports.web.redirectTo.port\u0026#34; value = \u0026#34;websecure\u0026#34; } # Trust private AKS IP range set { name = \u0026#34;additionalArguments\u0026#34; value = \u0026#34;{--entryPoints.websecure.forwardedHeaders.trustedIPs=10.0.0.0/8}\u0026#34; } } Setting ports.web.redirectTo.port to websecure forces all HTTP traffic to be redirected to HTTPS.\nTo configure Traefik to trust forwarded headers from Azure, we set entryPoints.websecure.forwardedHeaders.trustedIPs=10.0.0.0/8.\nAfter running terraform apply, we check the deployment by running kubectl get all --namespace traefik:\nNAME READY STATUS RESTARTS AGE pod/traefik-6b6767d778-hxzzw 1/1 Running 0 68s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.0.247.4 51.103.157.225 80:32468/TCP,443:31284/TCP 69s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 69s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-6b6767d778 1 1 1 69s Next, we add a DNS record with the IP of our Traefik service. To obtain the external IP address of the service, we leverage the kubernetes_service Data Source of the kubernetes provider. We then add the DNS record k8s.schnerring.net pointing to the external IP of Traefik.\nLet us update the k8s.tf file accordingly and terraform apply the changes:\ndata \u0026#34;kubernetes_service\u0026#34; \u0026#34;traefik\u0026#34; { metadata { name = helm_release.traefik.name namespace = helm_release.traefik.namespace } } resource \u0026#34;cloudflare_record\u0026#34; \u0026#34;traefik\u0026#34; { zone_id = cloudflare_zone.schnerring_net.id name = \u0026#34;k8s\u0026#34; type = \u0026#34;A\u0026#34; value = data.kubernetes_service.traefik.status.0.load_balancer.0.ingress.0.ip proxied = true } How awesome is that?\nStep 6: Deploy a Demo Application# We are almost at the finish line. All that is missing is reaping the fruit of our hard labor. To create a simple demo, we will use the nginxdemos/hello image and make it available at https://hello.k8s.schnerring.net/.\nI moved the demo to https://hello.schnerring.net. I put my AKS cluster behind Cloudflare and the free universal SSL certificate only supports subdomains (sub.schnerring.net) but not subsubdomains (sub.sub.schnerring.net).\nTo make it happen, we add a kubernetes_namespace, kubernetes_deployment, kubernetes_service, and kubernetes_ingress resource to a new hello.tf file:\nresource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;hello\u0026#34; { metadata { name = \u0026#34;hello\u0026#34; } } resource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;hello\u0026#34; { metadata { name = \u0026#34;hello-deploy\u0026#34; namespace = kubernetes_namespace.hello.metadata.0.name labels = { app = \u0026#34;hello\u0026#34; } } spec { replicas = 2 selector { match_labels = { app = \u0026#34;hello\u0026#34; } } template { metadata { labels = { app = \u0026#34;hello\u0026#34; } } spec { container { image = \u0026#34;nginxdemos/hello\u0026#34; name = \u0026#34;hello\u0026#34; port { container_port = 80 } } } } } } resource \u0026#34;kubernetes_service\u0026#34; \u0026#34;hello\u0026#34; { metadata { name = \u0026#34;hello-svc\u0026#34; namespace = kubernetes_namespace.hello.metadata.0.name } spec { selector = { app = kubernetes_deployment.hello.metadata.0.labels.app } port { port = 80 target_port = 80 } } } resource \u0026#34;kubernetes_ingress_v1\u0026#34; \u0026#34;hello\u0026#34; { metadata { name = \u0026#34;hello-ing\u0026#34; namespace = kubernetes_namespace.hello.metadata.0.name annotations = { \u0026#34;cert-manager.io/cluster-issuer\u0026#34; = \u0026#34;letsencrypt-staging\u0026#34; \u0026#34;traefik.ingress.kubernetes.io/router.tls\u0026#34; = \u0026#34;true\u0026#34; } } spec { rule { host = \u0026#34;hello.k8s.schnerring.net\u0026#34; http { path { path = \u0026#34;/\u0026#34; backend { service { name = \u0026#34;hello-svc\u0026#34; port { number = 80 } } } } } } tls { hosts = [\u0026#34;hello.k8s.schnerring.net\u0026#34;] secret_name = \u0026#34;hello-tls-secret\u0026#34; } } } After running terraform apply again, we should be able to visit the demo site https://hello.k8s.schnerring.net/:\nTo verify the HTTPS redirect works, we run curl -svDL http://hello.k8s.schnerring.net (PowerShell), or curl -sLD - http://hello.k8s.schnerring.net (Bash):\n... \u0026lt; HTTP/1.1 301 Moved Permanently \u0026lt; Location: https://hello.k8s.schnerring.net/ ... To get rid of the certificate warning, set \u0026quot;cert-manager.io/cluster-issuer\u0026quot; = \u0026quot;letsencrypt-production\u0026quot;. But be aware of rate limits that apply to the Let\u0026rsquo;s Encrypt production API!\nOne Last Thing# If we want to tear down the cluster and rebuild, we cannot achieve this in one terraform apply operation. The reason is that the kubernetes provider requires an operational cluster during the terraform plan phase. On top of that, any CRDs we deploy have to be available during terraform plan, too.\nSo when rebuilding, we would first create the AKS cluster and deploy cert-manager and then apply the rest:\nterraform destroy -target \u0026#34;azurerm_resource_group.k8s\u0026#34; terraform plan -out infrastructure.tfplan -target \u0026#34;helm_release.cert_manager\u0026#34; terraform apply infrastructure.tfplan terraform plan -out infrastructure.tfplan terraform apply infrastructure.tfplan I already mentioned this earlier. The need for the workaround above originates from stacking Kubernetes cluster infrastructure with Kubernetes resources which the official Kubernetes provider documentation discourages. Adhering to the docs and separating cluster and Kubernetes resources into different modules will probably save you a headache!\nOther than that, we created a pretty cool solution, fully managed by Terraform, did we not?\nYou can find all the code on GitHub in my schnerring/infrastructure-core repository, which is evolving continuously.\nI have updated and refactored my Terraform code several times since I published this post. The original, all-in-one but outdated Terraform code can be found here.\nThe up-to-date code looks slightly different and can be found here:\naks.tf cert-manager.tf traefik-v2.tf letsencrypt.tf hello.tf ","date":"2021-04-25","id":21,"permalink":"/blog/use-terraform-to-deploy-an-azure-kubernetes-service-aks-cluster-traefik-2-cert-manager-and-lets-encrypt-certificates/","summary":"In this post, we will deploy a simple Azure Kubernetes Service (AKS) cluster from scratch. To expose our web services securely, we will install Traefik 2 and configure cert-manager to manage Let\u0026rsquo;s Encrypt certificates. The best part about it: we will do everything with Terraform.\n","tags":["AKS","Azure","Azure Kubernetes Service","cert-manager","Cloud","DevOps","Helm","k8s","Kubernetes","Let's Encrypt","Network","Reverse Proxy","Self-host","Terraform","Traefik"],"title":"Use Terraform to Deploy an Azure Kubernetes Service (AKS) Cluster, Traefik 2, cert-manager, and Let's Encrypt Certificates"},{"content":"In this beginner guide, you\u0026rsquo;ll create a Hugo website from scratch and publish the website on GitHub Pages. You\u0026rsquo;ll configure Cloudflare\u0026rsquo;s DNS and utilize its caching capabilities to optimize page speeds. Finally, implementing automated deployments with GitHub Pages will enable you to publish new content on your site easily.\nMotivation# Over the last couple of years, I\u0026rsquo;ve written documentation for private hobby projects, most of it in Markdown and managed with Git. It\u0026rsquo;s all over the place, some parts quite elaborate, other stuff just bullet point lists.\nI think some of it might be useful for others, so I started looking for ways to publish Markdown documentation and found Hugo. It\u0026rsquo;s a simple to use, modern, and very popular static site generator that encourages the use of Markdown files. Perfect!\nHaving found Hugo, I started looking into how to best host static content. I\u0026rsquo;ve been using GitHub forever, so obviously I chose GitHub Pages. Accompanied by Cloudflare\u0026rsquo;s caching capabilities, a blazingly fast website is guaranteed.\nThe only thing missing was a way to automagically publish changes made to the Git repository on the website. This is where GitHub Actions come in.\nWith the technology figured out, in my very first guide, I\u0026rsquo;ll show you step by step how I created this website in its first version. You can check out the code on my GitHub where I tagged the resulting commit with version v1.0.0.\nThe Plan# Here is an overview of what you\u0026rsquo;ll do:\nRegister for third-party services and install the required software Prepare the Git repository Set up the development environment Create a Hugo site from scratch and run it locally Set up Cloudflare for a custom root (apex) domain Manually deploy the website to GitHub Pages Deploy the website automatically to GitHub Pages with GitHub Actions I created this guide with Windows users in mind, but the workflow should be easily adaptable to other platforms. The steps are detailed and beginner-friendly, so if you\u0026rsquo;re more experienced, you can skip through most parts of steps 1 to 3.\nFor this guide, I configured the schnerring.net domain and used schnerring.github.io as the GitHub Pages site. In the instructions, you\u0026rsquo;ll need to replace these accordingly.\nStep #1: Prerequisites# First, you\u0026rsquo;ll need to register for a couple of services and install some software. Everything mentioned is free, except registering a custom domain name. I\u0026rsquo;m not affiliated with any of the products I recommend in this guide.\nSign Up at Third-Party Services# Register a domain name with your registrar ‚Äî I\u0026rsquo;m a very happy Namecheap customer Sign up at Cloudflare Sign up at GitHub Install Software for Development# Install the following programs for local development on our workstation:\nHugo ‚Äî make sure to install hugo-extended to be able to use themes that utilize Sass/SCSS Git for Windows ‚Äî source control management and shell Visual Studio Code ‚Äî editor for coding Google Chrome ‚Äî web browser for debugging I used the Git Bash terminal to create the instructions for this guide. It\u0026rsquo;s bundled with Git for Windows and provides UNIX-like commands like rm, touch, and more.\nStep #2: Prepare Git# Create a GitHub Repository# Sign in to your GitHub account and create a new repository. To create a user type website with GitHub Pages, name the repository schnerring.github.io.\nIf you want to know more about GitHub Page types, you can find further information in the GitHub Docs.\nMake sure to initialize the repository with a README and a LICENSE file. For this project, I chose the MIT license.\nClone the Newly Created Repository and Open It in VS Code# If you haven\u0026rsquo;t already, configure an SSH key to use with GitHub. You\u0026rsquo;re now ready to clone the GitHub repository to your workstation. Open a Git Bash terminal and run the following commands:\ngit clone git@github.com:schnerring/schnerring.github.io.git cd schnerring.github.io code . Step #3: Prepare VS Code# To make it easier to write clean and consistent code, install the following extensions:\nEditorConfig\nEditorConfig helps maintain consistent coding styles for multiple developers working on the same project across various editors and IDEs.\nmarkdownlint\nmarkdownlint is a static analysis tool for Node.js with a library of rules to enforce standards and consistency for Markdown files\nshellcheck\nShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell scripts\nAdd the .vscode/extensions.json File# Workspace recommended extensions for VS Code make it easy to share a set of extensions easily across development environments. When you open a repository folder in VS Code and some recommended extensions are missing, you\u0026rsquo;ll be notified, and just one click away from installing them.\nHere\u0026rsquo;s what the file looks like:\n{ \u0026#34;recommendations\u0026#34;: [ \u0026#34;editorconfig.editorconfig\u0026#34;, \u0026#34;davidanson.vscode-markdownlint\u0026#34;, \u0026#34;omartawfik.github-actions-vscode\u0026#34;, \u0026#34;timonwong.shellcheck\u0026#34; ] } Add the .vscode/launch.json File# When running hugo server locally during development, it\u0026rsquo;s nice to be able to open http://localhost:1313 in Chrome from within VS Code by just hitting the F5 key. For this, you need to add a launch configuration. VS Code stores launch configurations in the launch.json file located in the .vscode/ folder.\nFor Windows, the configuration looks like this:\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;pwa-chrome\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Launch Chrome against localhost\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost:1313\u0026#34;, \u0026#34;webRoot\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; } ] } Check out the official documentation to learn more about debugging in VS Code.\nAdd the EditorConfig Configuration# Add the .editorconfig file to the root folder of the repository:\n# top-most EditorConfig file root = true # every file [*] charset = utf-8 indent_size = 2 indent_style = space insert_final_newline = true trim_trailing_whitespace = true # Markdown files [*.md] trim_trailing_whitespace = false Check out EditorConfig if you want to learn more, it\u0026rsquo;s awesome!\nAdd the .gitignore File# To prevent cluttering of the repository with files generated by VS Code, add a .gitignore file to the root of the repository. To generate the .gitignore file, I like to use gitignore.io. Hugo adds a debug.log file from time to time which I manually add, as well.\ndebug.log # Created by https://www.toptal.com/developers/gitignore/api/hugo,vscode # Edit at https://www.toptal.com/developers/gitignore?templates=hugo,vscode ### Hugo ### # Generated files by hugo /public/ /resources/_gen/ hugo_stats.json # Executable may be added to repository hugo.exe hugo.darwin hugo.linux ### vscode ### .vscode/* !.vscode/settings.json !.vscode/tasks.json !.vscode/launch.json !.vscode/extensions.json *.code-workspace # End of https://www.toptal.com/developers/gitignore/api/hugo,vscode Push Development Environment Settings to GitHub# The folder structure now looks like this:\n.vscode/ ‚îú‚îÄ extensions.json ‚îú‚îÄ launch.json .editorconfig .gitignore LICENSE README.md Push the changes to GitHub:\ngit add --all git commit --message=\u0026#34;Add vscode launch.json / extensions.json, .editorconfig, .gitignore\u0026#34; git push Step #4: Enter Hugo# Only a few steps are required to create a functional Hugo site.\nCreate a New Site# To create a Hugo site in your current working folder, use the Hugo CLI:\nhugo new site . --force With --force, Hugo won\u0026rsquo;t complain about the folder containing files.\nAdd a Theme# To get started quickly, add the Hello Friend theme as a Git Submodule. Check out themes.gohugo.io for more themes.\ngit submodule add https://github.com/panr/hugo-theme-hello-friend.git themes/hello-friend Update the Hugo Configuration# The Hello Friend theme requires minimal configuration steps to work. As a baseline, use the sample provided in the themes\u0026rsquo; README and make some straightforward changes to it.\nThe resulting config.toml looks like this:\nbaseURL = \u0026#34;https://schnerring.net\u0026#34; languageCode = \u0026#34;en-us\u0026#34; theme = \u0026#34;hello-friend\u0026#34; paginate = 5 ignoreFiles = [ \u0026#34;LICENSE$\u0026#34; ] [params] # dir name of your blog content (default is `content/posts`). # the list of set content will show up on your index page (baseurl). #contentTypeName = \u0026#34;posts\u0026#34; # \u0026#34;light\u0026#34; or \u0026#34;dark\u0026#34; defaultTheme = \u0026#34;dark\u0026#34; # if you set this to 0, only submenu trigger will be visible showMenuItems = 2 # Show reading time in minutes for posts showReadingTime = false # Show table of contents at the top of your posts (defaults to false) # Alternatively, add this param to post front matter for specific posts # toc = true # Show full page content in RSS feed items #(default is Description or Summary metadata in the front matter) # rssFullText = true [languages] [languages.en] title = \u0026#34;Michael Schnerring\u0026#34; subtitle = \u0026#34;Coder and Computer Enthusiast\u0026#34; keywords = \u0026#34;\u0026#34; description = \u0026#34;Michael Schnerring is a coder and computer enthusiast.\u0026#34; copyright = \u0026#34;\u0026lt;p\u0026gt;Copyright ¬© 2020 Michael Schnerring\u0026lt;/p\u0026gt;\u0026lt;p xmlns:dct=\u0026#39;http://purl.org/dc/terms/\u0026#39; xmlns:cc=\u0026#39;http://creativecommons.org/ns#\u0026#39; class=\u0026#39;license-text\u0026#39;\u0026gt;\u0026lt;a rel=\u0026#39;cc:attributionURL\u0026#39; property=\u0026#39;dct:title\u0026#39; href=\u0026#39;https://github.com/schnerring/schnerring.github.io/tree/main/content\u0026#39;\u0026gt;Content\u0026lt;/a\u0026gt; licensed under \u0026lt;a rel=\u0026#39;license\u0026#39; href=\u0026#39;https://creativecommons.org/licenses/by/4.0\u0026#39;\u0026gt;CC BY 4.0\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#34; menuMore = \u0026#34;Show more\u0026#34; writtenBy = \u0026#34;Written by\u0026#34; readMore = \u0026#34;Read more\u0026#34; readOtherPosts = \u0026#34;Read other posts\u0026#34; newerPosts = \u0026#34;Newer posts\u0026#34; olderPosts = \u0026#34;Older posts\u0026#34; minuteReadingTime = \u0026#34;min read\u0026#34; dateFormatSingle = \u0026#34;2006-01-02\u0026#34; dateFormatList = \u0026#34;2006-01-02\u0026#34; # leave empty to disable, enter display text to enable #lastModDisplay = \u0026#34;modified\u0026#34; [languages.en.params.logo] logoText = \u0026#34;schnerring.net\u0026#34; logoHomeLink = \u0026#34;/\u0026#34; # or # # path = \u0026#34;/img/your-example-logo.svg\u0026#34; # alt = \u0026#34;Your example logo alt text\u0026#34; [languages.en.menu] [[languages.en.menu.main]] identifier = \u0026#34;about\u0026#34; name = \u0026#34;About\u0026#34; url = \u0026#34;/about\u0026#34; The themes\u0026rsquo; GitHub repository features another config.toml example. You can read more about how to configure Hugo in the official documentation.\nAdd Some Content# With the help of the Hugo CLI, add an about page and the first blog post:\nhugo new about.md hugo new posts/hello-world.md about.md# --- title: \u0026#34;About\u0026#34; draft: false --- I code by day and toy around with computers by night... posts/hello-world.md# --- title: \u0026#34;Hello World\u0026#34; description: \u0026#34;The first post of this blog\u0026#34; date: 2021-03-14T15:00:21+01:00 draft: false tags: - \u0026#34;csharp\u0026#34; - \u0026#34;hello world\u0026#34; --- I\u0026#39;m a .NET developer by trade, so let\u0026#39;s say hello in C#! ```csharp using System; class Program { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Hello, world!\u0026#34;); } } ``` Note the post\u0026rsquo;s default status draft: true, so newly created content has to be published manually. You can read more about it in the official documentation. Make sure to set this to false before deployment or the post won\u0026rsquo;t be displayed.\nRun the Site Locally# To finally test the changes locally, run the following in a Git Bash terminal. The --buildDrafts (or -D) option enables you to also view content that has set draft: true.\nhugo server --buildDrafts Hit the F5 button in VS Code to open the page in Chrome.\nPush the Changes# Git ignores empty folders, so if you push the current changes and re-clone the repository, the Hugo folder structure would be partially gone. A common practice is to add empty .gitkeep marker files to empty folders to prevent this:\ntouch {data,layouts,static}/.gitkeep Your folder structure should look like this, excluding resources or public folders, since those are .gitignored:\n.vscode/ ‚îú‚îÄ extensions.json ‚îú‚îÄ launch.json archetypes/ ‚îú‚îÄ default.md content/ ‚îú‚îÄ posts/ ‚îÇ ‚îú‚îÄ hello-world.md ‚îú‚îÄ about.md data/ ‚îú‚îÄ .gitkeep layouts/ ‚îú‚îÄ .gitkeep static/ ‚îú‚îÄ .gitkeep themes/ ‚îú‚îÄ hello-friend/ (submodule) .editorconfig .gitignore .gitmodules config.toml LICENSE README.md Commit and push everything:\ngit add -A git commit -m \u0026#34;Add Hugo site, hello-friend theme, about page and hello-world post\u0026#34; git push Step #5: Configure Cloudflare# Add the Site to Cloudflare# Sign in to your Cloudflare account Click + Add site in the navigation bar and add schnerring.net Select Free plan Navigate to the Overview page of the newly created site Take note of Cloudflare\u0026rsquo;s nameservers, in my case carol.ns.cloudflare.com and cody.ns.cloudflare.com Change the Nameservers for Your Domain# Sign in to the administrator account of your domain registrar and change the nameservers. My Namecheap nameserver configuration for schnerring.net looks like this after adding the Cloudflare nameservers:\nAdd the CNAME Records# A CNAME record is used to map one domain name to another. Go to the DNS management at Cloudflare and add the following records to point your domain to GitHub Pages:\nMake sure the ‚Äúorange cloud‚Äù is enabled, so you can define rules and cache static content with Cloudflare.\nEnable Full SSL/TLS Encryption Mode# Go to your site\u0026rsquo;s SSL/TLS settings and set encryption to Full:\nConfigure the Browser Cache TTL# Go to Cache ‚Üí Configuration and choose 2 months or higher, depending on how often you think your already published static content changes.\nAdd Page Rules# The Free Tier lets you create up to three page rules. Go to your site\u0026rsquo;s Page Rules settings and click Create Page Rule.\nEnforce HTTPS# Add Forward from www.schnerring.net Subdomain to schnerring.net Root Domain# If you use a subdomain, skip this step.\nCache All Static Content to Speed Up the Website# Page Rules Overview# Note that as of May 2018 GitHub Pages supports HTTPS for custom domains out of the box. They utilize Let\u0026rsquo;s Encrypt certificates which are better than the shared certificates you get with Cloudflare. But to issue Let\u0026rsquo;s Encrypt certificates for both www.schnerring.net and schnerring.net, you\u0026rsquo;d have to resort this, in my opinion hacky, solution.\nStep #6: Deploy the Site to GitHub Pages# You\u0026rsquo;ll deploy the website to GitHub Pages on a separate, parallel gh-pages branch since published artifacts and source code are segregated this way. You can learn more about other options in the official Hugo documentation.\nInitialize gh-pages branch# Create an orphan branch which has a git init like state with no history:\ngit checkout --orphan gh-pages git reset --hard git commit --allow-empty -m \u0026#34;Init gh-pages branch\u0026#34; git push origin gh-pages git checkout main Checkout the gh-pages Branch in public/# To build the site, use the hugo command. The build artifacts will be placed in the public/ folder, the contents of which need to be published on the gh-pages branch. To be able to checkout multiple branches, use Git\u0026rsquo;s worktree feature:\ngit worktree add -B gh-pages public origin/gh-pages Add CNAME File to static/ Folder# Earlier, you configured Cloudflare to properly map www.schnerring.net and schnerring.net to schnerring.github.io\u0026rsquo;s destination. However, you also need a redirect from schnerring.github.io to schnerring.net. You accomplish this by adding a static/CNAME file to the repo, containing your custom domain. When generating the site with hugo, the published site will contain the CNAME file at its root:\necho \u0026#34;schnerring.net\u0026#34; \u0026gt; static/CNAME rm static/.gitkeep git add -A git commit -m \u0026#34;Add CNAME file\u0026#34; git push Build the Hugo Site# Make sure to set draft: false to publish the hello-world.md post. Push the change to GitHub by running git commit -am \u0026quot;Publish hello-world.md\u0026quot; \u0026amp;\u0026amp; git push.\nThen run hugo.\nPush the Changes on the gh-pages Branch# cd public git add -A git commit -m \u0026#34;Publish to gh-pages\u0026#34; git push cd .. Set gh-pages as Publishing Branch# Navigate to your repository on GitHub\nGo to Settings ‚Üí Pages Under Source, select Branch: gh-pages and click Save It should look like this:\nShortly after you perform these steps, your website should be available at schnerring.net.\nSubsequent Deployments# For future deployments, several steps are required:\nDelete contents of public/ folder, since hugo does not remove generated files before building Generate the site with hugo Push changes to the gh-pages branch Purge Cloudflare cache This is an error-prone and tedious process if repeated frequently, so let\u0026rsquo;s automate these steps with GitHub Pages next.\nStep #7: Automate Deployments with GitHub Actions# Configure GitHub Actions# GitHub Actions Workflows are configured mostly through configuration files in the .github/workflows folder. This allows your configuration to be version controlled and flexible.\nAdd the following .github/workflows/hugo.yml file to the repository:\nname: Hugo on: push: branches: - main # Allows to run workflow manually from Actions tab workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository and update Hugo themes uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Install Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;0.81.0\u0026#34; extended: true - name: Build Hugo run: hugo --minify - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public - name: Purge Cloudflare Cache env: CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }} CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }} GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | chmod +x ./purge_cloudflare_cache.sh ./purge_cloudflare_cache.sh Name the GitHub Actions Workflow Hugo which is triggered when pushed to the main branch. As OS, the latest version of ubuntu is used.\nWe only run one job, build, and perform the following steps:\nCheckout the repository and update Hugo themes Install Hugo extended since the theme uses SCSS. Build Hugo and --minify Deploy to GitHub Pages. The default deployment branch is gh-pages. Choose the ./public folder to be published. Purge the Cloudflare Cache by running the ./purge_cloudflare_cache.sh script that you\u0026rsquo;ll create in the next steps. The above is straight forward, thanks to the free Actions available at the GitHub Marketplace. Thanks to peaceiris for the awesome work:\nGitHub Actions for Hugo GitHub Actions for GitHub Pages Note the mapping of the environment variables required in step 4 and 5. You\u0026rsquo;ll later configure the GitHub secrets that will be injected as environment variables into the context of the respective steps. The GITHUB_TOKEN doesn\u0026rsquo;t need to be configured for GitHub Actions, since GitHub automatically creates it to use in your workflow. For local debugging though, you\u0026rsquo;ll have to create a token, anyway.\nCreate a GitHub Personal Access Token# Personal access tokens (PAT) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line.\nGo to your GitHub profile\u0026rsquo;s Settings ‚Üí Developer settings ‚Üí Personal access tokens. Click Generate new token, select only the public_repo scope and enter the note schnerring.github.io, to be able to later recognize what it\u0026rsquo;s used for.\nYou only get one chance to copy the token\u0026rsquo;s value, so add a new environment variable named GITHUB_TOKEN to your local system:\nIf you lose the token, just delete and then recreate it.\nCreate the purge_cloudflare_cache.sh Script# Purge the Cloudflare Cache# Use Cloudflare\u0026rsquo;s API to purge all cached files with curl after successful deployment:\ncurl \\ --silent \\ --request POST \\ --header \u0026#34;Authorization: Bearer ${CLOUDFLARE_API_TOKEN}\u0026#34; \\ --header \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{\u0026#34;purge_everything\u0026#34;:true}\u0026#39; \\ \u0026#34;https://api.cloudflare.com/client/v4/zones/${CLOUDFLARE_ZONE_ID}/purge_cache\u0026#34; Before you can run the command above, set CLOUDFLARE_ZONE_ID and CLOUDFLARE_API_TOKEN as environment variables in your GitHub repository and your workstation. You can add secrets in your GitHub repository\u0026rsquo;s Settings ‚Üí Secrets ‚Üí New repository secret:\nSet CLOUDFLARE_ZONE_ID to your Cloudflare site\u0026rsquo;s Zone ID which you can find it on your site\u0026rsquo;s Overview page.\nSet CLOUDFLARE_API_TOKEN to an API Token with Zone.Cache Purge permissions. Create one at My Profile ‚Üí API Tokens ‚Üí Create Token ‚Üí Create Custom Token ‚Üí Get started:\nYou should then be able to locally run the curl snippet above which should output \u0026quot;success\u0026quot;: true:\n{ \u0026#34;result\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;********************************\u0026#34; }, \u0026#34;success\u0026#34;: true, \u0026#34;errors\u0026#34;: [], \u0026#34;messages\u0026#34;: [] } Polling the GitHub Pages Build Status# Before purging the site\u0026rsquo;s cache on Cloudflare, make sure that the GitHub Pages build completed successfully. Via GitHub\u0026rsquo;s API, query the latest GitHub Pages build with curl:\ncurl \\ --silent \\ --user \u0026#34;schnerring:${GITHUB_TOKEN}\u0026#34; \\ --header \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ \u0026#34;https://api.github.com/repos/schnerring/schnerring.github.io/pages/builds/latest\u0026#34; The GITHUB_TOKEN you configured earlier is used here. Executing the snippet locally will output \u0026quot;status\u0026quot;: \u0026quot;built\u0026quot; if the GitHub Pages build succeeded:\n{ \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/schnerring/schnerring.github.io/pages/builds/123456789\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;built\u0026#34;, \u0026#34;error\u0026#34;: { \u0026#34;message\u0026#34;: null }, ... } Put It All Together# #!/usr/bin/env bash readonly DELAY_STEP_SECONDS=15 readonly INTERVAL_SECONDS=5 readonly TIMEOUT_SECONDS=120 readonly GITHUB_USER=schnerring readonly GITHUB_REPO=schnerring.github.io ################################################## # Poll status of latest GitHub Pages build every INTERVAL_SECONDS seconds for up # to TIMEOUT_SECONDS seconds. # Globals: # GITHUB_REPO # GITHUB_TOKEN # GITHUB_USER # INTERVAL_SECONDS # TIMEOUT_SECONDS # Arguments: # None # Outputs: # Success message to stdout or error message to stderr. # Returns: # 0 on success, 1 otherwise. ################################################## function poll_build_status() { echo \u0026#34;Awaiting completion of latest GitHub Pages build ...\u0026#34; local waited_seconds=0 while [[ \u0026#34;${waited_seconds}\u0026#34; -lt \u0026#34;${TIMEOUT_SECONDS}\u0026#34; ]]; do if curl \\ --silent \\ --user \u0026#34;${GITHUB_USER}:${GITHUB_TOKEN}\u0026#34; \\ --header \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ \u0026#34;https://api.github.com/repos/${GITHUB_USER}/${GITHUB_REPO}/pages/builds/latest\u0026#34; \\ | grep -q \u0026#39;\u0026#34;status\u0026#34;: \u0026#34;built\u0026#34;\u0026#39;; then echo \u0026#34;Success.\u0026#34; return 0 fi echo \u0026#34; Sleeping ${INTERVAL_SECONDS} seconds until next status poll ...\u0026#34; sleep \u0026#34;${INTERVAL_SECONDS}\u0026#34; (( waited_seconds += INTERVAL_SECONDS )) done echo \u0026#34;Failure.\u0026#34; \u0026gt;\u0026amp;2 return 1 } ################################################## # Purge entire Cloudflare cache. # Globals: # CLOUDFLARE_API_TOKEN # CLOUDFLARE_ZONE_ID # Arguments: # None # Outputs: # Success message to stdout or error message to stderr. # Returns: # 0 on success, 1 otherwise. ################################################## function purge_cache() { echo \u0026#34;Purging Cloudflare cache ...\u0026#34; if curl \\ --silent \\ --request POST \\ --header \u0026#34;Authorization: Bearer ${CLOUDFLARE_API_TOKEN}\u0026#34; \\ --header \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{\u0026#34;purge_everything\u0026#34;:true}\u0026#39; \\ \u0026#34;https://api.cloudflare.com/client/v4/zones/${CLOUDFLARE_ZONE_ID}/purge_cache\u0026#34; \\ | grep -q \u0026#39;\u0026#34;success\u0026#34;: true\u0026#39;; then echo \u0026#34;Success.\u0026#34; return 0 else echo \u0026#34;Failure.\u0026#34; \u0026gt;\u0026amp;2 return 1 fi } ################################################## # Main function of script. # Globals: # DELAY_STEP_SECONDS # Arguments: # None ################################################## function main() { echo \u0026#34;Sleeping ${DELAY_STEP_SECONDS} seconds ...\u0026#34; sleep \u0026#34;${DELAY_STEP_SECONDS}\u0026#34; poll_build_status || exit 1 echo \u0026#34;Sleeping ${DELAY_STEP_SECONDS} seconds ...\u0026#34; sleep \u0026#34;${DELAY_STEP_SECONDS}\u0026#34; purge_cache || exit 1 } # Entrypoint main \u0026#34;$@\u0026#34; poll_build_status and purge_cache contain the functionality. The main function serves as the entry point executing those functions.\npoll_build_status implements a while loop to repeatedly poll GitHub\u0026rsquo;s API. It succeeds if the response contains \u0026quot;status\u0026quot;: \u0026quot;built\u0026quot; or times out after two minutes and fails.\nBefore each step, the script heuristically sleeps 15 seconds in case of latency issues with GitHub\u0026rsquo;s API or GitHub Pages updates.\nRunning the ./purge_cloudflare_cache.sh script locally should output:\nAwaiting completion of latest GitHub Pages build ... Success. Purging Cloudflare cache ... Success. Note that for sites with lots of files, purging the whole cache should be avoided. Cloudflare supports purging the cache for individual files.\nWe could list only changed files if we wanted to with a command like git diff-tree -r --no-commit-id --name-only --diff-filter=DM gh-pages and only purge the cache for these files, but that\u0026rsquo;s out of scope for this post.\nNow push .github/workflows/hugo.yml and purge_cloudflare_cache.sh to the repository:\ngit add -A git commit -m \u0026#34;Add .github/workflows/hugo.yml, purge_cloudflare_cache.sh\u0026#34; git push Go check out the Actions tab on your GitHub repository. If everything is configured correctly, GitHub Actions should be building your site:\nAfter the workflow succeeded, there won\u0026rsquo;t be any changes to the site because we didn\u0026rsquo;t change anything about our site, yet.\nTo make a change, switch to the light theme by setting defaultTheme = \u0026quot;light\u0026quot; inside the config.toml file. Push the changes by running git commit -am \u0026quot;Set default theme to light\u0026quot; \u0026amp;\u0026amp; git push and another GitHub Action workflow should automatically be triggered.\nAfter the build completes, your website should be displayed in the light theme. If it isn\u0026rsquo;t, make sure to also purge your browser cache.\nWrapping Up# It took quite a bit of effort, but you now have a hands-off system in place that helps you to publish content to your website in an automated way. All you have to do is pushing new changes to your GitHub repo and after a minute the changes will be live.\n","date":"2021-03-19","id":22,"permalink":"/blog/create-a-hugo-website-with-github-pages-github-actions-and-cloudflare/","summary":"In this beginner guide, you\u0026rsquo;ll create a Hugo website from scratch and publish the website on GitHub Pages. You\u0026rsquo;ll configure Cloudflare\u0026rsquo;s DNS and utilize its caching capabilities to optimize page speeds. Finally, implementing automated deployments with GitHub Pages will enable you to publish new content on your site easily.\n","tags":["Hugo","Cloudflare","GitHub Actions","GitHub Pages"],"title":"Create a Hugo Website with GitHub Pages, GitHub Actions, and Cloudflare"},{"content":"I\u0026rsquo;m a .NET developer by trade, so let\u0026rsquo;s say hello in C#!\nusing System; class Program { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Hello, world!\u0026#34;); } } ","date":"2021-03-14","id":23,"permalink":"/blog/hello-world/","summary":"I\u0026rsquo;m a .NET developer by trade, so let\u0026rsquo;s say hello in C#!\nusing System; class Program { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Hello, world!\u0026#34;); } } ","tags":["CSharp","Hello World"],"title":"Hello World"}]